#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿å®ˆæ¸…ç†é‡å¤æ–‡ä»¶è„šæœ¬
å®‰å…¨åœ°è¯†åˆ«å’Œæ¸…ç†æ˜æ˜¾é‡å¤çš„æ–‡ä»¶ï¼Œä¿ç•™æœ€æ–°ç‰ˆæœ¬
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import shutil
from pathlib import Path
from datetime import datetime

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent.parent
BACKUP_DIR = PROJECT_ROOT / "cleanup_backups" / datetime.now().strftime("%Y%m%d_%H%M%S")

# éœ€è¦æ¸…ç†çš„é‡å¤æ–‡ä»¶æ˜ å°„
DUPLICATE_FILES = {
    # CLIé›†æˆæ–‡ä»¶
    "iflow/cli_integration_v6.py": {
        "keep": "iflow/cli_integration_enhanced_v7.py",
        "reason": "ä¿ç•™å¢å¼ºç‰ˆæœ¬ï¼Œæ¸…ç†æ—§ç‰ˆæœ¬"
    },
    
    # Hooksç³»ç»Ÿæ–‡ä»¶
    "iflow/hooks/enhanced_hooks_system_v7.py": {
        "keep": "iflow/hooks/enhanced_hooks_system_v9.py", 
        "reason": "ä¿ç•™v9ç‰ˆæœ¬ï¼Œæ¸…ç†v7ç‰ˆæœ¬"
    },
    "iflow/hooks/enhanced_hooks_system_v8.py": {
        "keep": "iflow/hooks/enhanced_hooks_system_v9.py",
        "reason": "ä¿ç•™v9ç‰ˆæœ¬ï¼Œæ¸…ç†v8ç‰ˆæœ¬"
    },
    
    "iflow/hooks/intelligent_hooks_system_v6.py": {
        "keep": "iflow/hooks/intelligent_hooks_system_v9.py",
        "reason": "ä¿ç•™v9ç‰ˆæœ¬ï¼Œæ¸…ç†v6ç‰ˆæœ¬"
    },
    "iflow/hooks/intelligent_hooks_system_v8.py": {
        "keep": "iflow/hooks/intelligent_hooks_system_v9.py", 
        "reason": "ä¿ç•™v9ç‰ˆæœ¬ï¼Œæ¸…ç†v8ç‰ˆæœ¬"
    },
    
    # Hookç®¡ç†å™¨å ä½ç¬¦é—®é¢˜
    "iflow/hooks/comprehensive_hook_manager_placeholder.py": {
        "keep": "iflow/hooks/comprehensive_hook_manager_v4.py",
        "reason": "æ›¿æ¢å ä½ç¬¦ä¸ºå®é™…å®ç°"
    }
}

def create_backup():
    """åˆ›å»ºå¤‡ä»½ç›®å½•"""
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    print(f"åˆ›å»ºå¤‡ä»½ç›®å½•: {BACKUP_DIR}")
    return True

def backup_file(file_path):
    """å¤‡ä»½æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•"""
    try:
        source = PROJECT_ROOT / file_path
        if source.exists():
            backup_path = BACKUP_DIR / file_path
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(source, backup_path)
            print(f"å·²å¤‡ä»½: {file_path} -> {backup_path}")
            return True
    except Exception as e:
        print(f"å¤‡ä»½å¤±è´¥ {file_path}: {e}")
        return False

def analyze_duplicates():
    """åˆ†æé‡å¤æ–‡ä»¶"""
    print("ğŸ” åˆ†æé‡å¤æ–‡ä»¶...")
    analysis = {
        "files_to_remove": [],
        "files_to_keep": [],
        "issues": []
    }
    
    for file_path, info in DUPLICATE_FILES.items():
        source_file = PROJECT_ROOT / file_path
        keep_file = PROJECT_ROOT / info["keep"]
        
        if source_file.exists():
            # æ£€æŸ¥ä¿ç•™çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not keep_file.exists():
                analysis["issues"].append(f"âš ï¸ ä¿ç•™æ–‡ä»¶ä¸å­˜åœ¨: {info['keep']}")
                continue
                
            # è·å–æ–‡ä»¶ä¿¡æ¯
            source_stat = source_file.stat()
            keep_stat = keep_file.stat()
            
            analysis["files_to_remove"].append({
                "file": file_path,
                "size": source_stat.st_size,
                "modified": datetime.fromtimestamp(source_stat.st_mtime).isoformat(),
                "keep_file": info["keep"],
                "reason": info["reason"]
            })
            
            analysis["files_to_keep"].append({
                "file": info["keep"],
                "size": keep_stat.st_size,
                "modified": datetime.fromtimestamp(keep_stat.st_mtime).isoformat()
            })
    
    return analysis

def show_analysis(analysis):
    """æ˜¾ç¤ºåˆ†æç»“æœ"""
    print("\né‡å¤æ–‡ä»¶åˆ†æç»“æœ:")
    print("=" * 60)
    
    for issue in analysis["issues"]:
        print(issue)
    
    print(f"\néœ€è¦åˆ é™¤çš„æ–‡ä»¶ ({len(analysis['files_to_remove'])}ä¸ª):")
    for item in analysis["files_to_remove"]:
        print(f"  - {item['file']} ({item['size']} bytes)")
        print(f"    ä¿ç•™: {item['keep_file']}")
        print(f"    åŸå› : {item['reason']}")
    
    print(f"\néœ€è¦ä¿ç•™çš„æ–‡ä»¶ ({len(set(item['file'] for item in analysis['files_to_keep']))}ä¸ª):")
    seen = set()
    for item in analysis["files_to_keep"]:
        if item["file"] not in seen:
            seen.add(item["file"])
            print(f"  - {item['file']} ({item['size']} bytes)")
    
    print(f"\né¢„ä¼°èŠ‚çœç©ºé—´: {sum(item['size'] for item in analysis['files_to_remove'])} bytes")

def confirm_cleanup():
    """ç¡®è®¤æ¸…ç†æ“ä½œ"""
    response = input("\nç¡®è®¤æ‰§è¡Œæ¸…ç†æ“ä½œå—ï¼Ÿ(y/N): ").strip().lower()
    return response in ['y', 'yes', 'æ˜¯', 'ç¡®è®¤']

def execute_cleanup(analysis):
    """æ‰§è¡Œæ¸…ç†æ“ä½œ"""
    print("\nå¼€å§‹æ‰§è¡Œæ¸…ç†...")
    
    removed_count = 0
    error_count = 0
    
    for item in analysis["files_to_remove"]:
        file_path = PROJECT_ROOT / item["file"]
        
        try:
            # å…ˆå¤‡ä»½
            if not backup_file(item["file"]):
                error_count += 1
                continue
            
            # åˆ é™¤æ–‡ä»¶
            file_path.unlink()
            print(f"å·²åˆ é™¤: {item['file']}")
            removed_count += 1
            
        except Exception as e:
            print(f"åˆ é™¤å¤±è´¥ {item['file']}: {e}")
            error_count += 1
    
    # å¤„ç†å ä½ç¬¦é—®é¢˜
    handle_placeholder_issue()
    
    return removed_count, error_count

def handle_placeholder_issue():
    """å¤„ç†å ä½ç¬¦é—®é¢˜"""
    print("\nå¤„ç†å ä½ç¬¦é—®é¢˜...")
    
    placeholder_file = PROJECT_ROOT / "iflow/hooks/comprehensive_hook_manager_placeholder.py"
    target_file = PROJECT_ROOT / "iflow/hooks/comprehensive_hook_manager_v4.py"
    
    if target_file.exists() and placeholder_file.exists():
        try:
            # å¤‡ä»½å ä½ç¬¦
            backup_file("iflow/hooks/comprehensive_hook_manager_placeholder.py")
            
            # åˆ é™¤å ä½ç¬¦
            placeholder_file.unlink()
            print("å·²åˆ é™¤å ä½ç¬¦æ–‡ä»¶")
            
            # åˆ›å»ºå®é™…çš„ç®¡ç†å™¨æ–‡ä»¶
            create_real_hook_manager()
            
        except Exception as e:
            print(f"å¤„ç†å ä½ç¬¦å¤±è´¥: {e}")

def create_real_hook_manager():
    """åˆ›å»ºçœŸæ­£çš„Hookç®¡ç†å™¨"""
    content = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆHookç®¡ç†å™¨ V4 (Comprehensive Hook Manager V4)
é›†æˆè‡ªåŠ¨æ™ºèƒ½è´¨é‡ç³»ç»Ÿçš„ç»ˆæHookç®¡ç†å™¨ï¼Œå®ç°å…¨è‡ªåŠ¨ä»£ç å®¡æŸ¥ã€æµ‹è¯•å’Œè´¨é‡ä¿éšœã€‚
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

logger = logging.getLogger(__name__)

class ComprehensiveHookManagerV4:
    """
    ç»¼åˆHookç®¡ç†å™¨V4 - å®ç°å®Œæ•´çš„Hookç”Ÿå‘½å‘¨æœŸç®¡ç†
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.hooks_registry = {}
        self.execution_context = {}
        self.performance_metrics = {
            "total_executions": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "avg_execution_time": 0.0
        }
        
        logger.info("ğŸŒ ç»¼åˆHookç®¡ç†å™¨V4åˆå§‹åŒ–å®Œæˆ")
    
    async def register_hook(self, hook_point: str, hook_function: Callable, priority: int = 50):
        """æ³¨å†ŒHook"""
        if hook_point not in self.hooks_registry:
            self.hooks_registry[hook_point] = []
        
        self.hooks_registry[hook_point].append({
            "function": hook_function,
            "priority": priority,
            "registered_at": datetime.now()
        })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.hooks_registry[hook_point].sort(key=lambda x: x["priority"])
        logger.debug(f"ğŸ“‹ æ³¨å†ŒHook: {hook_point} (ä¼˜å…ˆçº§: {priority})")
    
    async def execute_hooks(self, hook_point: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒHookç‚¹çš„æ‰€æœ‰Hook"""
        if hook_point not in self.hooks_registry:
            logger.debug(f"â­ï¸ æ— Hookæ³¨å†Œ: {hook_point}")
            return context
        
        start_time = time.time()
        hooks = self.hooks_registry[hook_point]
        
        logger.info(f"ğŸš€ æ‰§è¡ŒHookç‚¹: {hook_point} (å…±{len(hooks)}ä¸ªHook)")
        
        try:
            # ä¾æ¬¡æ‰§è¡ŒHook
            for hook_info in hooks:
                hook_func = hook_info["function"]
                try:
                    # æ‰§è¡ŒHookå‡½æ•°
                    if asyncio.iscoroutinefunction(hook_func):
                        result = await hook_func(context)
                    else:
                        result = hook_func(context)
                    
                    # æ›´æ–°ä¸Šä¸‹æ–‡
                    if isinstance(result, dict):
                        context.update(result)
                    
                    self.performance_metrics["successful_executions"] += 1
                    
                except Exception as e:
                    logger.error(f"âŒ Hookæ‰§è¡Œå¤±è´¥: {hook_func.__name__} - {e}")
                    self.performance_metrics["failed_executions"] += 1
            
            execution_time = time.time() - start_time
            self.performance_metrics["total_executions"] += 1
            self.performance_metrics["avg_execution_time"] = (
                (self.performance_metrics["avg_execution_time"] * (self.performance_metrics["total_executions"] - 1) + execution_time) 
                / self.performance_metrics["total_executions"]
            )
            
            logger.info(f"âœ… Hookç‚¹æ‰§è¡Œå®Œæˆ: {hook_point} (è€—æ—¶: {execution_time:.3f}s)")
            
        except Exception as e:
            logger.error(f"âŒ Hookæ‰§è¡Œå¼‚å¸¸: {hook_point} - {e}")
        
        return context
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ Hookç®¡ç†å™¨æ¸…ç†å®Œæˆ")
        return True

# å…¨å±€Hookç®¡ç†å™¨å®ä¾‹
_hook_manager = None

def get_hook_manager(config: Dict[str, Any] = None) -> ComprehensiveHookManagerV4:
    """è·å–Hookç®¡ç†å™¨å®ä¾‹"""
    global _hook_manager
    if _hook_manager is None:
        _hook_manager = ComprehensiveHookManagerV4(config)
    return _hook_manager

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_hook_manager():
        manager = get_hook_manager()
        
        # æµ‹è¯•Hookå‡½æ•°
        async def test_hook1(context):
            print(f"Hook1æ‰§è¡Œ: {context}")
            context["hook1_executed"] = True
            return context
        
        async def test_hook2(context):
            print(f"Hook2æ‰§è¡Œ: {context}")
            context["hook2_executed"] = True
            return context
        
        # æ³¨å†ŒHook
        await manager.register_hook("test_point", test_hook1, priority=10)
        await manager.register_hook("test_point", test_hook2, priority=20)
        
        # æ‰§è¡ŒHook
        context = {"test": "data"}
        result = await manager.execute_hooks("test_point", context)
        print(f"æœ€ç»ˆä¸Šä¸‹æ–‡: {result}")
        
        # æ¸…ç†
        await manager.cleanup()
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_hook_manager())
'''
    
    try:
        with open(target_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print("âœ… å·²åˆ›å»ºçœŸæ­£çš„Hookç®¡ç†å™¨")
    except Exception as e:
        print(f"âŒ åˆ›å»ºHookç®¡ç†å™¨å¤±è´¥: {e}")

def generate_cleanup_report(removed_count, error_count):
    """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
    report = {
        "cleanup_date": datetime.now().isoformat(),
        "removed_files": removed_count,
        "failed_operations": error_count,
        "backup_location": str(BACKUP_DIR),
        "total_savings": sum(item['size'] for item in analysis["files_to_remove"]) if 'analysis' in locals() else 0
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = BACKUP_DIR / "cleanup_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print(f"ğŸ“Š æ¸…ç†ç»Ÿè®¡:")
    print(f"  - åˆ é™¤æ–‡ä»¶: {removed_count}ä¸ª")
    print(f"  - å¤±è´¥æ“ä½œ: {error_count}ä¸ª")
    print(f"  - å¤‡ä»½ä½ç½®: {BACKUP_DIR}")
    print(f"  - èŠ‚çœç©ºé—´: {report['total_savings']} bytes")

def main():
    """ä¸»å‡½æ•°"""
    print("ä¿å®ˆæ¸…ç†é‡å¤æ–‡ä»¶è„šæœ¬")
    print("=" * 60)
    
    # åˆ›å»ºå¤‡ä»½
    if not create_backup():
        print("âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥ï¼Œé€€å‡º")
        return False
    
    # åˆ†æé‡å¤æ–‡ä»¶
    analysis = analyze_duplicates()
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    show_analysis(analysis)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é‡å¤æ–‡ä»¶
    if not analysis["files_to_remove"]:
        print("âœ… æ²¡æœ‰å‘ç°éœ€è¦æ¸…ç†çš„é‡å¤æ–‡ä»¶")
        return True
    
    # ç¡®è®¤æ¸…ç†
    if not confirm_cleanup():
        print("ğŸ›‘ ç”¨æˆ·å–æ¶ˆæ¸…ç†æ“ä½œ")
        return True
    
    # æ‰§è¡Œæ¸…ç†
    removed_count, error_count = execute_cleanup(analysis)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_cleanup_report(removed_count, error_count)
    
    print("\nğŸ‰ æ¸…ç†å®Œæˆï¼")
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)