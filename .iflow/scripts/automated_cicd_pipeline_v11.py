#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”„ è‡ªåŠ¨åŒ–CI/CDæµæ°´çº¿ V11 (ä»£å·ï¼š"å®ˆæŠ¤è€…ä¹‹è½®")
==========================================================

æœ¬æ–‡ä»¶æ˜¯ T-MIA å‡¤å‡°æ¶æ„ä¸‹çš„è‡ªåŠ¨åŒ–CI/CDæµæ°´çº¿å®ç°ï¼Œæä¾›ï¼š
- è‡ªåŠ¨åŒ–ä»£ç è´¨é‡æ£€æŸ¥
- è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œ
- è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹
- æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦
- å›æ»šæœºåˆ¶

ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚

ä½œè€…: AIæ¶æ„å¸ˆå›¢é˜Ÿ
ç‰ˆæœ¬: 11.0.0 (ä»£å·ï¼š"å®ˆæŠ¤è€…ä¹‹è½®")
æ—¥æœŸ: 2025-11-15
"""

import os
import sys
import json
import asyncio
import logging
import subprocess
import time
import shutil
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum

# --- åŠ¨æ€è·¯å¾„è®¾ç½® ---
try:
    PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
    if str(PROJECT_ROOT) not in sys.path:
        sys.path.insert(0, str(PROJECT_ROOT))
except Exception as e:
    PROJECT_ROOT = Path.cwd()
    print(f"è­¦å‘Š: è·¯å¾„è§£æå¤±è´¥ï¼Œå›é€€åˆ°å½“å‰å·¥ä½œç›®å½•: {PROJECT_ROOT}. é”™è¯¯: {e}")

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("AutomatedCICDPipelineV11")

# --- æšä¸¾å®šä¹‰ ---
class PipelineStage(Enum):
    """æµæ°´çº¿é˜¶æ®µ"""
    INITIALIZATION = "initialization"
    CODE_QUALITY_CHECK = "code_quality_check"
    UNIT_TEST = "unit_test"
    INTEGRATION_TEST = "integration_test"
    PERFORMANCE_TEST = "performance_test"
    SECURITY_TEST = "security_test"
    BUILD = "build"
    DEPLOY_STAGING = "deploy_staging"
    STAGING_VALIDATION = "staging_validation"
    DEPLOY_PRODUCTION = "deploy_production"
    PRODUCTION_VALIDATION = "production_validation"

class DeploymentStatus(Enum):
    """éƒ¨ç½²çŠ¶æ€"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    SUCCESS = "success"
    FAILED = "failed"
    ROLLED_BACK = "rolled_back"

# --- æ•°æ®ç»“æ„å®šä¹‰ ---
@dataclass
class PipelineConfig:
    """æµæ°´çº¿é…ç½®"""
    project_name: str
    version: str
    environment: str  # development, staging, production
    auto_deploy: bool = False
    rollback_on_failure: bool = True
    notification_enabled: bool = True
    test_threshold: float = 0.95  # æµ‹è¯•é€šè¿‡ç‡é˜ˆå€¼
    performance_threshold: float = 0.9  # æ€§èƒ½æµ‹è¯•é˜ˆå€¼
    security_threshold: float = 0.95  # å®‰å…¨æµ‹è¯•é˜ˆå€¼

@dataclass
class StageResult:
    """é˜¶æ®µæ‰§è¡Œç»“æœ"""
    stage: PipelineStage
    status: str  # 'success', 'failed', 'skipped'
    execution_time: float
    output: str
    error_message: Optional[str] = None
    metrics: Dict[str, Any] = field(default_factory=dict)
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class PipelineExecution:
    """æµæ°´çº¿æ‰§è¡Œè®°å½•"""
    execution_id: str
    config: PipelineConfig
    stage_results: List[StageResult] = field(default_factory=list)
    overall_status: str = "pending"
    start_time: str = field(default_factory=lambda: datetime.now().isoformat())
    end_time: Optional[str] = None
    total_time: float = 0.0

class AutomatedCICDPipelineV11:
    """è‡ªåŠ¨åŒ–CI/CDæµæ°´çº¿ V11 å®ç°"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.current_execution: Optional[PipelineExecution] = None
        self.work_dir = PROJECT_ROOT
        self.backup_dir = PROJECT_ROOT / ".iflow" / "backups"
        self.reports_dir = PROJECT_ROOT / ".iflow" / "reports"
        self.deployments_dir = PROJECT_ROOT / ".iflow" / "deployments"
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        for directory in [self.backup_dir, self.reports_dir, self.deployments_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"AutomatedCICDPipelineV11 åˆå§‹åŒ–å®Œæˆï¼Œé¡¹ç›®: {config.project_name}")
    
    async def execute_pipeline(self) -> PipelineExecution:
        """
        æ‰§è¡Œå®Œæ•´çš„CI/CDæµæ°´çº¿
        ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡ŒCI/CDæµæ°´çº¿ - é¡¹ç›®: {self.config.project_name}, ç‰ˆæœ¬: {self.config.version}")
        
        # åˆ›å»ºæ‰§è¡Œè®°å½•
        self.current_execution = PipelineExecution(
            execution_id=f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(self.config.project_name) % 10000}",
            config=self.config
        )
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œå„ä¸ªé˜¶æ®µ
            stages = [
                (PipelineStage.INITIALIZATION, self._stage_initialization),
                (PipelineStage.CODE_QUALITY_CHECK, self._stage_code_quality_check),
                (PipelineStage.UNIT_TEST, self._stage_unit_test),
                (PipelineStage.INTEGRATION_TEST, self._stage_integration_test),
                (PipelineStage.PERFORMANCE_TEST, self._stage_performance_test),
                (PipelineStage.SECURITY_TEST, self._stage_security_test),
                (PipelineStage.BUILD, self._stage_build),
            ]
            
            # æ ¹æ®ç¯å¢ƒå†³å®šæ˜¯å¦æ‰§è¡Œéƒ¨ç½²é˜¶æ®µ
            if self.config.environment in ['staging', 'production']:
                stages.extend([
                    (PipelineStage.DEPLOY_STAGING, self._stage_deploy_staging),
                    (PipelineStage.STAGING_VALIDATION, self._stage_staging_validation),
                ])
                
                if self.config.environment == 'production' and self.config.auto_deploy:
                    stages.extend([
                        (PipelineStage.DEPLOY_PRODUCTION, self._stage_deploy_production),
                        (PipelineStage.PRODUCTION_VALIDATION, self._stage_production_validation),
                    ])
            
            # æ‰§è¡Œæ‰€æœ‰é˜¶æ®µ
            for stage, stage_func in stages:
                stage_result = await stage_func()
                self.current_execution.stage_results.append(stage_result)
                
                # å¦‚æœé˜¶æ®µå¤±è´¥ï¼Œå†³å®šæ˜¯å¦ç»§ç»­
                if stage_result.status == 'failed':
                    logger.error(f"âŒ é˜¶æ®µ {stage.value} å¤±è´¥: {stage_result.error_message}")
                    
                    # å…³é”®é˜¶æ®µå¤±è´¥ï¼Œåœæ­¢æµæ°´çº¿
                    critical_stages = [
                        PipelineStage.CODE_QUALITY_CHECK,
                        PipelineStage.UNIT_TEST,
                        PipelineStage.INTEGRATION_TEST,
                        PipelineStage.BUILD
                    ]
                    
                    if stage in critical_stages:
                        self.current_execution.overall_status = 'failed'
                        break
                else:
                    logger.info(f"âœ… é˜¶æ®µ {stage.value} æˆåŠŸå®Œæˆ")
            
            # è®¾ç½®æœ€ç»ˆçŠ¶æ€
            if self.current_execution.overall_status == 'pending':
                self.current_execution.overall_status = 'success'
            
        except Exception as e:
            logger.error(f"ğŸ’¥ æµæ°´çº¿æ‰§è¡Œå¼‚å¸¸: {e}")
            self.current_execution.overall_status = 'failed'
            
            # å°è¯•å›æ»š
            if self.config.rollback_on_failure:
                await self._rollback_deployment()
        
        finally:
            # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
            self.current_execution.total_time = time.time() - start_time
            self.current_execution.end_time = datetime.now().isoformat()
            
            # ä¿å­˜æ‰§è¡Œè®°å½•
            await self._save_execution_record()
            
            # å‘é€é€šçŸ¥
            if self.config.notification_enabled:
                await self._send_notification()
        
        logger.info(f"ğŸ CI/CDæµæ°´çº¿æ‰§è¡Œå®Œæˆï¼ŒçŠ¶æ€: {self.current_execution.overall_status}")
        return self.current_execution
    
    async def _stage_initialization(self) -> StageResult:
        """åˆå§‹åŒ–é˜¶æ®µ"""
        stage = PipelineStage.INITIALIZATION
        start_time = time.time()
        
        try:
            logger.info("ğŸ”§ æ‰§è¡Œåˆå§‹åŒ–é˜¶æ®µ...")
            
            # æ£€æŸ¥å·¥ä½œç›®å½•
            if not self.work_dir.exists():
                raise Exception(f"å·¥ä½œç›®å½•ä¸å­˜åœ¨: {self.work_dir}")
            
            # æ£€æŸ¥å¿…è¦æ–‡ä»¶
            required_files = [
                ".iflow/settings.json",
                ".iflow/core/agi_core_v11.py",
                ".iflow/core/autonomous_evolution_engine_v11.py",
                ".iflow/tests/comprehensive_test_framework_v11.py"
            ]
            
            missing_files = []
            for file_path in required_files:
                if not (self.work_dir / file_path).exists():
                    missing_files.append(file_path)
            
            if missing_files:
                raise Exception(f"ç¼ºå°‘å¿…è¦æ–‡ä»¶: {', '.join(missing_files)}")
            
            # åˆ›å»ºå¤‡ä»½
            backup_path = await self._create_backup()
            
            # æ£€æŸ¥Pythonç¯å¢ƒ
            python_version = sys.version_info
            if python_version < (3, 8):
                raise Exception(f"Pythonç‰ˆæœ¬è¿‡ä½: {python_version}, éœ€è¦ >= 3.8")
            
            # æ£€æŸ¥ä¾èµ–
            dependencies = ['asyncio', 'numpy', 'psutil']
            missing_deps = []
            
            for dep in dependencies:
                try:
                    __import__(dep)
                except ImportError:
                    missing_deps.append(dep)
            
            if missing_deps:
                raise Exception(f"ç¼ºå°‘ä¾èµ–: {', '.join(missing_deps)}")
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success',
                execution_time=execution_time,
                output=f"åˆå§‹åŒ–æˆåŠŸï¼Œå¤‡ä»½è·¯å¾„: {backup_path}",
                metrics={
                    'python_version': f"{python_version.major}.{python_version.minor}.{python_version.micro}",
                    'backup_created': True,
                    'dependencies_checked': len(dependencies)
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _stage_code_quality_check(self) -> StageResult:
        """ä»£ç è´¨é‡æ£€æŸ¥é˜¶æ®µ"""
        stage = PipelineStage.CODE_QUALITY_CHECK
        start_time = time.time()
        
        try:
            logger.info("ğŸ” æ‰§è¡Œä»£ç è´¨é‡æ£€æŸ¥...")
            
            # æŸ¥æ‰¾Pythonæ–‡ä»¶
            python_files = list(self.work_dir.rglob("*.py"))
            python_files = [f for f in python_files if '.git' not in str(f) and '__pycache__' not in str(f) and 'backups' not in str(f)]
            
            if not python_files:
                raise Exception("æœªæ‰¾åˆ°Pythonæ–‡ä»¶")
            
            # ä»£ç è´¨é‡æŒ‡æ ‡
            total_lines = 0
            total_functions = 0
            total_classes = 0
            syntax_errors = 0
            style_violations = 0
            
            for file_path in python_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # è¯­æ³•æ£€æŸ¥
                    try:
                        compile(content, str(file_path), 'exec')
                    except SyntaxError as e:
                        syntax_errors += 1
                        logger.error(f"è¯­æ³•é”™è¯¯ {file_path}:{e.lineno} - {e.msg}")
                    
                    # ç»Ÿè®¡ä»£ç æŒ‡æ ‡
                    lines = content.split('\n')
                    total_lines += len(lines)
                    
                    # ç®€å•çš„å‡½æ•°å’Œç±»è®¡æ•°
                    import re
                    functions = re.findall(r'def\s+\w+', content)
                    classes = re.findall(r'class\s+\w+', content)
                    
                    total_functions += len(functions)
                    total_classes += len(classes)
                    
                    # ç®€å•çš„é£æ ¼æ£€æŸ¥ï¼ˆè¡Œé•¿åº¦ï¼‰
                    for line in lines:
                        if len(line) > 200:  # æ”¾å®½åˆ°200å­—ç¬¦é™åˆ¶
                            style_violations += 1
                
                except Exception as e:
                    logger.warning(f"æ£€æŸ¥æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            quality_score = 1.0
            if syntax_errors > 0:
                quality_score -= 0.3
            if style_violations > total_lines * 0.2:  # è¶…è¿‡20%çš„è¡Œæœ‰é£æ ¼é—®é¢˜
                quality_score -= 0.1
            
            quality_score = max(0.0, quality_score)
            
            # è´¨é‡é˜ˆå€¼æ£€æŸ¥
            quality_ok = quality_score >= 0.7 and syntax_errors == 0
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success' if quality_ok else 'failed',
                execution_time=execution_time,
                output=f"ä»£ç è´¨é‡æ£€æŸ¥å®Œæˆï¼Œè´¨é‡åˆ†æ•°: {quality_score:.2f}",
                metrics={
                    'files_checked': len(python_files),
                    'total_lines': total_lines,
                    'total_functions': total_functions,
                    'total_classes': total_classes,
                    'syntax_errors': syntax_errors,
                    'style_violations': style_violations,
                    'quality_score': quality_score,
                    'quality_threshold_met': quality_ok
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _stage_unit_test(self) -> StageResult:
        """å•å…ƒæµ‹è¯•é˜¶æ®µ"""
        stage = PipelineStage.UNIT_TEST
        start_time = time.time()
        
        try:
            logger.info("ğŸ§ª æ‰§è¡Œå•å…ƒæµ‹è¯•...")
            
            # å¯¼å…¥æµ‹è¯•æ¡†æ¶
            sys.path.insert(0, str(self.work_dir))
            try:
                from iflow.tests.comprehensive_test_framework_v11 import ComprehensiveTestFrameworkV11
            except ImportError as e:
                raise Exception(f"æ— æ³•å¯¼å…¥æµ‹è¯•æ¡†æ¶: {e}")
            
            # åˆ›å»ºæµ‹è¯•æ¡†æ¶å®ä¾‹
            test_framework = ComprehensiveTestFrameworkV11()
            
            # åªè¿è¡Œå•å…ƒæµ‹è¯•
            await test_framework._run_unit_tests()
            
            # è·å–æµ‹è¯•ç»“æœ
            unit_test_suite = test_framework.test_suites.get('unit_tests')
            
            if not unit_test_suite:
                raise Exception("å•å…ƒæµ‹è¯•å¥—ä»¶æœªæ‰§è¡Œ")
            
            # è®¡ç®—æµ‹è¯•é€šè¿‡ç‡
            success_rate = unit_test_suite.passed_tests / unit_test_suite.total_tests if unit_test_suite.total_tests > 0 else 0
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
            threshold_ok = success_rate >= self.config.test_threshold
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success' if threshold_ok else 'failed',
                execution_time=execution_time,
                output=f"å•å…ƒæµ‹è¯•å®Œæˆï¼Œé€šè¿‡ç‡: {success_rate:.2%}",
                metrics={
                    'total_tests': unit_test_suite.total_tests,
                    'passed_tests': unit_test_suite.passed_tests,
                    'failed_tests': unit_test_suite.failed_tests,
                    'error_tests': unit_test_suite.error_tests,
                    'success_rate': success_rate,
                    'threshold': self.config.test_threshold,
                    'threshold_met': threshold_ok
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _stage_integration_test(self) -> StageResult:
        """é›†æˆæµ‹è¯•é˜¶æ®µ"""
        stage = PipelineStage.INTEGRATION_TEST
        start_time = time.time()
        
        try:
            logger.info("ğŸ”— æ‰§è¡Œé›†æˆæµ‹è¯•...")
            
            # å¯¼å…¥æµ‹è¯•æ¡†æ¶
            sys.path.insert(0, str(self.work_dir))
            try:
                from iflow.tests.comprehensive_test_framework_v11 import ComprehensiveTestFrameworkV11
            except ImportError as e:
                raise Exception(f"æ— æ³•å¯¼å…¥æµ‹è¯•æ¡†æ¶: {e}")
            
            # åˆ›å»ºæµ‹è¯•æ¡†æ¶å®ä¾‹
            test_framework = ComprehensiveTestFrameworkV11()
            
            # è¿è¡Œé›†æˆæµ‹è¯•
            await test_framework._run_integration_tests()
            
            # è·å–æµ‹è¯•ç»“æœ
            integration_test_suite = test_framework.test_suites.get('integration_tests')
            
            if not integration_test_suite:
                raise Exception("é›†æˆæµ‹è¯•å¥—ä»¶æœªæ‰§è¡Œ")
            
            # è®¡ç®—æµ‹è¯•é€šè¿‡ç‡
            success_rate = integration_test_suite.passed_tests / integration_test_suite.total_tests if integration_test_suite.total_tests > 0 else 0
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
            threshold_ok = success_rate >= self.config.test_threshold
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success' if threshold_ok else 'failed',
                execution_time=execution_time,
                output=f"é›†æˆæµ‹è¯•å®Œæˆï¼Œé€šè¿‡ç‡: {success_rate:.2%}",
                metrics={
                    'total_tests': integration_test_suite.total_tests,
                    'passed_tests': integration_test_suite.passed_tests,
                    'failed_tests': integration_test_suite.failed_tests,
                    'error_tests': integration_test_suite.error_tests,
                    'success_rate': success_rate,
                    'threshold': self.config.test_threshold,
                    'threshold_met': threshold_ok
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _stage_performance_test(self) -> StageResult:
        """æ€§èƒ½æµ‹è¯•é˜¶æ®µ"""
        stage = PipelineStage.PERFORMANCE_TEST
        start_time = time.time()
        
        try:
            logger.info("âš¡ æ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
            
            # å¯¼å…¥æµ‹è¯•æ¡†æ¶
            sys.path.insert(0, str(self.work_dir))
            try:
                from iflow.tests.comprehensive_test_framework_v11 import ComprehensiveTestFrameworkV11
            except ImportError as e:
                raise Exception(f"æ— æ³•å¯¼å…¥æµ‹è¯•æ¡†æ¶: {e}")
            
            # åˆ›å»ºæµ‹è¯•æ¡†æ¶å®ä¾‹
            test_framework = ComprehensiveTestFrameworkV11()
            
            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            await test_framework._run_performance_tests()
            
            # è·å–æµ‹è¯•ç»“æœ
            performance_test_suite = test_framework.test_suites.get('performance_tests')
            
            if not performance_test_suite:
                raise Exception("æ€§èƒ½æµ‹è¯•å¥—ä»¶æœªæ‰§è¡Œ")
            
            # è®¡ç®—æµ‹è¯•é€šè¿‡ç‡
            success_rate = performance_test_suite.passed_tests / performance_test_suite.total_tests if performance_test_suite.total_tests > 0 else 0
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
            threshold_ok = success_rate >= self.config.performance_threshold
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success' if threshold_ok else 'failed',
                execution_time=execution_time,
                output=f"æ€§èƒ½æµ‹è¯•å®Œæˆï¼Œé€šè¿‡ç‡: {success_rate:.2%}",
                metrics={
                    'total_tests': performance_test_suite.total_tests,
                    'passed_tests': performance_test_suite.passed_tests,
                    'failed_tests': performance_test_suite.failed_tests,
                    'error_tests': performance_test_suite.error_tests,
                    'success_rate': success_rate,
                    'threshold': self.config.performance_threshold,
                    'threshold_met': threshold_ok
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _stage_security_test(self) -> StageResult:
        """å®‰å…¨æµ‹è¯•é˜¶æ®µ"""
        stage = PipelineStage.SECURITY_TEST
        start_time = time.time()
        
        try:
            logger.info("ğŸ›¡ï¸ æ‰§è¡Œå®‰å…¨æµ‹è¯•...")
            
            # å¯¼å…¥æµ‹è¯•æ¡†æ¶
            sys.path.insert(0, str(self.work_dir))
            try:
                from iflow.tests.comprehensive_test_framework_v11 import ComprehensiveTestFrameworkV11
            except ImportError as e:
                raise Exception(f"æ— æ³•å¯¼å…¥æµ‹è¯•æ¡†æ¶: {e}")
            
            # åˆ›å»ºæµ‹è¯•æ¡†æ¶å®ä¾‹
            test_framework = ComprehensiveTestFrameworkV11()
            
            # è¿è¡Œå®‰å…¨æµ‹è¯•
            await test_framework._run_security_tests()
            
            # è·å–æµ‹è¯•ç»“æœ
            security_test_suite = test_framework.test_suites.get('security_tests')
            
            if not security_test_suite:
                raise Exception("å®‰å…¨æµ‹è¯•å¥—ä»¶æœªæ‰§è¡Œ")
            
            # è®¡ç®—æµ‹è¯•é€šè¿‡ç‡
            success_rate = security_test_suite.passed_tests / security_test_suite.total_tests if security_test_suite.total_tests > 0 else 0
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
            threshold_ok = success_rate >= self.config.security_threshold
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success' if threshold_ok else 'failed',
                execution_time=execution_time,
                output=f"å®‰å…¨æµ‹è¯•å®Œæˆï¼Œé€šè¿‡ç‡: {success_rate:.2%}",
                metrics={
                    'total_tests': security_test_suite.total_tests,
                    'passed_tests': security_test_suite.passed_tests,
                    'failed_tests': security_test_suite.failed_tests,
                    'error_tests': security_test_suite.error_tests,
                    'success_rate': success_rate,
                    'threshold': self.config.security_threshold,
                    'threshold_met': threshold_ok
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _stage_build(self) -> StageResult:
        """æ„å»ºé˜¶æ®µ"""
        stage = PipelineStage.BUILD
        start_time = time.time()
        
        try:
            logger.info("ğŸ”¨ æ‰§è¡Œæ„å»ºé˜¶æ®µ...")
            
            # åˆ›å»ºæ„å»ºç›®å½•
            build_dir = self.deployments_dir / f"build_{self.config.version}"
            build_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶å¿…è¦æ–‡ä»¶åˆ°æ„å»ºç›®å½•
            essential_dirs = [
                ".iflow/core",
                ".iflow/tests",
                ".iflow/commands",
                ".iflow/settings.json"
            ]
            
            copied_items = 0
            
            for item in essential_dirs:
                source = self.work_dir / item
                target = build_dir / item
                
                if source.exists():
                    if source.is_dir():
                        shutil.copytree(source, target, dirs_exist_ok=True)
                    else:
                        shutil.copy2(source, target)
                    copied_items += 1
                else:
                    logger.warning(f"æ„å»ºé¡¹ä¸å­˜åœ¨: {item}")
            
            # åˆ›å»ºéƒ¨ç½²æ¸…å•
            deployment_manifest = {
                'project_name': self.config.project_name,
                'version': self.config.version,
                'build_time': datetime.now().isoformat(),
                'environment': self.config.environment,
                'copied_items': copied_items,
                'build_directory': str(build_dir),
                'files': []
            }
            
            # åˆ—å‡ºæ„å»ºæ–‡ä»¶
            for file_path in build_dir.rglob("*"):
                if file_path.is_file():
                    rel_path = file_path.relative_to(build_dir)
                    file_size = file_path.stat().st_size
                    file_hash = self._calculate_file_hash(file_path)
                    
                    deployment_manifest['files'].append({
                        'path': str(rel_path),
                        'size': file_size,
                        'hash': file_hash
                    })
            
            # ä¿å­˜éƒ¨ç½²æ¸…å•
            manifest_path = build_dir / "deployment_manifest.json"
            with open(manifest_path, 'w', encoding='utf-8') as f:
                json.dump(deployment_manifest, f, indent=2, ensure_ascii=False)
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success',
                execution_time=execution_time,
                output=f"æ„å»ºå®Œæˆï¼Œæ„å»ºç›®å½•: {build_dir}",
                metrics={
                    'build_directory': str(build_dir),
                    'copied_items': copied_items,
                    'total_files': len(deployment_manifest['files']),
                    'total_size': sum(f['size'] for f in deployment_manifest['files'])
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _stage_deploy_staging(self) -> StageResult:
        """éƒ¨ç½²åˆ°é¢„å‘å¸ƒç¯å¢ƒ"""
        stage = PipelineStage.DEPLOY_STAGING
        start_time = time.time()
        
        try:
            logger.info("ğŸš€ éƒ¨ç½²åˆ°é¢„å‘å¸ƒç¯å¢ƒ...")
            
            # æ¨¡æ‹Ÿéƒ¨ç½²è¿‡ç¨‹
            staging_dir = self.deployments_dir / "staging"
            staging_dir.mkdir(parents=True, exist_ok=True)
            
            # æŸ¥æ‰¾æ„å»ºç›®å½•
            build_dir = self.deployments_dir / f"build_{self.config.version}"
            if not build_dir.exists():
                raise Exception(f"æ„å»ºç›®å½•ä¸å­˜åœ¨: {build_dir}")
            
            # å¤åˆ¶æ„å»ºæ–‡ä»¶åˆ°é¢„å‘å¸ƒç¯å¢ƒ
            if staging_dir.exists():
                shutil.rmtree(staging_dir)
            shutil.copytree(build_dir, staging_dir)
            
            # åˆ›å»ºéƒ¨ç½²æ ‡è®°
            deployment_marker = {
                'deployment_id': f"staging_{self.config.version}_{int(time.time())}",
                'project_name': self.config.project_name,
                'version': self.config.version,
                'environment': 'staging',
                'deployment_time': datetime.now().isoformat(),
                'status': DeploymentStatus.SUCCESS.value
            }
            
            marker_path = staging_dir / "deployment_marker.json"
            with open(marker_path, 'w', encoding='utf-8') as f:
                json.dump(deployment_marker, f, indent=2, ensure_ascii=False)
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success',
                execution_time=execution_time,
                output=f"æˆåŠŸéƒ¨ç½²åˆ°é¢„å‘å¸ƒç¯å¢ƒï¼Œéƒ¨ç½²ID: {deployment_marker['deployment_id']}",
                metrics={
                    'deployment_id': deployment_marker['deployment_id'],
                    'staging_directory': str(staging_dir),
                    'deployment_status': deployment_marker['status']
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _stage_staging_validation(self) -> StageResult:
        """é¢„å‘å¸ƒç¯å¢ƒéªŒè¯"""
        stage = PipelineStage.STAGING_VALIDATION
        start_time = time.time()
        
        try:
            logger.info("âœ… éªŒè¯é¢„å‘å¸ƒç¯å¢ƒ...")
            
            staging_dir = self.deployments_dir / "staging"
            
            # æ£€æŸ¥éƒ¨ç½²æ ‡è®°
            marker_path = staging_dir / "deployment_marker.json"
            if not marker_path.exists():
                raise Exception("éƒ¨ç½²æ ‡è®°æ–‡ä»¶ä¸å­˜åœ¨")
            
            with open(marker_path, 'r', encoding='utf-8') as f:
                deployment_marker = json.load(f)
            
            # éªŒè¯éƒ¨ç½²å®Œæ•´æ€§
            manifest_path = staging_dir / "deployment_manifest.json"
            if not manifest_path.exists():
                raise Exception("éƒ¨ç½²æ¸…å•æ–‡ä»¶ä¸å­˜åœ¨")
            
            with open(manifest_path, 'r', encoding='utf-8') as f:
                deployment_manifest = json.load(f)
            
            # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
            missing_files = []
            corrupted_files = []
            
            for file_info in deployment_manifest['files']:
                file_path = staging_dir / file_info['path']
                
                if not file_path.exists():
                    missing_files.append(file_info['path'])
                else:
                    # éªŒè¯æ–‡ä»¶å“ˆå¸Œ
                    current_hash = self._calculate_file_hash(file_path)
                    if current_hash != file_info['hash']:
                        corrupted_files.append(file_info['path'])
            
            # éªŒè¯ç»“æœ
            validation_ok = len(missing_files) == 0 and len(corrupted_files) == 0
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success' if validation_ok else 'failed',
                execution_time=execution_time,
                output=f"é¢„å‘å¸ƒç¯å¢ƒéªŒè¯å®Œæˆï¼Œç¼ºå¤±æ–‡ä»¶: {len(missing_files)}ï¼ŒæŸåæ–‡ä»¶: {len(corrupted_files)}",
                metrics={
                    'deployment_id': deployment_marker['deployment_id'],
                    'total_files': len(deployment_manifest['files']),
                    'missing_files': len(missing_files),
                    'corrupted_files': len(corrupted_files),
                    'validation_passed': validation_ok
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _stage_deploy_production(self) -> StageResult:
        """éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"""
        stage = PipelineStage.DEPLOY_PRODUCTION
        start_time = time.time()
        
        try:
            logger.info("ğŸš€ éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ...")
            
            # æ¨¡æ‹Ÿç”Ÿäº§éƒ¨ç½²ï¼ˆå®é™…ç¯å¢ƒä¸­åº”è¯¥æ›´è°¨æ…ï¼‰
            production_dir = self.deployments_dir / "production"
            production_dir.mkdir(parents=True, exist_ok=True)
            
            # æŸ¥æ‰¾é¢„å‘å¸ƒç¯å¢ƒ
            staging_dir = self.deployments_dir / "staging"
            if not staging_dir.exists():
                raise Exception(f"é¢„å‘å¸ƒç¯å¢ƒä¸å­˜åœ¨: {staging_dir}")
            
            # å¤‡ä»½å½“å‰ç”Ÿäº§ç¯å¢ƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            current_production_backup = None
            if production_dir.exists() and any(production_dir.iterdir()):
                backup_name = f"production_backup_{int(time.time())}"
                backup_path = self.backup_dir / backup_name
                shutil.copytree(production_dir, backup_path)
                current_production_backup = str(backup_path)
            
            # å¤åˆ¶é¢„å‘å¸ƒç¯å¢ƒåˆ°ç”Ÿäº§ç¯å¢ƒ
            if production_dir.exists():
                shutil.rmtree(production_dir)
            shutil.copytree(staging_dir, production_dir)
            
            # åˆ›å»ºç”Ÿäº§éƒ¨ç½²æ ‡è®°
            deployment_marker = {
                'deployment_id': f"production_{self.config.version}_{int(time.time())}",
                'project_name': self.config.project_name,
                'version': self.config.version,
                'environment': 'production',
                'deployment_time': datetime.now().isoformat(),
                'status': DeploymentStatus.SUCCESS.value,
                'backup_path': current_production_backup
            }
            
            marker_path = production_dir / "deployment_marker.json"
            with open(marker_path, 'w', encoding='utf-8') as f:
                json.dump(deployment_marker, f, indent=2, ensure_ascii=False)
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success',
                execution_time=execution_time,
                output=f"æˆåŠŸéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œéƒ¨ç½²ID: {deployment_marker['deployment_id']}",
                metrics={
                    'deployment_id': deployment_marker['deployment_id'],
                    'production_directory': str(production_dir),
                    'backup_path': current_production_backup,
                    'deployment_status': deployment_marker['status']
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _stage_production_validation(self) -> StageResult:
        """ç”Ÿäº§ç¯å¢ƒéªŒè¯"""
        stage = PipelineStage.PRODUCTION_VALIDATION
        start_time = time.time()
        
        try:
            logger.info("âœ… éªŒè¯ç”Ÿäº§ç¯å¢ƒ...")
            
            production_dir = self.deployments_dir / "production"
            
            # æ£€æŸ¥éƒ¨ç½²æ ‡è®°
            marker_path = production_dir / "deployment_marker.json"
            if not marker_path.exists():
                raise Exception("ç”Ÿäº§éƒ¨ç½²æ ‡è®°æ–‡ä»¶ä¸å­˜åœ¨")
            
            with open(marker_path, 'r', encoding='utf-8') as f:
                deployment_marker = json.load(f)
            
            # åŸºæœ¬å¥åº·æ£€æŸ¥
            health_check = {
                'deployment_accessible': True,
                'core_modules_loadable': True,
                'basic_functionality': True
            }
            
            # æ¨¡æ‹Ÿå¥åº·æ£€æŸ¥
            try:
                # æ£€æŸ¥æ ¸å¿ƒæ¨¡å—æ˜¯å¦å¯åŠ è½½
                sys.path.insert(0, str(production_dir))
                # è¿™é‡Œåº”è¯¥å°è¯•å¯¼å…¥æ ¸å¿ƒæ¨¡å—è¿›è¡ŒéªŒè¯
                # ç”±äºæ˜¯æ¨¡æ‹Ÿï¼Œæˆ‘ä»¬è·³è¿‡å®é™…å¯¼å…¥
            except Exception as e:
                health_check['core_modules_loadable'] = False
                logger.warning(f"æ ¸å¿ƒæ¨¡å—åŠ è½½æ£€æŸ¥å¤±è´¥: {e}")
            
            # éªŒè¯ç»“æœ
            validation_ok = all(health_check.values())
            
            execution_time = time.time() - start_time
            
            return StageResult(
                stage=stage,
                status='success' if validation_ok else 'failed',
                execution_time=execution_time,
                output=f"ç”Ÿäº§ç¯å¢ƒéªŒè¯å®Œæˆï¼Œå¥åº·æ£€æŸ¥: {health_check}",
                metrics={
                    'deployment_id': deployment_marker['deployment_id'],
                    'health_check': health_check,
                    'validation_passed': validation_ok
                }
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return StageResult(
                stage=stage,
                status='failed',
                execution_time=execution_time,
                output="",
                error_message=str(e)
            )
    
    async def _create_backup(self) -> str:
        """åˆ›å»ºå¤‡ä»½"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"backup_{self.config.project_name}_{timestamp}"
        backup_path = self.backup_dir / backup_name
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_path.mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶é‡è¦æ–‡ä»¶
        important_items = [
            ".iflow/core",
            ".iflow/settings.json",
            ".iflow/tests"
        ]
        
        for item in important_items:
            source = self.work_dir / item
            target = backup_path / item
            
            if source.exists():
                if source.is_dir():
                    shutil.copytree(source, target, dirs_exist_ok=True)
                else:
                    shutil.copy2(source, target)
        
        return str(backup_path)
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    async def _rollback_deployment(self):
        """å›æ»šéƒ¨ç½²"""
        logger.warning("ğŸ”„ å¼€å§‹å›æ»šéƒ¨ç½²...")
        
        try:
            # æŸ¥æ‰¾æœ€è¿‘çš„å¤‡ä»½
            production_dir = self.deployments_dir / "production"
            
            if production_dir.exists():
                marker_path = production_dir / "deployment_marker.json"
                
                if marker_path.exists():
                    with open(marker_path, 'r', encoding='utf-8') as f:
                        deployment_marker = json.load(f)
                    
                    backup_path = deployment_marker.get('backup_path')
                    
                    if backup_path and Path(backup_path).exists():
                        # æ¢å¤å¤‡ä»½
                        shutil.rmtree(production_dir)
                        shutil.copytree(backup_path, production_dir)
                        
                        # æ›´æ–°éƒ¨ç½²æ ‡è®°
                        deployment_marker['status'] = DeploymentStatus.ROLLED_BACK.value
                        deployment_marker['rollback_time'] = datetime.now().isoformat()
                        
                        with open(marker_path, 'w', encoding='utf-8') as f:
                            json.dump(deployment_marker, f, indent=2, ensure_ascii=False)
                        
                        logger.info(f"âœ… éƒ¨ç½²å·²å›æ»šåˆ°å¤‡ä»½: {backup_path}")
                    else:
                        logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„å¤‡ä»½æ–‡ä»¶")
                else:
                    logger.warning("æœªæ‰¾åˆ°éƒ¨ç½²æ ‡è®°æ–‡ä»¶")
            else:
                logger.warning("ç”Ÿäº§ç¯å¢ƒç›®å½•ä¸å­˜åœ¨")
        
        except Exception as e:
            logger.error(f"å›æ»šéƒ¨ç½²å¤±è´¥: {e}")
    
    async def _save_execution_record(self):
        """ä¿å­˜æ‰§è¡Œè®°å½•"""
        if not self.current_execution:
            return
        
        record_file = self.reports_dir / f"pipeline_execution_{self.current_execution.execution_id}.json"
        
        try:
            with open(record_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(self.current_execution), f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ“„ æ‰§è¡Œè®°å½•å·²ä¿å­˜è‡³: {record_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ‰§è¡Œè®°å½•å¤±è´¥: {e}")
    
    async def _send_notification(self):
        """å‘é€é€šçŸ¥"""
        if not self.current_execution:
            return
        
        # æ¨¡æ‹Ÿé€šçŸ¥å‘é€
        logger.info("ğŸ“¢ å‘é€æ‰§è¡Œé€šçŸ¥...")
        
        notification = {
            'project': self.config.project_name,
            'version': self.config.version,
            'execution_id': self.current_execution.execution_id,
            'status': self.current_execution.overall_status,
            'total_time': self.current_execution.total_time,
            'stages': len(self.current_execution.stage_results)
        }
        
        # è¿™é‡Œåº”è¯¥é›†æˆå®é™…çš„é€šçŸ¥ç³»ç»Ÿï¼ˆé‚®ä»¶ã€Slackç­‰ï¼‰
        logger.info(f"ğŸ“§ é€šçŸ¥å†…å®¹: {notification}")

# --- ä¸»å‡½æ•° ---
async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨è‡ªåŠ¨åŒ–CI/CDæµæ°´çº¿ V11")
    
    # åˆ›å»ºé…ç½®
    config = PipelineConfig(
        project_name="iflow-cli-workflow",
        version="11.0.0",
        environment="staging",  # development, staging, production
        auto_deploy=False,  # ç”Ÿäº§ç¯å¢ƒè‡ªåŠ¨éƒ¨ç½²
        rollback_on_failure=True,
        notification_enabled=True,
        test_threshold=0.95,
        performance_threshold=0.9,
        security_threshold=0.95
    )
    
    # åˆ›å»ºå¹¶æ‰§è¡Œæµæ°´çº¿
    pipeline = AutomatedCICDPipelineV11(config)
    execution_result = await pipeline.execute_pipeline()
    
    # è¾“å‡ºæ‰§è¡Œæ‘˜è¦
    logger.info(f"ğŸ“Š æµæ°´çº¿æ‰§è¡Œæ‘˜è¦:")
    logger.info(f"  æ‰§è¡ŒID: {execution_result.execution_id}")
    logger.info(f"  æ€»ä½“çŠ¶æ€: {execution_result.overall_status}")
    logger.info(f"  æ€»æ‰§è¡Œæ—¶é—´: {execution_result.total_time:.2f}ç§’")
    logger.info(f"  æ‰§è¡Œé˜¶æ®µæ•°: {len(execution_result.stage_results)}")
    
    for stage_result in execution_result.stage_results:
        logger.info(f"  {stage_result.stage.value}: {stage_result.status} ({stage_result.execution_time:.2f}s)")
    
    logger.info("âœ… è‡ªåŠ¨åŒ–CI/CDæµæ°´çº¿æ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())
