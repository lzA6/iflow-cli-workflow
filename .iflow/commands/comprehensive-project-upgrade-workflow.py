#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ å…¨è‡ªåŠ¨åŒ–é¡¹ç›®å®¡æŸ¥å’Œå‡çº§å·¥ä½œæµ
Comprehensive Project Upgrade Workflow (CPUW)

è¿™æ˜¯iFlow CLIçš„æ——èˆ°çº§è‡ªåŠ¨åŒ–å·¥ä½œæµï¼Œæä¾›å…¨æ–¹ä½çš„é¡¹ç›®å®¡æŸ¥ã€å‡çº§ã€ä¼˜åŒ–å’Œè¿­ä»£åŠŸèƒ½ã€‚
é›†æˆAIé©±åŠ¨çš„æ™ºèƒ½åˆ†æã€è‡ªåŠ¨ä¿®å¤ã€æ€§èƒ½ä¼˜åŒ–ã€æ–‡æ¡£ç”Ÿæˆå’ŒæŒç»­å­¦ä¹ èƒ½åŠ›ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- ğŸ” å…¨æ–¹ä½é¡¹ç›®ç»“æ„æ·±åº¦åˆ†æ
- ğŸ› ï¸ ä»£ç è´¨é‡è‡ªåŠ¨å®¡æŸ¥å’Œä¿®å¤
- ğŸ“ˆ è‡ªåŠ¨ç‰ˆæœ¬è¿­ä»£å’Œæ™ºèƒ½å‡çº§
- âš¡ æ€§èƒ½ä¼˜åŒ–å’Œè‡ªåŠ¨æµ‹è¯•
- ğŸ“š æ™ºèƒ½æ–‡æ¡£ç”Ÿæˆå’Œæ€»ç»“
- ğŸ—‘ï¸ è‡ªåŠ¨æ¸…ç†æ—§ä»£ç å’Œæ–‡ä»¶
- ğŸ“Š å·®å¼‚åŒ–æŠ¥å‘Šå’Œå‡çº§æ—¥å¿—
- ğŸ—ï¸ é¡¹ç›®æ¶æ„æ·±åº¦åˆ†æ
- ğŸ§  AIè®­ç»ƒæ•°æ®é›†ç”Ÿæˆå’Œåå¥½å­¦ä¹ 
- ğŸ”„ æŒç»­è¿›åŒ–å’Œè‡ªæˆ‘å®Œå–„

ä½œè€…: AIæ¶æ„å¸ˆå›¢é˜Ÿ
ç‰ˆæœ¬: 1.0.0 Ultimate
æ—¥æœŸ: 2025-11-16
"""

import os
import sys
import json
import time
import asyncio
import logging
import argparse
import hashlib
import shutil
import subprocess
import tempfile
import difflib
import re
import ast
# import yaml  # æ³¨é‡Šæ‰å¯é€‰ä¾èµ–
# import toml   # æ³¨é‡Šæ‰å¯é€‰ä¾èµ–
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from enum import Enum
# import git  # æ³¨é‡Šæ‰ï¼Œé¿å…ä¾èµ–é—®é¢˜

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
try:
    from arq_reasoning_engine_v15_quantum import get_arq_engine_v15_quantum
    from hrrk_kernel_v2 import HRRKKernelV2
    from knowledge_base_manager import KnowledgeBaseManager
    from knowledge_base_ai_enhancer import get_ai_enhancer
except ImportError as e:
    print(f"âš ï¸ æ ¸å¿ƒç»„ä»¶å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºç¡€åŠŸèƒ½: {e}")

# å¯é€‰ä¾èµ–å¤„ç†
try:
    import toml
except ImportError:
    toml = None

try:
    import yaml
except ImportError:
    yaml = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class UpgradePhase(Enum):
    """å‡çº§é˜¶æ®µæšä¸¾"""
    ANALYSIS = "analysis"
    PLANNING = "planning"
    EXECUTION = "execution"
    VALIDATION = "validation"
    DOCUMENTATION = "documentation"
    CLEANUP = "cleanup"

class Severity(Enum):
    """é—®é¢˜ä¸¥é‡ç¨‹åº¦"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"

@dataclass
class ProjectMetrics:
    """é¡¹ç›®æŒ‡æ ‡"""
    total_files: int = 0
    code_files: int = 0
    test_files: int = 0
    doc_files: int = 0
    config_files: int = 0
    total_lines: int = 0
    code_lines: int = 0
    comment_lines: int = 0
    blank_lines: int = 0
    complexity_score: float = 0.0
    maintainability_index: float = 0.0
    test_coverage: float = 0.0
    security_score: float = 0.0
    performance_score: float = 0.0

@dataclass
class Issue:
    """é—®é¢˜è®°å½•"""
    id: str
    type: str
    severity: Severity
    title: str
    description: str
    file_path: str
    line_number: int
    evidence: str
    fix_suggestion: str
    auto_fixable: bool
    category: str
    impact: str
    effort: str

@dataclass
class UpgradeAction:
    """å‡çº§åŠ¨ä½œ"""
    id: str
    type: str
    description: str
    file_path: str
    changes: Dict[str, Any]
    priority: str
    risk_level: str
    estimated_time: int
    dependencies: List[str]

@dataclass
class AIProfile:
    """AIç”¨æˆ·åå¥½æ¡£æ¡ˆ"""
    coding_style: Dict[str, Any]
    preferred_patterns: List[str]
    avoided_patterns: List[str]
    framework_preferences: Dict[str, Any]
    documentation_style: str
    testing_approach: str
    optimization_focus: List[str]
    security_priorities: List[str]
    performance_targets: Dict[str, float]
    architectural_preferences: Dict[str, Any]

class ComprehensiveProjectUpgradeWorkflow:
    """å…¨è‡ªåŠ¨åŒ–é¡¹ç›®å®¡æŸ¥å’Œå‡çº§å·¥ä½œæµ"""
    
    def __init__(self, workspace_path: str, config: Optional[Dict] = None):
        self.workspace_path = Path(workspace_path)
        self.config = config or {}
        
        # æ ¸å¿ƒç»„ä»¶
        self.arq_engine = None
        self.hrrk_kernel = None
        self.knowledge_base = None
        self.ai_enhancer = None
        
        # å·¥ä½œæµçŠ¶æ€
        self.current_phase = UpgradePhase.ANALYSIS
        self.start_time = time.time()
        self.session_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # åˆ†æç»“æœ
        self.project_metrics = ProjectMetrics()
        self.issues: List[Issue] = []
        self.upgrade_actions: List[UpgradeAction] = []
        self.architecture_analysis = {}
        self.performance_benchmarks = {}
        self.security_findings = []
        
        # AIå­¦ä¹ æ•°æ®
        self.ai_profile = AIProfile(
            coding_style={},
            preferred_patterns=[],
            avoided_patterns=[],
            framework_preferences={},
            documentation_style="technical",
            testing_approach="comprehensive",
            optimization_focus=["performance", "maintainability"],
            security_priorities=["authentication", "data_protection"],
            performance_targets={},
            architectural_preferences={}
        )
        
        # å‡çº§å†å²
        self.upgrade_history = []
        self.changelog = []
        self.version_info = {"current": "1.0.0", "target": "1.1.0"}
        
        # é…ç½®é€‰é¡¹
        self.auto_fix = self.config.get("auto_fix", True)
        self.backup_enabled = self.config.get("backup_enabled", True)
        self.analysis_mode = self.config.get("dry_run", False)  # analysis_mode = True è¡¨ç¤ºåªåˆ†æä¸ä¿®æ”¹
        self.verbose = self.config.get("verbose", False)
        
        # æ–‡ä»¶ç±»å‹æ˜ å°„
        self.file_extensions = {
            "code": [".py", ".js", ".ts", ".java", ".cpp", ".c", ".go", ".rs", ".php", ".rb"],
            "test": ["test_", "_test.", ".test.", "spec_", "_spec.", ".spec."],
            "doc": [".md", ".rst", ".txt", ".doc", ".docx"],
            "config": [".json", ".yaml", ".yml", ".toml", ".ini", ".cfg", ".conf"],
            "build": ["Makefile", "CMakeLists.txt", "package.json", "requirements.txt", "pyproject.toml"]
        }
        
        logger.info("ğŸš€ å…¨è‡ªåŠ¨åŒ–é¡¹ç›®å®¡æŸ¥å’Œå‡çº§å·¥ä½œæµåˆå§‹åŒ–å®Œæˆ")

    async def initialize(self):
        """åˆå§‹åŒ–å·¥ä½œæµç¯å¢ƒ"""
        logger.info("ğŸ”§ åˆå§‹åŒ–å·¥ä½œæµç¯å¢ƒ...")
        
        try:
            # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
            await self._initialize_core_components()
            
            # åˆ›å»ºå¤‡ä»½
            if self.backup_enabled and not self.analysis_mode:
                await self._create_backup()
            
            # åŠ è½½é¡¹ç›®å†å²æ•°æ®
            await self._load_project_history()
            
            # åˆå§‹åŒ–AIå­¦ä¹ ç³»ç»Ÿ
            await self._initialize_ai_learning()
            
            logger.info("âœ… å·¥ä½œæµç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def _initialize_core_components(self):
        """åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶"""
        try:
            # ARQæ¨ç†å¼•æ“
            self.arq_engine = get_arq_engine_v15_quantum()
            logger.info("  âœ… ARQæ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
            
            # HRRKå†…æ ¸
            self.hrrk_kernel = HRRKKernelV2()
            logger.info("  âœ… HRRKå†…æ ¸åˆå§‹åŒ–å®Œæˆ")
            
            # çŸ¥è¯†åº“
            self.knowledge_base = KnowledgeBaseManager()
            logger.info("  âœ… çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ")
            
            # AIå¢å¼ºå™¨
            self.ai_enhancer = get_ai_enhancer()
            logger.info("  âœ… AIå¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºç¡€åŠŸèƒ½: {e}")

    async def _create_backup(self):
        """åˆ›å»ºé¡¹ç›®å¤‡ä»½"""
        logger.info("ğŸ’¾ åˆ›å»ºé¡¹ç›®å¤‡ä»½...")
        
        try:
            backup_dir = self.workspace_path / ".iflow" / "backups" / f"upgrade_backup_{self.session_id}"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤‡ä»½é‡è¦æ–‡ä»¶
            important_patterns = [
                "*.py", "*.js", "*.ts", "*.json", "*.yaml", "*.yml", "*.toml",
                "*.md", "requirements.txt", "package.json", "pyproject.toml"
            ]
            
            for pattern in important_patterns:
                for file_path in self.workspace_path.rglob(pattern):
                    if file_path.is_file() and not any(skip in str(file_path) for skip in ['.git', '__pycache__', 'node_modules', '.venv']):
                        relative_path = file_path.relative_to(self.workspace_path)
                        backup_path = backup_dir / relative_path
                        backup_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(file_path, backup_path)
            
            logger.info(f"  âœ… å¤‡ä»½å®Œæˆ: {backup_dir}")
            
        except Exception as e:
            logger.error(f"  âŒ å¤‡ä»½å¤±è´¥: {e}")

    async def _load_project_history(self):
        """åŠ è½½é¡¹ç›®å†å²æ•°æ®"""
        logger.info("ğŸ“š åŠ è½½é¡¹ç›®å†å²æ•°æ®...")
        
        try:
            history_file = self.workspace_path / ".iflow" / "data" / "upgrade_history.json"
            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    self.upgrade_history = json.load(f)
                logger.info(f"  âœ… åŠ è½½äº† {len(self.upgrade_history)} æ¡å†å²è®°å½•")
            
            # åŠ è½½AIåå¥½æ¡£æ¡ˆ
            profile_file = self.workspace_path / ".iflow" / "data" / "ai_profile.json"
            if profile_file.exists():
                with open(profile_file, 'r', encoding='utf-8') as f:
                    profile_data = json.load(f)
                    self.ai_profile = AIProfile(**profile_data)
                logger.info("  âœ… AIåå¥½æ¡£æ¡ˆåŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ å†å²æ•°æ®åŠ è½½å¤±è´¥: {e}")

    async def _initialize_ai_learning(self):
        """åˆå§‹åŒ–AIå­¦ä¹ ç³»ç»Ÿ"""
        logger.info("ğŸ§  åˆå§‹åŒ–AIå­¦ä¹ ç³»ç»Ÿ...")
        
        try:
            # åˆ†æç°æœ‰ä»£ç æ¨¡å¼
            await self._analyze_coding_patterns()
            
            # å­¦ä¹ é¡¹ç›®æ¶æ„åå¥½
            await self._learn_architectural_preferences()
            
            # ç†è§£ç”¨æˆ·æ–‡æ¡£é£æ ¼
            await self._understand_documentation_style()
            
            logger.info("  âœ… AIå­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ AIå­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")

    async def execute_comprehensive_upgrade(self) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨é¢å‡çº§æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå…¨é¢é¡¹ç›®å‡çº§...")
        
        try:
            # é˜¶æ®µ1: æ·±åº¦åˆ†æ
            await self._phase_analysis()
            
            # é˜¶æ®µ2: æ™ºèƒ½è§„åˆ’
            await self._phase_planning()
            
            # é˜¶æ®µ3: æ‰§è¡Œå‡çº§
            await self._phase_execution()
            
            # é˜¶æ®µ4: éªŒè¯æµ‹è¯•
            await self._phase_validation()
            
            # é˜¶æ®µ5: æ–‡æ¡£ç”Ÿæˆ
            await self._phase_documentation()
            
            # é˜¶æ®µ6: æ¸…ç†ä¼˜åŒ–
            await self._phase_cleanup()
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = await self._generate_final_report()
            
            # ä¿å­˜å‡çº§å†å²
            await self._save_upgrade_history(final_report)
            
            logger.info("ğŸ‰ å…¨é¢é¡¹ç›®å‡çº§å®Œæˆ!")
            
            return final_report
            
        except Exception as e:
            logger.error(f"âŒ å‡çº§æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            raise

    async def _phase_analysis(self):
        """é˜¶æ®µ1: æ·±åº¦åˆ†æ"""
        logger.info("ğŸ“Š é˜¶æ®µ1: æ·±åº¦åˆ†æ...")
        self.current_phase = UpgradePhase.ANALYSIS
        
        try:
            # 1.1 é¡¹ç›®ç»“æ„åˆ†æ
            await self._analyze_project_structure()
            
            # 1.2 ä»£ç è´¨é‡åˆ†æ
            await self._analyze_code_quality()
            
            # 1.3 æ¶æ„åˆ†æ
            await self._analyze_architecture()
            
            # 1.4 æ€§èƒ½åˆ†æ
            await self._analyze_performance()
            
            # 1.5 å®‰å…¨åˆ†æ
            await self._analyze_security()
            
            # 1.6 ä¾èµ–åˆ†æ
            await self._analyze_dependencies()
            
            # 1.7 æµ‹è¯•è¦†ç›–ç‡åˆ†æ
            await self._analyze_test_coverage()
            
            logger.info("  âœ… æ·±åº¦åˆ†æå®Œæˆ")
            
        except Exception as e:
            logger.error(f"  âŒ æ·±åº¦åˆ†æå¤±è´¥: {e}")
            raise

    async def _analyze_project_structure(self):
        """åˆ†æé¡¹ç›®ç»“æ„"""
        logger.info("  ğŸ“ åˆ†æé¡¹ç›®ç»“æ„...")
        
        try:
            structure_analysis = {
                "directory_tree": await self._build_directory_tree(),
                "file_distribution": await self._analyze_file_distribution(),
                "module_dependencies": await self._analyze_module_dependencies(),
                "naming_conventions": await self._analyze_naming_conventions(),
                "organization_patterns": await self._analyze_organization_patterns()
            }
            
            self.architecture_analysis["project_structure"] = structure_analysis
            
            # æ›´æ–°é¡¹ç›®æŒ‡æ ‡
            self.project_metrics.total_files = sum(
                len(list(self.workspace_path.rglob(f"*{ext}")))
                for category in self.file_extensions.values()
                for ext in category
            )
            
            logger.info(f"    å‘ç° {self.project_metrics.total_files} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"    é¡¹ç›®ç»“æ„åˆ†æå¤±è´¥: {e}")

    async def _build_directory_tree(self) -> Dict[str, Any]:
        """æ„å»ºç›®å½•æ ‘"""
        def build_tree(path: Path, max_depth=3, current_depth=0):
            if current_depth >= max_depth:
                return {"type": "directory", "name": path.name, "children": "..."}
            
            tree = {"type": "directory", "name": path.name, "children": []}
            
            try:
                for item in sorted(path.iterdir()):
                    if item.name.startswith('.'):
                        continue
                    
                    if item.is_dir():
                        subtree = build_tree(item, max_depth, current_depth + 1)
                        tree["children"].append(subtree)
                    else:
                        tree["children"].append({
                            "type": "file",
                            "name": item.name,
                            "size": item.stat().st_size,
                            "extension": item.suffix
                        })
            except PermissionError:
                pass
            
            return tree
        
        return build_tree(self.workspace_path)

    async def _analyze_file_distribution(self) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶åˆ†å¸ƒ"""
        distribution = defaultdict(int)
        size_distribution = defaultdict(int)
        
        for file_path in self.workspace_path.rglob("*"):
            if file_path.is_file() and not any(skip in str(file_path) for skip in ['.git', '__pycache__', 'node_modules']):
                ext = file_path.suffix.lower()
                distribution[ext] += 1
                try:
                    size_distribution[ext] += file_path.stat().st_size
                except:
                    pass
        
        return {
            "count_by_type": dict(distribution),
            "size_by_type": dict(size_distribution),
            "total_size": sum(size_distribution.values())
        }

    async def _analyze_module_dependencies(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å—ä¾èµ–"""
        dependencies = defaultdict(set)
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ç®€å•çš„å¯¼å…¥åˆ†æ
                    imports = re.findall(r'^import\s+(\w+)|^from\s+(\w+)', content, re.MULTILINE)
                    for import_match in imports:
                        module = import_match[0] or import_match[1]
                        if module and not module.startswith('.'):
                            dependencies[str(file_path)].add(module)
                except:
                    pass
        
        return {
            "dependencies": {k: list(v) for k, v in dependencies.items()},
            "dependency_graph": self._build_dependency_graph(dependencies)
        }

    def _build_dependency_graph(self, dependencies: Dict[str, set]) -> Dict[str, List[str]]:
        """æ„å»ºä¾èµ–å›¾"""
        graph = {}
        for file_path, deps in dependencies.items():
            graph[file_path] = list(deps)
        return graph

    async def _analyze_naming_conventions(self) -> Dict[str, Any]:
        """åˆ†æå‘½åçº¦å®š"""
        conventions = {
            "file_naming": {},
            "variable_naming": {},
            "function_naming": {},
            "class_naming": {}
        }
        
        # åˆ†ææ–‡ä»¶å‘½å
        file_patterns = defaultdict(list)
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                name = file_path.stem
                if name.islower():
                    file_patterns["snake_case"].append(str(file_path))
                elif any(c.isupper() for c in name):
                    file_patterns["camel_case"].append(str(file_path))
        
        conventions["file_naming"] = dict(file_patterns)
        
        return conventions

    async def _analyze_organization_patterns(self) -> Dict[str, Any]:
        """åˆ†æç»„ç»‡æ¨¡å¼"""
        patterns = {
            "has_tests": any("test" in str(p).lower() for p in self.workspace_path.rglob("*")),
            "has_docs": any(p.suffix in ['.md', '.rst'] for p in self.workspace_path.rglob("*")),
            "has_config": any(p.name in ['config', 'settings', '.env'] for p in self.workspace_path.rglob("*")),
            "has_ci": any(p.name in ['.github', '.gitlab-ci.yml', 'travis.yml'] for p in self.workspace_path.rglob("*")),
            "package_managers": []
        }
        
        # æ£€æµ‹åŒ…ç®¡ç†å™¨
        if (self.workspace_path / "package.json").exists():
            patterns["package_managers"].append("npm")
        if (self.workspace_path / "requirements.txt").exists() or (self.workspace_path / "pyproject.toml").exists():
            patterns["package_managers"].append("pip")
        if (self.workspace_path / "Cargo.toml").exists():
            patterns["package_managers"].append("cargo")
        
        return patterns

    async def _analyze_code_quality(self):
        """åˆ†æä»£ç è´¨é‡"""
        logger.info("  ğŸ” åˆ†æä»£ç è´¨é‡...")
        
        try:
            # é™æ€ä»£ç åˆ†æ
            await self._perform_static_analysis()
            
            # å¤æ‚åº¦åˆ†æ
            await self._analyze_complexity()
            
            # å¯ç»´æŠ¤æ€§åˆ†æ
            await self._analyze_maintainability()
            
            # ä»£ç é£æ ¼åˆ†æ
            await self._analyze_code_style()
            
            # é‡å¤ä»£ç åˆ†æ
            await self._analyze_code_duplication()
            
            logger.info("    å‘ç° {} ä¸ªè´¨é‡é—®é¢˜".format(len(self.issues)))
            
        except Exception as e:
            logger.error(f"    ä»£ç è´¨é‡åˆ†æå¤±è´¥: {e}")

    async def _perform_static_analysis(self):
        """æ‰§è¡Œé™æ€ä»£ç åˆ†æ"""
        logger.info("    æ‰§è¡Œé™æ€ä»£ç åˆ†æ...")
        
        # å®šä¹‰è´¨é‡è§„åˆ™
        quality_rules = {
            "long_lines": {
                "pattern": r".{120,}",  # è¶…è¿‡120å­—ç¬¦çš„è¡Œ
                "severity": Severity.MEDIUM,
                "message": "è¡Œé•¿åº¦è¶…è¿‡120å­—ç¬¦"
            },
            "trailing_whitespace": {
                "pattern": r".+\s+$",
                "severity": Severity.LOW,
                "message": "è¡Œå°¾æœ‰å¤šä½™ç©ºæ ¼"
            },
            "missing_docstrings": {
                "pattern": r"def\s+\w+\([^)]*\):\s*$",
                "severity": Severity.MEDIUM,
                "message": "å‡½æ•°ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²"
            },
            "unused_imports": {
                "pattern": r"^import\s+\w+",
                "severity": Severity.LOW,
                "message": "å¯èƒ½æœªä½¿ç”¨çš„å¯¼å…¥"
            },
            "hardcoded_values": {
                "pattern": r"\b\d{3,}\b",
                "severity": Severity.MEDIUM,
                "message": "ç¡¬ç¼–ç æ•°å€¼"
            }
        }
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    for line_num, line in enumerate(lines, 1):
                        for rule_name, rule_info in quality_rules.items():
                            if re.search(rule_info["pattern"], line):
                                issue = Issue(
                                    id=f"{rule_name}_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}",
                                    type="code_quality",
                                    severity=rule_info["severity"],
                                    title=rule_info["message"],
                                    description=f"åœ¨æ–‡ä»¶ {file_path} ç¬¬{line_num}è¡Œå‘ç° {rule_info['message']}",
                                    file_path=str(file_path),
                                    line_number=line_num,
                                    evidence=line.strip(),
                                    fix_suggestion=self._get_fix_suggestion(rule_name),
                                    auto_fixable=rule_name in ["trailing_whitespace", "long_lines"],
                                    category="code_style",
                                    impact="maintainability",
                                    effort="low"
                                )
                                self.issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†ææ–‡ä»¶ {file_path}: {e}")

    def _get_fix_suggestion(self, rule_name: str) -> str:
        """è·å–ä¿®å¤å»ºè®®"""
        suggestions = {
            "long_lines": "å°†é•¿è¡Œåˆ†è§£ä¸ºå¤šè¡Œï¼Œæé«˜ä»£ç å¯è¯»æ€§",
            "trailing_whitespace": "åˆ é™¤è¡Œå°¾ç©ºæ ¼",
            "missing_docstrings": "ä¸ºå‡½æ•°æ·»åŠ è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²",
            "unused_imports": "åˆ é™¤æœªä½¿ç”¨çš„å¯¼å…¥è¯­å¥",
            "hardcoded_values": "å°†ç¡¬ç¼–ç å€¼æå–ä¸ºå¸¸é‡æˆ–é…ç½®é¡¹"
        }
        return suggestions.get(rule_name, "è¯·å‚è€ƒæœ€ä½³å®è·µè¿›è¡Œä¿®å¤")

    async def _analyze_complexity(self):
        """åˆ†æä»£ç å¤æ‚åº¦"""
        logger.info("    åˆ†æä»£ç å¤æ‚åº¦...")
        
        total_complexity = 0
        file_count = 0
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ç®€å•çš„å¤æ‚åº¦è®¡ç®—
                    complexity = self._calculate_complexity(content)
                    total_complexity += complexity
                    file_count += 1
                    
                    if complexity > 10:
                        issue = Issue(
                            id=f"complexity_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}",
                            type="complexity",
                            severity=Severity.HIGH if complexity > 20 else Severity.MEDIUM,
                            title="å‡½æ•°å¤æ‚åº¦è¿‡é«˜",
                            description=f"æ–‡ä»¶ {file_path} çš„å¤æ‚åº¦ä¸º {complexity}ï¼Œè¶…è¿‡æ¨èå€¼",
                            file_path=str(file_path),
                            line_number=1,
                            evidence=f"å¤æ‚åº¦: {complexity}",
                            fix_suggestion="è€ƒè™‘å°†å¤æ‚å‡½æ•°æ‹†åˆ†ä¸ºå¤šä¸ªå°å‡½æ•°",
                            auto_fixable=False,
                            category="complexity",
                            impact="maintainability",
                            effort="medium"
                        )
                        self.issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æå¤æ‚åº¦ {file_path}: {e}")
        
        if file_count > 0:
            self.project_metrics.complexity_score = total_complexity / file_count

    def _calculate_complexity(self, content: str) -> int:
        """è®¡ç®—ä»£ç å¤æ‚åº¦"""
        try:
            tree = ast.parse(content)
            complexity = 0
            
            for node in ast.walk(tree):
                if isinstance(node, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                    complexity += 1
                elif isinstance(node, ast.BoolOp):
                    complexity += len(node.values) - 1
            
            return complexity
        except:
            return 0

    async def _analyze_maintainability(self):
        """åˆ†æå¯ç»´æŠ¤æ€§"""
        logger.info("    åˆ†æå¯ç»´æŠ¤æ€§...")
        
        # è®¡ç®—å¯ç»´æŠ¤æ€§æŒ‡æ•°
        total_lines = 0
        comment_lines = 0
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    for line in lines:
                        total_lines += 1
                        stripped = line.strip()
                        if stripped.startswith('#') or '"""' in stripped or "'''" in stripped:
                            comment_lines += 1
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æå¯ç»´æŠ¤æ€§ {file_path}: {e}")
        
        self.project_metrics.total_lines = total_lines
        self.project_metrics.comment_lines = comment_lines
        self.project_metrics.code_lines = total_lines - comment_lines
        
        if total_lines > 0:
            comment_ratio = comment_lines / total_lines
            # ç®€å•çš„å¯ç»´æŠ¤æ€§æŒ‡æ•°è®¡ç®—
            maintainability = min(100, (comment_ratio * 100) + (50 - min(50, self.project_metrics.complexity_score)))
            self.project_metrics.maintainability_index = maintainability

    async def _analyze_code_style(self):
        """åˆ†æä»£ç é£æ ¼"""
        logger.info("    åˆ†æä»£ç é£æ ¼...")
        
        # PEP 8 é£æ ¼æ£€æŸ¥
        style_issues = [
            {
                "pattern": r"def\s+([a-z][a-z0-9_]*)\s*\(",
                "severity": Severity.MEDIUM,
                "message": "å‡½æ•°ååº”ä½¿ç”¨å°å†™å­—æ¯å’Œä¸‹åˆ’çº¿"
            },
            {
                "pattern": r"class\s+([A-Z][a-zA-Z0-9_]*)\s*:",
                "severity": Severity.MEDIUM,
                "message": "ç±»ååº”ä½¿ç”¨é©¼å³°å‘½åæ³•"
            }
        ]
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    for issue_rule in style_issues:
                        matches = re.finditer(issue_rule["pattern"], content)
                        for match in matches:
                            issue = Issue(
                                id=f"style_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}_{len(self.issues)}",
                                type="code_style",
                                severity=issue_rule["severity"],
                                title=issue_rule["message"],
                                description=f"åœ¨æ–‡ä»¶ {file_path} ä¸­å‘ç°é£æ ¼é—®é¢˜",
                                file_path=str(file_path),
                                line_number=content[:match.start()].count('\n') + 1,
                                evidence=match.group(0),
                                fix_suggestion="æŒ‰ç…§PEP 8è§„èŒƒè°ƒæ•´å‘½å",
                                auto_fixable=False,
                                category="code_style",
                                impact="readability",
                                effort="low"
                            )
                            self.issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æä»£ç é£æ ¼ {file_path}: {e}")

    async def _analyze_code_duplication(self):
        """åˆ†æä»£ç é‡å¤"""
        logger.info("    åˆ†æä»£ç é‡å¤...")
        
        # ç®€å•çš„é‡å¤ä»£ç æ£€æµ‹
        code_blocks = defaultdict(list)
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    # æ£€æŸ¥5è¡Œä»¥ä¸Šçš„é‡å¤å—
                    for i in range(len(lines) - 4):
                        block = ''.join(lines[i:i+5]).strip()
                        if len(block) > 50:  # å¿½ç•¥å¤ªçŸ­çš„å—
                            code_blocks[block].append((str(file_path), i+1))
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æä»£ç é‡å¤ {file_path}: {e}")
        
        # æŠ¥å‘Šé‡å¤ä»£ç 
        for block, occurrences in code_blocks.items():
            if len(occurrences) > 1:
                issue = Issue(
                    id=f"duplication_{hashlib.md5(block.encode()).hexdigest()[:8]}",
                    type="code_duplication",
                    severity=Severity.MEDIUM,
                    title="ä»£ç é‡å¤",
                    description=f"å‘ç°é‡å¤ä»£ç å—ï¼Œå‡ºç°åœ¨ {len(occurrences)} ä¸ªä½ç½®",
                    file_path=occurrences[0][0],
                    line_number=occurrences[0][1],
                    evidence=block[:100] + "..." if len(block) > 100 else block,
                    fix_suggestion="è€ƒè™‘å°†é‡å¤ä»£ç æå–ä¸ºå‡½æ•°æˆ–æ¨¡å—",
                    auto_fixable=False,
                    category="duplication",
                    impact="maintainability",
                    effort="medium"
                )
                self.issues.append(issue)

    async def _analyze_architecture(self):
        """åˆ†ææ¶æ„"""
        logger.info("  ğŸ—ï¸ åˆ†ææ¶æ„...")
        
        try:
            # æ¶æ„æ¨¡å¼è¯†åˆ«
            await self._identify_architecture_patterns()
            
            # æ¨¡å—è€¦åˆåº¦åˆ†æ
            await self._analyze_coupling()
            
            # è®¾è®¡æ¨¡å¼è¯†åˆ«
            await self._identify_design_patterns()
            
            # åˆ†å±‚æ¶æ„åˆ†æ
            await self._analyze_layered_architecture()
            
        except Exception as e:
            logger.error(f"    æ¶æ„åˆ†æå¤±è´¥: {e}")

    async def _identify_architecture_patterns(self):
        """è¯†åˆ«æ¶æ„æ¨¡å¼"""
        patterns = {
            "mvc": ["models", "views", "controllers"],
            "mvp": ["models", "views", "presenters"],
            "mvvm": ["models", "views", "viewmodels"],
            "layered": ["controllers", "services", "repositories", "models"],
            "microservice": ["services", "apis", "gateways"],
            "plugin": ["plugins", "extensions", "core"]
        }
        
        detected_patterns = []
        
        for pattern_name, pattern_dirs in patterns.items():
            pattern_found = True
            for required_dir in pattern_dirs:
                if not any(required_dir in str(p).lower() for p in self.workspace_path.iterdir() if p.is_dir()):
                    pattern_found = False
                    break
            
            if pattern_found:
                detected_patterns.append(pattern_name)
        
        self.architecture_analysis["detected_patterns"] = detected_patterns

    async def _analyze_coupling(self):
        """åˆ†æè€¦åˆåº¦"""
        # ç®€åŒ–çš„è€¦åˆåº¦åˆ†æ
        coupling_score = 0
        module_count = 0
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # è®¡ç®—å¯¼å…¥æ•°é‡ä½œä¸ºè€¦åˆåº¦æŒ‡æ ‡
                    imports = len(re.findall(r'^import\s+|^from\s+\w+', content, re.MULTILINE))
                    coupling_score += imports
                    module_count += 1
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æè€¦åˆåº¦ {file_path}: {e}")
        
        if module_count > 0:
            avg_coupling = coupling_score / module_count
            self.architecture_analysis["average_coupling"] = avg_coupling

    async def _identify_design_patterns(self):
        """è¯†åˆ«è®¾è®¡æ¨¡å¼"""
        # ç®€åŒ–çš„è®¾è®¡æ¨¡å¼è¯†åˆ«
        patterns_found = []
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # å•ä¾‹æ¨¡å¼æ£€æµ‹
                    if re.search(r'class\s+\w+.*__new__.*instance', content):
                        patterns_found.append(("singleton", str(file_path)))
                    
                    # å·¥å‚æ¨¡å¼æ£€æµ‹
                    if re.search(r'def\s+create_\w+|class\s+\w*Factory\w*', content):
                        patterns_found.append(("factory", str(file_path)))
                    
                    # è§‚å¯Ÿè€…æ¨¡å¼æ£€æµ‹
                    if re.search(r'add_observer|notify_observers|attach.*detach', content):
                        patterns_found.append(("observer", str(file_path)))
                
                except Exception as e:
                    logger.warning(f"æ— æ³•è¯†åˆ«è®¾è®¡æ¨¡å¼ {file_path}: {e}")
        
        self.architecture_analysis["design_patterns"] = patterns_found

    async def _analyze_layered_architecture(self):
        """åˆ†æåˆ†å±‚æ¶æ„"""
        layers = {
            "presentation": ["views", "controllers", "handlers", "apis"],
            "business": ["services", "business", "logic", "domain"],
            "data": ["repositories", "data", "models", "entities"],
            "infrastructure": ["config", "utils", "helpers", "common"]
        }
        
        layer_files = defaultdict(list)
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                file_str = str(file_path).lower()
                for layer_name, layer_keywords in layers.items():
                    if any(keyword in file_str for keyword in layer_keywords):
                        layer_files[layer_name].append(str(file_path))
        
        self.architecture_analysis["layer_distribution"] = dict(layer_files)

    async def _analyze_performance(self):
        """åˆ†ææ€§èƒ½"""
        logger.info("  âš¡ åˆ†ææ€§èƒ½...")
        
        try:
            # æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
            await self._identify_performance_bottlenecks()
            
            # ç®—æ³•å¤æ‚åº¦åˆ†æ
            await self._analyze_algorithmic_complexity()
            
            # å†…å­˜ä½¿ç”¨åˆ†æ
            await self._analyze_memory_usage()
            
            # I/Oæ“ä½œåˆ†æ
            await self._analyze_io_operations()
            
        except Exception as e:
            logger.error(f"    æ€§èƒ½åˆ†æå¤±è´¥: {e}")

    async def _identify_performance_bottlenecks(self):
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottleneck_patterns = [
            {
                "pattern": r"for\s+\w+\s+in\s+.*\.keys\(\)",
                "severity": Severity.MEDIUM,
                "message": "ä½¿ç”¨.keys()éå†å­—å…¸æ•ˆç‡è¾ƒä½"
            },
            {
                "pattern": r"\.format\(|%\s*.*%|f['\"]",
                "severity": Severity.LOW,
                "message": "å­—ç¬¦ä¸²æ ¼å¼åŒ–å¯èƒ½å½±å“æ€§èƒ½"
            },
            {
                "pattern": r"time\.sleep\(",
                "severity": Severity.MEDIUM,
                "message": "åŒæ­¥sleepå¯èƒ½é˜»å¡çº¿ç¨‹"
            }
        ]
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    for pattern_info in bottleneck_patterns:
                        matches = re.finditer(pattern_info["pattern"], content)
                        for match in matches:
                            issue = Issue(
                                id=f"performance_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}_{len(self.issues)}",
                                type="performance",
                                severity=pattern_info["severity"],
                                title=pattern_info["message"],
                                description=f"åœ¨æ–‡ä»¶ {file_path} ä¸­å‘ç°æ½œåœ¨æ€§èƒ½é—®é¢˜",
                                file_path=str(file_path),
                                line_number=content[:match.start()].count('\n') + 1,
                                evidence=match.group(0),
                                fix_suggestion=self._get_performance_fix_suggestion(pattern_info["message"]),
                                auto_fixable=False,
                                category="performance",
                                impact="performance",
                                effort="medium"
                            )
                            self.issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†ææ€§èƒ½ç“¶é¢ˆ {file_path}: {e}")

    def _get_performance_fix_suggestion(self, message: str) -> str:
        """è·å–æ€§èƒ½ä¿®å¤å»ºè®®"""
        suggestions = {
            "ä½¿ç”¨.keys()éå†å­—å…¸æ•ˆç‡è¾ƒä½": "ç›´æ¥éå†å­—å…¸è€Œä¸æ˜¯.keys()",
            "å­—ç¬¦ä¸²æ ¼å¼åŒ–å¯èƒ½å½±å“æ€§èƒ½": "è€ƒè™‘ä½¿ç”¨f-stringæˆ–æ›´é«˜æ•ˆçš„æ ¼å¼åŒ–æ–¹æ³•",
            "åŒæ­¥sleepå¯èƒ½é˜»å¡çº¿ç¨‹": "è€ƒè™‘ä½¿ç”¨å¼‚æ­¥sleepæˆ–éé˜»å¡æ–¹å¼"
        }
        return suggestions.get(message, "è¯·å‚è€ƒæ€§èƒ½æœ€ä½³å®è·µè¿›è¡Œä¼˜åŒ–")

    async def _analyze_algorithmic_complexity(self):
        """åˆ†æç®—æ³•å¤æ‚åº¦"""
        complexity_patterns = [
            {
                "pattern": r"for\s+.*\s+in\s+.*:\s*for\s+.*\s+in\s+.*",
                "severity": Severity.HIGH,
                "message": "åµŒå¥—å¾ªç¯å¯èƒ½å¯¼è‡´O(nÂ²)å¤æ‚åº¦"
            },
            {
                "pattern": r"\.sort\(\)|sorted\(",
                "severity": Severity.MEDIUM,
                "message": "æ’åºæ“ä½œçš„æ—¶é—´å¤æ‚åº¦ä¸ºO(n log n)"
            }
        ]
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    for pattern_info in complexity_patterns:
                        matches = re.finditer(pattern_info["pattern"], content)
                        for match in matches:
                            issue = Issue(
                                id=f"algorithm_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}_{len(self.issues)}",
                                type="algorithmic_complexity",
                                severity=pattern_info["severity"],
                                title=pattern_info["message"],
                                description=f"åœ¨æ–‡ä»¶ {file_path} ä¸­å‘ç°ç®—æ³•å¤æ‚åº¦é—®é¢˜",
                                file_path=str(file_path),
                                line_number=content[:match.start()].count('\n') + 1,
                                evidence=match.group(0),
                                fix_suggestion="è€ƒè™‘ä¼˜åŒ–ç®—æ³•æˆ–ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„",
                                auto_fixable=False,
                                category="algorithm",
                                impact="performance",
                                effort="high"
                            )
                            self.issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æç®—æ³•å¤æ‚åº¦ {file_path}: {e}")

    async def _analyze_memory_usage(self):
        """åˆ†æå†…å­˜ä½¿ç”¨"""
        memory_patterns = [
            {
                "pattern": r"\[\w+\s+for\s+.*\s+in\s+.*\s+if\s+.*\]",
                "severity": Severity.MEDIUM,
                "message": "åˆ—è¡¨æ¨å¯¼å¼å¯èƒ½æ¶ˆè€—å¤§é‡å†…å­˜"
            },
            {
                "pattern": r"append\(.*\)\s*for\s+.*\s+in\s+.*:",
                "severity": Severity.LOW,
                "message": "å¾ªç¯ä¸­appendå¯èƒ½å¯¼è‡´é¢‘ç¹å†…å­˜åˆ†é…"
            }
        ]
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    for pattern_info in memory_patterns:
                        matches = re.finditer(pattern_info["pattern"], content)
                        for match in matches:
                            issue = Issue(
                                id=f"memory_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}_{len(self.issues)}",
                                type="memory_usage",
                                severity=pattern_info["severity"],
                                title=pattern_info["message"],
                                description=f"åœ¨æ–‡ä»¶ {file_path} ä¸­å‘ç°å†…å­˜ä½¿ç”¨é—®é¢˜",
                                file_path=str(file_path),
                                line_number=content[:match.start()].count('\n') + 1,
                                evidence=match.group(0),
                                fix_suggestion="è€ƒè™‘ä½¿ç”¨ç”Ÿæˆå™¨æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨æ¨¡å¼",
                                auto_fixable=False,
                                category="memory",
                                impact="memory",
                                effort="medium"
                            )
                            self.issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æå†…å­˜ä½¿ç”¨ {file_path}: {e}")

    async def _analyze_io_operations(self):
        """åˆ†æI/Oæ“ä½œ"""
        io_patterns = [
            {
                "pattern": r"open\([^)]*\)\.read\(\)",
                "severity": Severity.MEDIUM,
                "message": "ä¸€æ¬¡æ€§è¯»å–å¤§æ–‡ä»¶å¯èƒ½æ¶ˆè€—å¤§é‡å†…å­˜"
            },
            {
                "pattern": r"with\s+open\([^)]*\)\s+as\s+f:",
                "severity": Severity.LOW,
                "message": "æ–‡ä»¶I/Oæ“ä½œå»ºè®®ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨"
            }
        ]
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    for pattern_info in io_patterns:
                        matches = re.finditer(pattern_info["pattern"], content)
                        for match in matches:
                            issue = Issue(
                                id=f"io_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}_{len(self.issues)}",
                                type="io_operation",
                                severity=pattern_info["severity"],
                                title=pattern_info["message"],
                                description=f"åœ¨æ–‡ä»¶ {file_path} ä¸­å‘ç°I/Oæ“ä½œé—®é¢˜",
                                file_path=str(file_path),
                                line_number=content[:match.start()].count('\n') + 1,
                                evidence=match.group(0),
                                fix_suggestion="ä¼˜åŒ–I/Oæ“ä½œï¼Œè€ƒè™‘åˆ†å—è¯»å–æˆ–å¼‚æ­¥å¤„ç†",
                                auto_fixable=False,
                                category="io",
                                impact="performance",
                                effort="medium"
                            )
                            self.issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æI/Oæ“ä½œ {file_path}: {e}")

    async def _analyze_security(self):
        """åˆ†æå®‰å…¨æ€§"""
        logger.info("  ğŸ›¡ï¸ åˆ†æå®‰å…¨æ€§...")
        
        try:
            # å®‰å…¨æ¼æ´æ‰«æ
            await self._scan_security_vulnerabilities()
            
            # æ•æ„Ÿä¿¡æ¯æ£€æµ‹
            await self._detect_sensitive_information()
            
            # æƒé™åˆ†æ
            await self._analyze_permissions()
            
            # ä¾èµ–å®‰å…¨åˆ†æ
            await self._analyze_dependency_security()
            
        except Exception as e:
            logger.error(f"    å®‰å…¨åˆ†æå¤±è´¥: {e}")

    async def _scan_security_vulnerabilities(self):
        """æ‰«æå®‰å…¨æ¼æ´"""
        security_patterns = [
            {
                "pattern": r"eval\(|exec\(",
                "severity": Severity.CRITICAL,
                "message": "ä½¿ç”¨evalæˆ–execå­˜åœ¨ä»£ç æ³¨å…¥é£é™©",
                "cwe": "CWE-94"
            },
            {
                "pattern": r"shell=True|subprocess\.call.*shell=True",
                "severity": Severity.CRITICAL,
                "message": "shell=Trueå­˜åœ¨å‘½ä»¤æ³¨å…¥é£é™©",
                "cwe": "CWE-78"
            },
            {
                "pattern": r"pickle\.loads|cPickle\.loads",
                "severity": Severity.HIGH,
                "message": "pickleååºåˆ—åŒ–å­˜åœ¨å®‰å…¨é£é™©",
                "cwe": "CWE-502"
            },
            {
                "pattern": r"random\.random|random\.randint",
                "severity": Severity.MEDIUM,
                "message": "ä½¿ç”¨ä¼ªéšæœºæ•°ç”Ÿæˆå™¨å¯èƒ½ä¸å®‰å…¨",
                "cwe": "CWE-338"
            },
            {
                "pattern": r"hashlib\.md5\(|hashlib\.sha1\(",
                "severity": Severity.MEDIUM,
                "message": "ä½¿ç”¨å¼±å“ˆå¸Œç®—æ³•",
                "cwe": "CWE-327"
            }
        ]
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    for pattern_info in security_patterns:
                        matches = re.finditer(pattern_info["pattern"], content)
                        for match in matches:
                            issue = Issue(
                                id=f"security_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}_{len(self.issues)}",
                                type="security_vulnerability",
                                severity=pattern_info["severity"],
                                title=pattern_info["message"],
                                description=f"åœ¨æ–‡ä»¶ {file_path} ä¸­å‘ç°å®‰å…¨æ¼æ´ ({pattern_info['cwe']})",
                                file_path=str(file_path),
                                line_number=content[:match.start()].count('\n') + 1,
                                evidence=match.group(0),
                                fix_suggestion=self._get_security_fix_suggestion(pattern_info["message"]),
                                auto_fixable=False,
                                category="security",
                                impact="security",
                                effort="high"
                            )
                            self.issues.append(issue)
                            self.security_findings.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•æ‰«æå®‰å…¨æ¼æ´ {file_path}: {e}")

    def _get_security_fix_suggestion(self, message: str) -> str:
        """è·å–å®‰å…¨ä¿®å¤å»ºè®®"""
        suggestions = {
            "ä½¿ç”¨evalæˆ–execå­˜åœ¨ä»£ç æ³¨å…¥é£é™©": "é¿å…ä½¿ç”¨eval/execï¼Œä½¿ç”¨å®‰å…¨çš„æ›¿ä»£æ–¹æ¡ˆ",
            "shell=Trueå­˜åœ¨å‘½ä»¤æ³¨å…¥é£é™©": "é¿å…shell=Trueï¼Œä½¿ç”¨å‚æ•°åŒ–å‘½ä»¤",
            "pickleååºåˆ—åŒ–å­˜åœ¨å®‰å…¨é£é™©": "ä½¿ç”¨å®‰å…¨çš„åºåˆ—åŒ–æ ¼å¼å¦‚JSON",
            "ä½¿ç”¨ä¼ªéšæœºæ•°ç”Ÿæˆå™¨å¯èƒ½ä¸å®‰å…¨": "ä½¿ç”¨secretsæ¨¡å—ç”Ÿæˆå®‰å…¨éšæœºæ•°",
            "ä½¿ç”¨å¼±å“ˆå¸Œç®—æ³•": "ä½¿ç”¨å¼ºå“ˆå¸Œç®—æ³•å¦‚SHA-256æˆ–SHA-3"
        }
        return suggestions.get(message, "è¯·å‚è€ƒå®‰å…¨æœ€ä½³å®è·µè¿›è¡Œä¿®å¤")

    async def _detect_sensitive_information(self):
        """æ£€æµ‹æ•æ„Ÿä¿¡æ¯"""
        sensitive_patterns = [
            {
                "pattern": r"(password|passwd|pwd)\s*=\s*['\"][^'\"]+['\"]",
                "severity": Severity.HIGH,
                "message": "ç¡¬ç¼–ç å¯†ç "
            },
            {
                "pattern": r"(api_key|apikey|secret_key)\s*=\s*['\"][^'\"]+['\"]",
                "severity": Severity.HIGH,
                "message": "ç¡¬ç¼–ç APIå¯†é’¥"
            },
            {
                "pattern": r"(token|auth)\s*=\s*['\"][^'\"]+['\"]",
                "severity": Severity.HIGH,
                "message": "ç¡¬ç¼–ç è®¤è¯ä»¤ç‰Œ"
            }
        ]
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    for pattern_info in sensitive_patterns:
                        matches = re.finditer(pattern_info["pattern"], content, re.IGNORECASE)
                        for match in matches:
                            issue = Issue(
                                id=f"sensitive_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}_{len(self.issues)}",
                                type="sensitive_information",
                                severity=pattern_info["severity"],
                                title=pattern_info["message"],
                                description=f"åœ¨æ–‡ä»¶ {file_path} ä¸­å‘ç°æ•æ„Ÿä¿¡æ¯",
                                file_path=str(file_path),
                                line_number=content[:match.start()].count('\n') + 1,
                                evidence=match.group(0)[:50] + "...",
                                fix_suggestion="å°†æ•æ„Ÿä¿¡æ¯ç§»è‡³ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶",
                                auto_fixable=True,
                                category="security",
                                impact="security",
                                effort="medium"
                            )
                            self.issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•æ£€æµ‹æ•æ„Ÿä¿¡æ¯ {file_path}: {e}")

    async def _analyze_permissions(self):
        """åˆ†ææƒé™"""
        # æ£€æŸ¥æ–‡ä»¶æƒé™
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    stat_info = file_path.stat()
                    mode = oct(stat_info.st_mode)[-3:]
                    
                    # æ£€æŸ¥æ˜¯å¦å¯¹å…¶ä»–ç”¨æˆ·å¯å†™
                    if mode[2] in ['2', '3', '6', '7']:
                        issue = Issue(
                            id=f"permission_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}",
                            type="file_permission",
                            severity=Severity.MEDIUM,
                            title="æ–‡ä»¶æƒé™è¿‡äºå®½æ¾",
                            description=f"æ–‡ä»¶ {file_path} å¯¹å…¶ä»–ç”¨æˆ·å¯å†™",
                            file_path=str(file_path),
                            line_number=1,
                            evidence=f"æƒé™æ¨¡å¼: {mode}",
                            fix_suggestion="è°ƒæ•´æ–‡ä»¶æƒé™ï¼Œç§»é™¤å…¶ä»–ç”¨æˆ·çš„å†™æƒé™",
                            auto_fixable=True,
                            category="security",
                            impact="security",
                            effort="low"
                        )
                        self.issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†ææƒé™ {file_path}: {e}")

    async def _analyze_dependency_security(self):
        """åˆ†æä¾èµ–å®‰å…¨"""
        # æ£€æŸ¥ä¾èµ–æ–‡ä»¶
        dependency_files = ["requirements.txt", "pyproject.toml", "package.json"]
        
        for dep_file in dependency_files:
            file_path = self.workspace_path / dep_file
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ç®€å•çš„å·²çŸ¥æ¼æ´ä¾èµ–æ£€æŸ¥
                    vulnerable_packages = [
                        "urllib3==1.24.2",  # ç¤ºä¾‹æ¼æ´åŒ…
                        "requests==2.20.0",
                        "pillow<6.2.0"
                    ]
                    
                    for vuln_pkg in vulnerable_packages:
                        if vuln_pkg in content:
                            issue = Issue(
                                id=f"dep_vuln_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}",
                                type="dependency_vulnerability",
                                severity=Severity.HIGH,
                                title="ä¾èµ–åŒ…å­˜åœ¨å·²çŸ¥æ¼æ´",
                                description=f"åœ¨ {dep_file} ä¸­å‘ç°æ¼æ´ä¾èµ–: {vuln_pkg}",
                                file_path=str(file_path),
                                line_number=content.split('\n').index([line for line in content.split('\n') if vuln_pkg in line][0]) + 1,
                                evidence=vuln_pkg,
                                fix_suggestion="å‡çº§åˆ°å®‰å…¨ç‰ˆæœ¬",
                                auto_fixable=True,
                                category="security",
                                impact="security",
                                effort="medium"
                            )
                            self.issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æä¾èµ–å®‰å…¨ {file_path}: {e}")

    async def _analyze_dependencies(self):
        """åˆ†æä¾èµ–"""
        logger.info("  ğŸ“¦ åˆ†æä¾èµ–...")
        
        try:
            # ä¾èµ–å…³ç³»åˆ†æ
            await self._analyze_dependency_graph()
            
            # ç‰ˆæœ¬å…¼å®¹æ€§åˆ†æ
            await self._analyze_version_compatibility()
            
            # è®¸å¯è¯åˆè§„æ€§åˆ†æ
            await self._analyze_license_compliance()
            
        except Exception as e:
            logger.error(f"    ä¾èµ–åˆ†æå¤±è´¥: {e}")

    async def _analyze_dependency_graph(self):
        """åˆ†æä¾èµ–å›¾"""
        dependencies = defaultdict(set)
        
        # åˆ†æPythonä¾èµ–
        if (self.workspace_path / "requirements.txt").exists():
            with open(self.workspace_path / "requirements.txt", 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        pkg_name = line.split('==')[0].split('>=')[0].split('<=')[0].strip()
                        dependencies["python"].add(pkg_name)
        
        # åˆ†æNode.jsä¾èµ–
        if (self.workspace_path / "package.json").exists():
            try:
                with open(self.workspace_path / "package.json", 'r') as f:
                    package_data = json.load(f)
                
                for dep_type in ["dependencies", "devDependencies"]:
                    if dep_type in package_data:
                        for pkg_name in package_data[dep_type].keys():
                            dependencies["nodejs"].add(pkg_name)
            except Exception as e:
                logger.warning(f"è§£æpackage.jsonå¤±è´¥: {e}")
        
        self.architecture_analysis["dependencies"] = {k: list(v) for k, v in dependencies.items()}

    async def _analyze_version_compatibility(self):
        """åˆ†æç‰ˆæœ¬å…¼å®¹æ€§"""
        # ç®€åŒ–çš„ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥
        compatibility_issues = []
        
        for file_path in self.workspace_path.rglob("requirements.txt"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æ£€æŸ¥å›ºå®šç‰ˆæœ¬
                    fixed_versions = re.findall(r'(\w+)==([0-9.]+)', content)
                    for pkg, version in fixed_versions:
                        if version.startswith('0.') or version.startswith('1.0.'):
                            compatibility_issues.append((pkg, version, "ç‰ˆæœ¬è¿‡æ—§ï¼Œå¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜"))
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æç‰ˆæœ¬å…¼å®¹æ€§ {file_path}: {e}")
        
        self.architecture_analysis["compatibility_issues"] = compatibility_issues

    async def _analyze_license_compliance(self):
        """åˆ†æè®¸å¯è¯åˆè§„æ€§"""
        # ç®€åŒ–çš„è®¸å¯è¯æ£€æŸ¥
        allowed_licenses = ["MIT", "Apache-2.0", "BSD", "ISC"]
        problematic_licenses = []
        
        # è¿™é‡Œåº”è¯¥å®ç°çœŸæ­£çš„è®¸å¯è¯æ£€æŸ¥é€»è¾‘
        # ç›®å‰åªæ˜¯å ä½ç¬¦
        
        self.architecture_analysis["license_compliance"] = {
            "allowed_licenses": allowed_licenses,
            "problematic_licenses": problematic_licenses,
            "compliance_status": "compliant" if not problematic_licenses else "non_compliant"
        }

    async def _analyze_test_coverage(self):
        """åˆ†ææµ‹è¯•è¦†ç›–ç‡"""
        logger.info("  ğŸ§ª åˆ†ææµ‹è¯•è¦†ç›–ç‡...")
        
        try:
            test_files = list(self.workspace_path.rglob("*test*.py"))
            test_files.extend(list(self.workspace_path.rglob("test_*.py")))
            
            code_files = list(self.workspace_path.rglob("*.py"))
            code_files = [f for f in code_files if not any(pattern in f.name for pattern in ["test", "spec"])]
            
            self.project_metrics.test_files = len(test_files)
            self.project_metrics.code_files = len(code_files)
            
            if len(code_files) > 0:
                coverage_ratio = len(test_files) / len(code_files)
                self.project_metrics.test_coverage = coverage_ratio * 100
                
                if coverage_ratio < 0.5:
                    issue = Issue(
                        id="test_coverage_low",
                        type="test_coverage",
                        severity=Severity.MEDIUM,
                        title="æµ‹è¯•è¦†ç›–ç‡è¿‡ä½",
                        description=f"æµ‹è¯•è¦†ç›–ç‡ä»…ä¸º {coverage_ratio*100:.1f}%ï¼Œå»ºè®®å¢åŠ æµ‹è¯•",
                        file_path="",
                        line_number=1,
                        evidence=f"ä»£ç æ–‡ä»¶: {len(code_files)}, æµ‹è¯•æ–‡ä»¶: {len(test_files)}",
                        fix_suggestion="å¢åŠ å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•",
                        auto_fixable=False,
                        category="testing",
                        impact="quality",
                        effort="high"
                    )
                    self.issues.append(issue)
        
        except Exception as e:
            logger.error(f"    æµ‹è¯•è¦†ç›–ç‡åˆ†æå¤±è´¥: {e}")

    async def _phase_planning(self):
        """é˜¶æ®µ2: æ™ºèƒ½è§„åˆ’"""
        logger.info("ğŸ“‹ é˜¶æ®µ2: æ™ºèƒ½è§„åˆ’...")
        self.current_phase = UpgradePhase.PLANNING
        
        try:
            # 2.1 ä¼˜å…ˆçº§è¯„ä¼°
            await self._assess_priorities()
            
            # 2.2 å‡çº§è®¡åˆ’åˆ¶å®š
            await self._create_upgrade_plan()
            
            # 2.3 é£é™©è¯„ä¼°
            await self._assess_risks()
            
            # 2.4 èµ„æºè¯„ä¼°
            await self._assess_resources()
            
            logger.info(f"  âœ… æ™ºèƒ½è§„åˆ’å®Œæˆï¼Œåˆ¶å®šäº† {len(self.upgrade_actions)} ä¸ªå‡çº§åŠ¨ä½œ")
            
        except Exception as e:
            logger.error(f"  âŒ æ™ºèƒ½è§„åˆ’å¤±è´¥: {e}")
            raise

    async def _assess_priorities(self):
        """è¯„ä¼°ä¼˜å…ˆçº§"""
        logger.info("    è¯„ä¼°é—®é¢˜ä¼˜å…ˆçº§...")
        
        # æ ¹æ®ä¸¥é‡ç¨‹åº¦å’Œå½±å“è¯„ä¼°ä¼˜å…ˆçº§
        for issue in self.issues:
            if issue.severity == Severity.CRITICAL:
                issue.priority = "P0"
            elif issue.severity == Severity.HIGH:
                issue.priority = "P1"
            elif issue.severity == Severity.MEDIUM:
                issue.priority = "P2"
            else:
                issue.priority = "P3"
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.issues.sort(key=lambda x: (x.severity.value, x.priority))

    async def _create_upgrade_plan(self):
        """åˆ›å»ºå‡çº§è®¡åˆ’"""
        logger.info("    åˆ¶å®šå‡çº§è®¡åˆ’...")
        
        # æŒ‰ç±»å‹åˆ†ç»„é—®é¢˜
        issues_by_type = defaultdict(list)
        for issue in self.issues:
            issues_by_type[issue.category].append(issue)
        
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºå‡çº§åŠ¨ä½œ
        for category, issues in issues_by_type.items():
            action = UpgradeAction(
                id=f"upgrade_{category}_{len(self.upgrade_actions)}",
                type=category,
                description=f"ä¿®å¤ {len(issues)} ä¸ª {category} ç±»é—®é¢˜",
                file_path="",
                changes={
                    "issues": [asdict(issue) for issue in issues[:10]],  # é™åˆ¶æ•°é‡
                    "category": category,
                    "total_issues": len(issues)
                },
                priority=self._calculate_priority(issues),
                risk_level=self._assess_action_risk(category, issues),
                estimated_time=self._estimate_time(issues),
                dependencies=[]
            )
            self.upgrade_actions.append(action)

    def _calculate_priority(self, issues: List[Issue]) -> str:
        """è®¡ç®—ä¼˜å…ˆçº§"""
        if any(issue.severity == Severity.CRITICAL for issue in issues):
            return "critical"
        elif any(issue.severity == Severity.HIGH for issue in issues):
            return "high"
        elif any(issue.severity == Severity.MEDIUM for issue in issues):
            return "medium"
        else:
            return "low"

    def _assess_action_risk(self, category: str, issues: List[Issue]) -> str:
        """è¯„ä¼°åŠ¨ä½œé£é™©"""
        high_risk_categories = ["security", "architecture", "algorithm"]
        if category in high_risk_categories:
            return "high"
        elif category in ["performance", "memory"]:
            return "medium"
        else:
            return "low"

    def _estimate_time(self, issues: List[Issue]) -> int:
        """ä¼°ç®—æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰"""
        base_time = len(issues) * 5  # æ¯ä¸ªé—®é¢˜5åˆ†é’Ÿ
        
        # æ ¹æ®ç±»å‹è°ƒæ•´
        category_multipliers = {
            "security": 2.0,
            "architecture": 1.5,
            "performance": 1.3,
            "algorithm": 1.8,
            "code_style": 0.5,
            "duplication": 1.2
        }
        
        if issues:
            category = issues[0].category
            multiplier = category_multipliers.get(category, 1.0)
            return int(base_time * multiplier)
        
        return base_time

    async def _assess_risks(self):
        """è¯„ä¼°é£é™©"""
        logger.info("    è¯„ä¼°å‡çº§é£é™©...")
        
        # è®¡ç®—æ•´ä½“é£é™©åˆ†æ•°
        critical_issues = len([i for i in self.issues if i.severity == Severity.CRITICAL])
        high_issues = len([i for i in self.issues if i.severity == Severity.HIGH])
        
        risk_score = (critical_issues * 10) + (high_issues * 5) + len(self.issues)
        
        if risk_score > 50:
            overall_risk = "high"
        elif risk_score > 20:
            overall_risk = "medium"
        else:
            overall_risk = "low"
        
        self.architecture_analysis["upgrade_risk"] = {
            "risk_score": risk_score,
            "overall_risk": overall_risk,
            "critical_issues": critical_issues,
            "high_issues": high_issues
        }

    async def _assess_resources(self):
        """è¯„ä¼°èµ„æº"""
        logger.info("    è¯„ä¼°æ‰€éœ€èµ„æº...")
        
        total_estimated_time = sum(action.estimated_time for action in self.upgrade_actions)
        auto_fixable_count = len([i for i in self.issues if i.auto_fixable])
        
        self.architecture_analysis["resource_requirements"] = {
            "estimated_time_minutes": total_estimated_time,
            "auto_fixable_issues": auto_fixable_count,
            "manual_fix_required": len(self.issues) - auto_fixable_count,
            "recommended_parallel_actions": min(3, len(self.upgrade_actions))
        }

    async def _phase_execution(self):
        """é˜¶æ®µ3: æ‰§è¡Œå‡çº§"""
        logger.info("ğŸ”§ é˜¶æ®µ3: æ‰§è¡Œå‡çº§...")
        self.current_phase = UpgradePhase.EXECUTION
        
        try:
            # 3.1 è‡ªåŠ¨ä¿®å¤
            if self.auto_fix:
                await self._execute_auto_fixes()
            
            # 3.2 ä»£ç é‡æ„
            await self._execute_refactoring()
            
            # 3.3 æ€§èƒ½ä¼˜åŒ–
            await self._execute_performance_optimization()
            
            # 3.4 å®‰å…¨åŠ å›º
            await self._execute_security_hardening()
            
            # 3.5 æ¶æ„æ”¹è¿›
            await self._execute_architecture_improvements()
            
            logger.info("  âœ… å‡çº§æ‰§è¡Œå®Œæˆ")
            
        except Exception as e:
            logger.error(f"  âŒ å‡çº§æ‰§è¡Œå¤±è´¥: {e}")
            raise

    async def _execute_auto_fixes(self):
        """ç”Ÿæˆè‡ªåŠ¨ä¿®å¤å»ºè®®æŠ¥å‘Šï¼ˆä¸æ‰§è¡Œå®é™…ä¿®å¤ï¼‰"""
        logger.info("    ç”Ÿæˆè‡ªåŠ¨ä¿®å¤å»ºè®®æŠ¥å‘Š...")
        
        auto_fixable_issues = [issue for issue in self.issues if issue.auto_fixable]
        
        for issue in auto_fixable_issues:
            logger.info(f"        ğŸ“‹ è‡ªåŠ¨ä¿®å¤å»ºè®®: {issue.title}")
            logger.info(f"           æ–‡ä»¶: {issue.file_path}")
            logger.info(f"           ä½ç½®: ç¬¬{issue.line_number}è¡Œ")
            logger.info(f"           å»ºè®®: {issue.fix_suggestion}")
            logger.info(f"           ç±»å‹: {issue.category}")
            logger.info(f"           å½±å“: {issue.impact}")
            
            # è®°å½•ä¸ºå»ºè®®è€Œä¸æ˜¯ä¿®å¤
            self.changelog.append({
                "timestamp": datetime.now().isoformat(),
                "action": "auto_fix_suggestion",
                "issue": issue.title,
                "file": issue.file_path,
                "status": "suggested",
                "fix_suggestion": issue.fix_suggestion,
                "line_number": issue.line_number,
                "category": issue.category,
                "impact": issue.impact
            })
        
        logger.info(f"        ç”Ÿæˆäº† {len(auto_fixable_issues)} ä¸ªè‡ªåŠ¨ä¿®å¤å»ºè®®")

    async def _apply_fix(self, issue: Issue):
        """åº”ç”¨ä¿®å¤"""
        file_path = Path(issue.file_path)
        
        if not file_path.exists():
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            
            # æ ¹æ®é—®é¢˜ç±»å‹åº”ç”¨ä¸åŒçš„ä¿®å¤ç­–ç•¥
            if issue.type == "code_quality" and "trailing_whitespace" in issue.title:
                # ä¿®å¤è¡Œå°¾ç©ºæ ¼
                if issue.line_number <= len(lines):
                    lines[issue.line_number - 1] = lines[issue.line_number - 1].rstrip()
            
            elif issue.type == "code_quality" and "é•¿è¡Œ" in issue.title:
                # ä¿®å¤é•¿è¡Œï¼ˆç®€å•æ‹†åˆ†ï¼‰
                if issue.line_number <= len(lines):
                    long_line = lines[issue.line_number - 1]
                    if len(long_line) > 120:
                        # ç®€å•çš„è¡Œæ‹†åˆ†é€»è¾‘
                        lines[issue.line_number - 1] = long_line[:80] + " \\\n    " + long_line[80:]
            
            elif issue.type == "sensitive_information":
                # ä¿®å¤æ•æ„Ÿä¿¡æ¯ï¼ˆç§»é™¤å¹¶æ·»åŠ å ä½ç¬¦ï¼‰
                if issue.line_number <= len(lines):
                    line = lines[issue.line_number - 1]
                    # æ›¿æ¢ä¸ºç¯å¢ƒå˜é‡å¼•ç”¨
                    line = re.sub(r"=\s*['\"][^'\"]+['\"]", "= os.getenv('SENSITIVE_VALUE')", line)
                    lines[issue.line_number - 1] = line
            
            # å†™å›æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines))
        
        except Exception as e:
            logger.error(f"åº”ç”¨ä¿®å¤å¤±è´¥: {e}")
            raise

    async def _execute_refactoring(self):
        """åˆ†æä»£ç é‡æ„æœºä¼š - æ£€æµ‹æ¨¡å¼"""
        logger.info("    åˆ†æä»£ç é‡æ„æœºä¼š...")
        
        try:
            # 1. æ£€æµ‹é‡æ„æœºä¼š
            refactor_opportunities = await self._detect_refactor_opportunities()
            
            # 2. ç”Ÿæˆé‡æ„åˆ†ææŠ¥å‘Š
            refactor_report = await self._generate_refactor_analysis_report(refactor_opportunities)
            
            # 3. ä¿å­˜é‡æ„å»ºè®®æ•°æ®
            await self._save_refactor_suggestions(refactor_opportunities, refactor_report)
            
            logger.info(f"      âœ… ä»£ç é‡æ„åˆ†æå®Œæˆï¼Œå‘ç° {len(refactor_opportunities)} ä¸ªé‡æ„æœºä¼š")
            
        except Exception as e:
            logger.error(f"      âŒ ä»£ç é‡æ„åˆ†æå¤±è´¥: {e}")

    async def _generate_refactor_analysis_report(self, opportunities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆé‡æ„åˆ†ææŠ¥å‘Š"""
        report = {
            "summary": {
                "total_files_with_issues": len(opportunities),
                "total_refactor_opportunities": sum(len(opp["issues"]) for opp in opportunities),
                "priority_distribution": defaultdict(int),
                "complexity_levels": defaultdict(int)
            },
            "detailed_analysis": [],
            "ai_suggestions": [],
            "estimated_effort": {}
        }
        
        for file_opportunity in opportunities:
            file_analysis = {
                "file_path": file_opportunity["file_path"],
                "file_size": self._get_file_size(file_opportunity["file_path"]),
                "issues": []
            }
            
            for issue in file_opportunity["issues"]:
                issue_analysis = {
                    "type": issue["type"],
                    "description": issue["description"],
                    "severity": issue["severity"],
                    "location": issue["location"],
                    "context": await self._extract_code_context(file_opportunity["file_path"], issue["location"]),
                    "impact_analysis": await self._analyze_issue_impact(issue),
                    "ai_fix_strategy": await self._generate_ai_fix_strategy(issue),
                    "estimated_complexity": self._estimate_fix_complexity(issue),
                    "dependencies": await self._identify_dependencies(file_opportunity["file_path"], issue)
                }
                
                file_analysis["issues"].append(issue_analysis)
                report["summary"]["priority_distribution"][issue["severity"]] += 1
            
            report["detailed_analysis"].append(file_analysis)
        
        # ç”ŸæˆAIå‹å¥½çš„å»ºè®®
        report["ai_suggestions"] = await self._generate_ai_friendly_suggestions(opportunities)
        
        # ä¼°ç®—å·¥ä½œé‡
        report["estimated_effort"] = self._calculate_total_effort(opportunities)
        
        return report

    async def _extract_code_context(self, file_path: str, location: Dict[str, Any]) -> str:
        """æå–ä»£ç ä¸Šä¸‹æ–‡"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            line_num = location.get("line", 1)
            start = max(0, line_num - 3)
            end = min(len(lines), line_num + 2)
            
            context_lines = []
            for i in range(start, end):
                context_lines.append(f"{i+1:4d}: {lines[i].rstrip()}")
            
            return '\n'.join(context_lines)
        
        except Exception as e:
            return f"æ— æ³•æå–ä¸Šä¸‹æ–‡: {e}"

    async def _analyze_issue_impact(self, issue: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé—®é¢˜å½±å“"""
        impact = {
            "maintainability": "medium",
            "readability": "medium", 
            "performance": "low",
            "security": "low",
            "testability": "low"
        }
        
        # æ ¹æ®é—®é¢˜ç±»å‹è°ƒæ•´å½±å“
        if issue["type"] == "duplicate_code":
            impact["maintainability"] = "high"
            impact["readability"] = "medium"
        elif issue["type"] == "long_function":
            impact["maintainability"] = "high"
            impact["testability"] = "medium"
        elif issue["type"] == "complex_condition":
            impact["readability"] = "high"
            impact["maintainability"] = "medium"
        
        return impact

    async def _generate_ai_fix_strategy(self, issue: Dict[str, Any]) -> str:
        """ç”ŸæˆAIä¿®å¤ç­–ç•¥æè¿°"""
        strategies = {
            "duplicate_code": "æå–é‡å¤ä»£ç ä¸ºç‹¬ç«‹å‡½æ•°ï¼Œä½¿ç”¨å‚æ•°åŒ–å¤„ç†å·®å¼‚ï¼Œç¡®ä¿å‡½æ•°èŒè´£å•ä¸€",
            "long_function": "å°†é•¿å‡½æ•°åˆ†è§£ä¸ºå¤šä¸ªå°å‡½æ•°ï¼Œæ¯ä¸ªå‡½æ•°è´Ÿè´£å•ä¸€èŒè´£ï¼Œæé«˜å¯è¯»æ€§å’Œå¯æµ‹è¯•æ€§",
            "complex_condition": "å°†å¤æ‚æ¡ä»¶è¡¨è¾¾å¼æå–ä¸ºæœ‰æ„ä¹‰çš„å˜é‡åæˆ–è¾…åŠ©å‡½æ•°ï¼Œæé«˜ä»£ç å¯è¯»æ€§",
            "magic_numbers": "å°†é­”æ³•æ•°å­—æå–ä¸ºå‘½åå¸¸é‡ï¼Œä½¿ç”¨æè¿°æ€§åç§°ï¼Œæé«˜ä»£ç å¯ç»´æŠ¤æ€§"
        }
        
        return strategies.get(issue["type"], "æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œé‡æ„ï¼Œéµå¾ªå•ä¸€èŒè´£åŸåˆ™")

    def _estimate_fix_complexity(self, issue: Dict[str, Any]) -> str:
        """ä¼°ç®—ä¿®å¤å¤æ‚åº¦"""
        complexity_map = {
            "duplicate_code": "medium",
            "long_function": "high", 
            "complex_condition": "low",
            "magic_numbers": "low"
        }
        
        return complexity_map.get(issue["type"], "medium")

    async def _identify_dependencies(self, file_path: str, issue: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«ä¾èµ–å…³ç³»"""
        dependencies = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç®€å•çš„ä¾èµ–åˆ†æ
            if issue["type"] == "long_function":
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¼å…¥æ–°æ¨¡å—
                if "utils" not in content and "helper" not in content:
                    dependencies.append("å¯èƒ½éœ€è¦åˆ›å»ºå·¥å…·å‡½æ•°æ¨¡å—")
            
            elif issue["type"] == "magic_numbers":
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¸¸é‡æ–‡ä»¶
                if "constants" not in content and "config" not in content:
                    dependencies.append("å»ºè®®åˆ›å»ºå¸¸é‡é…ç½®æ–‡ä»¶")
        
        except Exception:
            pass
        
        return dependencies

    async def _generate_ai_friendly_suggestions(self, opportunities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”ŸæˆAIå‹å¥½çš„å»ºè®®"""
        suggestions = []
        
        for file_opportunity in opportunities:
            for issue in file_opportunity["issues"]:
                suggestion = {
                    "file": file_opportunity["file_path"],
                    "issue_type": issue["type"],
                    "description": issue["description"],
                    "severity": issue["severity"],
                    "context": await self._extract_code_context(file_opportunity["file_path"], issue["location"]),
                    "recommended_action": await self._generate_ai_fix_strategy(issue),
                    "reasoning": await self._generate_fix_reasoning(issue),
                    "implementation_notes": await self._generate_implementation_notes(issue),
                    "test_suggestions": await self._generate_test_suggestions(issue),
                    "impact_assessment": await self._analyze_issue_impact(issue)
                }
                suggestions.append(suggestion)
        
        return suggestions

    async def _generate_fix_reasoning(self, issue: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¿®å¤ç†ç”±"""
        reasoning_map = {
            "duplicate_code": "é‡å¤ä»£ç è¿åDRYåŸåˆ™ï¼Œå¢åŠ ç»´æŠ¤æˆæœ¬ï¼Œä¿®æ”¹æ—¶éœ€è¦åœ¨å¤šå¤„åŒæ­¥æ›´æ–°",
            "long_function": "é•¿å‡½æ•°è¿åå•ä¸€èŒè´£åŸåˆ™ï¼Œéš¾ä»¥ç†è§£å’Œæµ‹è¯•ï¼Œå¢åŠ è®¤çŸ¥è´Ÿæ‹…",
            "complex_condition": "å¤æ‚æ¡ä»¶é™ä½ä»£ç å¯è¯»æ€§ï¼Œå¢åŠ å‡ºé”™æ¦‚ç‡ï¼Œéš¾ä»¥è°ƒè¯•å’Œç»´æŠ¤",
            "magic_numbers": "é­”æ³•æ•°å­—ç¼ºä¹è¯­ä¹‰ï¼Œé™ä½ä»£ç å¯è¯»æ€§ï¼Œéš¾ä»¥ç†è§£å’Œä¿®æ”¹"
        }
        
        return reasoning_map.get(issue["type"], "éµå¾ªè½¯ä»¶å·¥ç¨‹æœ€ä½³å®è·µï¼Œæé«˜ä»£ç è´¨é‡")

    async def _generate_implementation_notes(self, issue: Dict[str, Any]) -> str:
        """ç”Ÿæˆå®ç°è¯´æ˜"""
        notes_map = {
            "duplicate_code": "1. è¯†åˆ«é‡å¤ä»£ç å— 2. æå–ä¸ºç‹¬ç«‹å‡½æ•° 3. å‚æ•°åŒ–å·®å¼‚éƒ¨åˆ† 4. æ›¿æ¢æ‰€æœ‰è°ƒç”¨ç‚¹",
            "long_function": "1. è¯†åˆ«å‡½æ•°èŒè´£ 2. æŒ‰èŒè´£åˆ†ç»„ 3. æå–å­å‡½æ•° 4. ä¿æŒæ¥å£ä¸€è‡´æ€§",
            "complex_condition": "1. è¯†åˆ«æ¡ä»¶é€»è¾‘ 2. æå–ä¸ºå˜é‡ 3. ä½¿ç”¨è¾…åŠ©å‡½æ•° 4. æ·»åŠ æ³¨é‡Šè¯´æ˜",
            "magic_numbers": "1. è¯†åˆ«é­”æ³•æ•°å­— 2. ç¡®å®šè¯­ä¹‰ 3. åˆ›å»ºå¸¸é‡ 4. æ›¿æ¢æ‰€æœ‰ä½¿ç”¨ç‚¹"
        }
        
        return notes_map.get(issue["type"], "æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œè¯¦ç»†å®ç°")

    async def _generate_test_suggestions(self, issue: Dict[str, Any]) -> str:
        """ç”Ÿæˆæµ‹è¯•å»ºè®®"""
        test_map = {
            "duplicate_code": "ä¸ºæå–çš„å‡½æ•°ç¼–å†™å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿é‡æ„ååŠŸèƒ½ä¸€è‡´ï¼Œæ·»åŠ è¾¹ç•Œæ¡ä»¶æµ‹è¯•",
            "long_function": "ä¸ºæ¯ä¸ªå­å‡½æ•°ç¼–å†™ç‹¬ç«‹æµ‹è¯•ï¼ŒéªŒè¯æ‹†åˆ†åçš„è¡Œä¸ºï¼Œæ·»åŠ é›†æˆæµ‹è¯•",
            "complex_condition": "æµ‹è¯•å„ç§æ¡ä»¶ç»„åˆï¼ŒéªŒè¯é€»è¾‘æ­£ç¡®æ€§ï¼Œæ·»åŠ è¾¹ç•Œå€¼æµ‹è¯•",
            "magic_numbers": "æµ‹è¯•å¸¸é‡å€¼å˜æ›´çš„å½±å“ï¼ŒéªŒè¯é…ç½®çµæ´»æ€§ï¼Œæ·»åŠ å‚æ•°åŒ–æµ‹è¯•"
        }
        
        return test_map.get(issue["type"], "ç¼–å†™ç›¸åº”çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•")

    def _calculate_total_effort(self, opportunities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—æ€»å·¥ä½œé‡"""
        effort = {
            "total_issues": sum(len(opp["issues"]) for opp in opportunities),
            "estimated_hours": 0,
            "complexity_breakdown": defaultdict(int),
            "priority_breakdown": defaultdict(int)
        }
        
        for file_opportunity in opportunities:
            for issue in file_opportunity["issues"]:
                # ä¼°ç®—æ¯ä¸ªé—®é¢˜çš„å·¥ä½œé‡ï¼ˆå°æ—¶ï¼‰
                hours_map = {
                    "duplicate_code": 2,
                    "long_function": 4,
                    "complex_condition": 1,
                    "magic_numbers": 0.5
                }
                
                hours = hours_map.get(issue["type"], 2)
                effort["estimated_hours"] += hours
                effort["complexity_breakdown"][self._estimate_fix_complexity(issue)] += 1
                effort["priority_breakdown"][issue["severity"]] += 1
        
        return effort

    async def _save_refactor_suggestions(self, opportunities: List[Dict[str, Any]], report: Dict[str, Any]):
        """ä¿å­˜é‡æ„å»ºè®®æ•°æ®"""
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = self.workspace_path / ".iflow" / "analysis_results"
            output_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
            analysis_file = output_dir / f"refactor_analysis_{self.session_id}.json"
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "session_id": self.session_id,
                    "timestamp": datetime.now().isoformat(),
                    "opportunities": opportunities,
                    "report": report
                }, f, indent=2, ensure_ascii=False, default=str)
            
            # ä¿å­˜AIå‹å¥½çš„å»ºè®®
            suggestions_file = output_dir / f"ai_suggestions_{self.session_id}.json"
            with open(suggestions_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "session_id": self.session_id,
                    "timestamp": datetime.now().isoformat(),
                    "suggestions": report["ai_suggestions"],
                    "metadata": {
                        "total_suggestions": len(report["ai_suggestions"]),
                        "estimated_hours": report["estimated_effort"]["estimated_hours"],
                        "priority_distribution": dict(report["summary"]["priority_distribution"])
                    }
                }, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"      ğŸ“‹ é‡æ„åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {analysis_file}")
            logger.info(f"      ğŸ¤– AIå»ºè®®æ•°æ®å·²ä¿å­˜: {suggestions_file}")
            
        except Exception as e:
            logger.error(f"      âŒ ä¿å­˜é‡æ„å»ºè®®å¤±è´¥: {e}")

    def _get_file_size(self, file_path: str) -> int:
        """è·å–æ–‡ä»¶å¤§å°"""
        try:
            return Path(file_path).stat().st_size
        except:
            return 0

    async def _detect_refactor_opportunities(self) -> List[Dict[str, Any]]:
        """æ£€æµ‹é‡æ„æœºä¼š"""
        logger.info("      æ£€æµ‹é‡æ„æœºä¼š...")
        
        opportunities = []
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    file_opportunities = {
                        "file_path": str(file_path),
                        "issues": []
                    }
                    
                    # æ£€æµ‹é‡å¤ä»£ç 
                    duplicates = self._detect_duplicate_code(content)
                    if duplicates:
                        file_opportunities["issues"].append({
                            "type": "duplicate_code",
                            "description": "å‘ç°é‡å¤ä»£ç å—",
                            "locations": duplicates,
                            "severity": "medium"
                        })
                    
                    # æ£€æµ‹é•¿å‡½æ•°
                    long_functions = self._detect_long_functions(content)
                    for func in long_functions:
                        file_opportunities["issues"].append({
                            "type": "long_function",
                            "description": f"å‡½æ•° {func['name']} è¿‡é•¿ ({func['lines']} è¡Œ)",
                            "location": func,
                            "severity": "medium"
                        })
                    
                    # æ£€æµ‹å¤æ‚æ¡ä»¶
                    complex_conditions = self._detect_complex_conditions(content)
                    for condition in complex_conditions:
                        file_opportunities["issues"].append({
                            "type": "complex_condition",
                            "description": "å¤æ‚çš„æ¡ä»¶è¡¨è¾¾å¼",
                            "location": condition,
                            "severity": "low"
                        })
                    
                    # æ£€æµ‹é­”æ³•æ•°å­—
                    magic_numbers = self._detect_magic_numbers(content)
                    if magic_numbers:
                        file_opportunities["issues"].append({
                            "type": "magic_numbers",
                            "description": f"å‘ç° {len(magic_numbers)} ä¸ªé­”æ³•æ•°å­—",
                            "locations": magic_numbers,
                            "severity": "low"
                        })
                    
                    if file_opportunities["issues"]:
                        opportunities.append(file_opportunities)
                
                except Exception as e:
                    logger.warning(f"æ£€æµ‹é‡æ„æœºä¼šå¤±è´¥ {file_path}: {e}")
        
        logger.info(f"        æ£€æµ‹åˆ° {len(opportunities)} ä¸ªæ–‡ä»¶çš„é‡æ„æœºä¼š")
        return opportunities

    def _detect_duplicate_code(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹é‡å¤ä»£ç """
        lines = content.split('\n')
        duplicates = []
        
        # æŸ¥æ‰¾3è¡Œä»¥ä¸Šçš„é‡å¤å—
        code_blocks = defaultdict(list)
        
        for i in range(len(lines) - 2):
            block = '\n'.join(lines[i:i+3]).strip()
            if len(block) > 30:  # å¿½ç•¥å¤ªçŸ­çš„å—
                code_blocks[block].append(i + 1)
        
        for block, line_numbers in code_blocks.items():
            if len(line_numbers) > 1:
                duplicates.append({
                    "block": block[:100] + "..." if len(block) > 100 else block,
                    "locations": line_numbers
                })
        
        return duplicates

    def _detect_long_functions(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹é•¿å‡½æ•°"""
        try:
            tree = ast.parse(content)
            long_functions = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # è®¡ç®—å‡½æ•°è¡Œæ•°
                    start_line = node.lineno
                    end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line
                    lines_count = end_line - start_line + 1
                    
                    if lines_count > 20:  # é•¿å‡½æ•°é˜ˆå€¼
                        long_functions.append({
                            "name": node.name,
                            "lines": lines_count,
                            "start_line": start_line,
                            "end_line": end_line
                        })
            
            return long_functions
        except:
            return []

    def _detect_complex_conditions(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¤æ‚æ¡ä»¶"""
        complex_conditions = []
        lines = content.split('\n')
        
        for i, line in enumerate(lines):
            # æ£€æŸ¥å¤æ‚çš„å¸ƒå°”è¡¨è¾¾å¼
            if (' and ' in line and line.count(' and ') > 2) or \
               (' or ' in line and line.count(' or ') > 2) or \
               (line.count('(') > 3 and line.count(')') > 3):
                complex_conditions.append({
                    "line": i + 1,
                    "content": line.strip()
                })
        
        return complex_conditions

    def _detect_magic_numbers(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹é­”æ³•æ•°å­—"""
        magic_numbers = []
        lines = content.split('\n')
        
        for i, line in enumerate(lines):
            # æŸ¥æ‰¾å¤§äº10çš„æ•°å­—ï¼ˆæ’é™¤ç‰ˆæœ¬å·ç­‰ï¼‰
            numbers = re.findall(r'\b([1-9]\d{2,})\b', line)
            for num in numbers:
                # æ’é™¤ä¸€äº›å¸¸è§çš„æ•°å­—
                if num not in ['100', '1000', '1024', '2048', '4096']:
                    magic_numbers.append({
                        "value": num,
                        "line": i + 1,
                        "context": line.strip()
                    })
        
        return magic_numbers

    async def _ai_analyze_refactor_plan(self, opportunities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """AIåˆ†æå’Œè§„åˆ’é‡æ„"""
        logger.info("      AIåˆ†æé‡æ„è®¡åˆ’...")
        
        # è¿™é‡Œè°ƒç”¨AIæ¥åˆ†æé‡æ„æœºä¼šå¹¶åˆ¶å®šè®¡åˆ’
        refactor_plan = {
            "priority_actions": [],
            "safe_actions": [],
            "risky_actions": [],
            "estimated_time": 0,
            "dependencies": []
        }
        
        for file_opportunity in opportunities:
            for issue in file_opportunity["issues"]:
                action = {
                    "file_path": file_opportunity["file_path"],
                    "issue_type": issue["type"],
                    "description": issue["description"],
                    "severity": issue["severity"],
                    "ai_suggestion": await self._generate_ai_suggestion(issue),
                    "confidence": 0.8  # AIå»ºè®®çš„ç½®ä¿¡åº¦
                }
                
                # æ ¹æ®é—®é¢˜ç±»å‹å’Œä¸¥é‡ç¨‹åº¦åˆ†ç±»
                if issue["severity"] == "medium" and issue["type"] in ["duplicate_code", "long_function"]:
                    refactor_plan["priority_actions"].append(action)
                elif issue["severity"] == "low":
                    refactor_plan["safe_actions"].append(action)
                else:
                    refactor_plan["risky_actions"].append(action)
        
        # ä¼°ç®—æ—¶é—´
        refactor_plan["estimated_time"] = len(refactor_plan["priority_actions"]) * 15 + \
                                       len(refactor_plan["safe_actions"]) * 10 + \
                                       len(refactor_plan["risky_actions"]) * 25
        
        logger.info(f"        AIè§„åˆ’å®Œæˆ: {len(refactor_plan['priority_actions'])} ä¸ªä¼˜å…ˆæ“ä½œ")
        return refactor_plan

    async def _generate_ai_suggestion(self, issue: Dict[str, Any]) -> str:
        """ç”ŸæˆAIå»ºè®®"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨çœŸæ­£çš„AIæ¨¡å‹æ¥ç”Ÿæˆåˆ†æå’Œå»ºè®®
        suggestions = {
            "duplicate_code": "å»ºè®®æå–é‡å¤ä»£ç ä¸ºç‹¬ç«‹å‡½æ•°ï¼Œä½¿ç”¨å‚æ•°åŒ–æ¥å¤„ç†å·®å¼‚",
            "long_function": "å»ºè®®å°†é•¿å‡½æ•°åˆ†è§£ä¸ºå¤šä¸ªæ›´å°çš„ã€èŒè´£å•ä¸€çš„å‡½æ•°",
            "complex_condition": "å»ºè®®å°†å¤æ‚æ¡ä»¶æå–ä¸ºæœ‰æ„ä¹‰çš„å˜é‡åæˆ–è¾…åŠ©å‡½æ•°",
            "magic_numbers": "å»ºè®®å°†é­”æ³•æ•°å­—æå–ä¸ºå‘½åå¸¸é‡ï¼Œæé«˜ä»£ç å¯è¯»æ€§"
        }
        
        return suggestions.get(issue["type"], "å»ºè®®è¿›è¡Œä»£ç é‡æ„ä»¥æé«˜è´¨é‡")

    async def _ai_execute_refactoring(self, refactor_plan: Dict[str, Any]):
        """ç”Ÿæˆé‡æ„å»ºè®®æŠ¥å‘Šï¼ˆä¸æ‰§è¡Œå®é™…ä¿®å¤ï¼‰"""
        logger.info("      ç”Ÿæˆé‡æ„å»ºè®®æŠ¥å‘Š...")
        
        # æŒ‰ä¼˜å…ˆçº§ç”Ÿæˆå»ºè®®
        all_actions = refactor_plan["priority_actions"] + refactor_plan["safe_actions"]
        
        for action in all_actions:
            logger.info(f"        ğŸ“‹ é‡æ„å»ºè®®: {action['description']}")
            logger.info(f"           æ–‡ä»¶: {action['file_path']}")
            logger.info(f"           AIå»ºè®®: {action['ai_suggestion']}")
            logger.info(f"           ç½®ä¿¡åº¦: {action['confidence']:.2f}")
            
            # è®°å½•åˆ°changelogä½œä¸ºå»ºè®®
            self.changelog.append({
                "timestamp": datetime.now().isoformat(),
                "action": "refactor_suggestion",
                "issue": action["description"],
                "file": action["file_path"],
                "status": "suggested",
                "ai_suggestion": action["ai_suggestion"],
                "confidence": action["confidence"]
            })
        
        logger.info(f"        ç”Ÿæˆäº† {len(all_actions)} ä¸ªé‡æ„å»ºè®®")

    async def _ai_apply_refactor_action(self, action: Dict[str, Any]) -> bool:
        """AIåº”ç”¨é‡æ„æ“ä½œ"""
        try:
            file_path = Path(action["file_path"])
            
            if not file_path.exists():
                return False
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ ¹æ®é—®é¢˜ç±»å‹åº”ç”¨ä¸åŒçš„AIé‡æ„ç­–ç•¥
            if action["issue_type"] == "duplicate_code":
                new_content = await self._ai_refactor_duplicate_code(content, action)
            elif action["issue_type"] == "long_function":
                new_content = await self._ai_refactor_long_function(content, action)
            elif action["issue_type"] == "complex_condition":
                new_content = await self._ai_refactor_complex_condition(content, action)
            elif action["issue_type"] == "magic_numbers":
                new_content = await self._ai_refactor_magic_numbers(content, action)
            else:
                return False
            
            # å¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼Œå†™å…¥æ–‡ä»¶
            if new_content != content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                return True
            
            return False
        
        except Exception as e:
            logger.error(f"AIåº”ç”¨é‡æ„æ“ä½œå¤±è´¥: {e}")
            return False

    async def _ai_refactor_duplicate_code(self, content: str, action: Dict[str, Any]) -> str:
        """AIé‡æ„é‡å¤ä»£ç """
        # è¿™é‡Œåº”è¯¥è°ƒç”¨AIæ¥ç†è§£ä»£ç ä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆé‡æ„ä»£ç 
        # ç°åœ¨ç”¨åˆ†æé€»è¾‘ç”Ÿæˆå»ºè®®
        logger.info(f"          AIåˆ†æé‡å¤ä»£ç : {action['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›é‡æ„åçš„å†…å®¹

    async def _ai_refactor_long_function(self, content: str, action: Dict[str, Any]) -> str:
        """AIé‡æ„é•¿å‡½æ•°"""
        logger.info(f"          AIåˆ†æé•¿å‡½æ•°: {action['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›é‡æ„åçš„å†…å®¹

    async def _ai_refactor_complex_condition(self, content: str, action: Dict[str, Any]) -> str:
        """AIé‡æ„å¤æ‚æ¡ä»¶"""
        logger.info(f"          AIåˆ†æå¤æ‚æ¡ä»¶: {action['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›é‡æ„åçš„å†…å®¹

    async def _ai_refactor_magic_numbers(self, content: str, action: Dict[str, Any]) -> str:
        """AIé‡æ„é­”æ³•æ•°å­—"""
        logger.info(f"          AIåˆ†æé­”æ³•æ•°å­—: {action['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›é‡æ„åçš„å†…å®¹

    async def _extract_duplicate_code(self):
        """æå–é‡å¤ä»£ç ä¸ºå‡½æ•°"""
        logger.info("      æå–é‡å¤ä»£ç ...")
        
        # åˆ†æé‡å¤ä»£ç å—
        code_blocks = defaultdict(list)
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    # æŸ¥æ‰¾3è¡Œä»¥ä¸Šçš„é‡å¤å—
                    for i in range(len(lines) - 2):
                        block = ''.join(lines[i:i+3]).strip()
                        if len(block) > 30:  # å¿½ç•¥å¤ªçŸ­çš„å—
                            code_blocks[block].append((str(file_path), i+1))
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†æ {file_path}: {e}")
        
        # æå–é‡å¤ä»£ç 
        for block, occurrences in code_blocks.items():
            if len(occurrences) > 1 and not self.analysis_mode:
                # ç”Ÿæˆå‡½æ•°å
                func_name = f"extracted_function_{hashlib.md5(block.encode()).hexdigest()[:8]}"
                
                # åœ¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ä¸­åˆ›å»ºå‡½æ•°
                first_file, first_line = occurrences[0]
                await self._create_extracted_function(first_file, func_name, block)
                
                # æ›¿æ¢æ‰€æœ‰å‡ºç°çš„ä½ç½®
                for file_path, line_num in occurrences:
                    await self._replace_with_function_call(file_path, line_num, func_name, block)
                
                logger.info(f"        æå–é‡å¤ä»£ç ä¸ºå‡½æ•°: {func_name}")

    async def _create_extracted_function(self, file_path: str, func_name: str, block: str):
        """åˆ›å»ºæå–çš„å‡½æ•°"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å‡½æ•°
            func_def = f"def {func_name}():\n    # æå–çš„é‡å¤ä»£ç \n"
            for line in block.split('\n'):
                if line.strip():
                    func_def += f"    {line}\n"
            func_def += "\n\n"
            
            # åœ¨ç¬¬ä¸€ä¸ªç±»æˆ–å‡½æ•°ä¹‹å‰æ’å…¥
            lines = content.split('\n')
            insert_pos = 0
            for i, line in enumerate(lines):
                if line.startswith('class ') or line.startswith('def '):
                    insert_pos = i
                    break
            
            lines.insert(insert_pos, func_def)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines))
        
        except Exception as e:
            logger.error(f"åˆ›å»ºå‡½æ•°å¤±è´¥: {e}")

    async def _replace_with_function_call(self, file_path: str, line_num: int, func_name: str, block: str):
        """ç”¨å‡½æ•°è°ƒç”¨æ›¿æ¢é‡å¤ä»£ç """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æ›¿æ¢é‡å¤ä»£ç å—
            block_lines = block.split('\n')
            for i, line in enumerate(block_lines):
                if line_num + i - 1 < len(lines):
                    if i == 0:
                        lines[line_num + i - 1] = f"    {func_name}()\n"
                    else:
                        lines[line_num + i - 1] = "\n"
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.writelines(lines)
        
        except Exception as e:
            logger.error(f"æ›¿æ¢å‡½æ•°è°ƒç”¨å¤±è´¥: {e}")

    async def _refactor_long_functions(self):
        """é‡æ„é•¿å‡½æ•°"""
        logger.info("      é‡æ„é•¿å‡½æ•°...")
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    tree = ast.parse(content)
                    
                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef):
                            # è®¡ç®—å‡½æ•°è¡Œæ•°
                            func_lines = len(node.body)
                            if func_lines > 20:  # é•¿å‡½æ•°é˜ˆå€¼
                                await self._break_down_long_function(file_path, node.name)
                                logger.info(f"        é‡æ„é•¿å‡½æ•°: {node.name} ({func_lines} è¡Œ)")
                
                except Exception as e:
                    logger.warning(f"é‡æ„é•¿å‡½æ•°å¤±è´¥ {file_path}: {e}")

    async def _break_down_long_function(self, file_path: Path, func_name: str):
        """åˆ†è§£é•¿å‡½æ•°"""
        # è¿™é‡Œå®ç°é•¿å‡½æ•°åˆ†è§£é€»è¾‘
        # åˆ†æå‡½æ•°é€»è¾‘å—ï¼Œæå–ä¸ºå­å‡½æ•°
        pass

    async def _optimize_class_structure(self):
        """ä¼˜åŒ–ç±»ç»“æ„"""
        logger.info("      ä¼˜åŒ–ç±»ç»“æ„...")
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    tree = ast.parse(content)
                    
                    for node in ast.walk(tree):
                        if isinstance(node, ast.ClassDef):
                            # æ£€æŸ¥ç±»çš„æ–¹æ³•æ•°é‡
                            methods = [n for n in node.body if isinstance(n, ast.FunctionDef)]
                            if len(methods) > 15:  # æ–¹æ³•è¿‡å¤š
                                await self._split_large_class(file_path, node.name)
                                logger.info(f"        ä¼˜åŒ–å¤§ç±»: {node.name} ({len(methods)} ä¸ªæ–¹æ³•)")
                
                except Exception as e:
                    logger.warning(f"ä¼˜åŒ–ç±»ç»“æ„å¤±è´¥ {file_path}: {e}")

    async def _split_large_class(self, file_path: Path, class_name: str):
        """æ‹†åˆ†å¤§ç±»"""
        # è¿™é‡Œå®ç°å¤§ç±»æ‹†åˆ†é€»è¾‘
        # æ ¹æ®èŒè´£å°†å¤§ç±»æ‹†åˆ†ä¸ºå¤šä¸ªå°ç±»
        pass

    async def _refactor_conditionals(self):
        """é‡æ„æ¡ä»¶è¡¨è¾¾å¼"""
        logger.info("      é‡æ„æ¡ä»¶è¡¨è¾¾å¼...")
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ç®€åŒ–å¤æ‚çš„æ¡ä»¶è¡¨è¾¾å¼
                    original_content = content
                    
                    # å°†é•¿if-elifé“¾æ›¿æ¢ä¸ºå­—å…¸æŸ¥æ‰¾
                    content = self._simplify_if_elif_chain(content)
                    
                    # å°†åµŒå¥—æ¡ä»¶æå–ä¸ºå˜é‡
                    content = self._extract_nested_conditions(content)
                    
                    if content != original_content and not self.analysis_mode:
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(content)
                        logger.info(f"        é‡æ„æ¡ä»¶è¡¨è¾¾å¼: {file_path.name}")
                
                except Exception as e:
                    logger.warning(f"é‡æ„æ¡ä»¶è¡¨è¾¾å¼å¤±è´¥ {file_path}: {e}")

    def _simplify_if_elif_chain(self, content: str) -> str:
        """ç®€åŒ–if-elifé“¾"""
        # æŸ¥æ‰¾å¯ä»¥è½¬æ¢ä¸ºå­—å…¸æŸ¥æ‰¾çš„if-elifé“¾
        pattern = r'if\s+(\w+)\s*==\s*(\w+):\s*\n(.*?)\nelif\s+\1\s*==\s*(\w+):\s*\n(.*?)(?=\n(?:elif|else|if|\Z))'
        
        def replace_chain(match):
            var_name = match.group(1)
            replacements = []
            
            # æå–æ‰€æœ‰æ¡ä»¶-å€¼å¯¹
            current_match = match
            while current_match:
                condition = current_match.group(2)
                value = current_match.group(3).strip()
                replacements.append(f'"{condition}": {value}')
                
                # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªelif
                rest = content[current_match.end():]
                next_match = re.search(r'elif\s+' + re.escape(var_name) + r'\s*==\s*(\w+):\s*\n(.*?)(?=\n(?:elif|else|if|\Z))', rest)
                if next_match:
                    current_match = next_match
                    current_match = type('Match', (), {
                        'group': lambda i, cm=current_match, nm=next_match: (
                            nm.group(i) if i <= 2 else 
                            (cm.group(i) if i == 3 else nm.group(i-2))
                        )[i],
                        'end': lambda cm=current_match, nm=next_match: cm.end() + nm.end()
                    })()
                else:
                    break
            
            # åˆ›å»ºå­—å…¸æŸ¥æ‰¾
            dict_def = f"{var_name}_map = {{\n        " + ",\n        ".join(replacements) + "\n    }"
            lookup = f"result = {var_name}_map.get({var_name}, default_value)"
            
            return f"{dict_def}\n    {lookup}"
        
        return re.sub(pattern, replace_chain, content, flags=re.DOTALL)

    def _extract_nested_conditions(self, content: str) -> str:
        """æå–åµŒå¥—æ¡ä»¶"""
        # æŸ¥æ‰¾å¤æ‚çš„åµŒå¥—æ¡ä»¶å¹¶æå–ä¸ºå˜é‡
        pattern = r'if\s+([^:]+):\s*\n(.*?)\n(?:else|elif)'
        
        def extract_condition(match):
            condition = match.group(1)
            if ' and ' in condition or ' or ' in condition:
                # ç”Ÿæˆå˜é‡å
                var_name = f"condition_{hashlib.md5(condition.encode()).hexdigest()[:6]}"
                # æå–ä¸ºå˜é‡
                return f"{var_name} = {condition}\n    if {var_name}:\n{match.group(2)}"
            return match.group(0)
        
        return re.sub(pattern, extract_condition, content, flags=re.DOTALL)

    async def _extract_constants(self):
        """æå–å¸¸é‡"""
        logger.info("      æå–å¸¸é‡...")
        
        # æŸ¥æ‰¾ç¡¬ç¼–ç çš„æ•°å€¼å’Œå­—ç¬¦ä¸²
        magic_numbers = defaultdict(list)
        magic_strings = defaultdict(list)
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æŸ¥æ‰¾é­”æ³•æ•°å­—ï¼ˆå¤§äº10çš„æ•°å­—ï¼‰
                    numbers = re.findall(r'\b([1-9]\d{2,})\b', content)
                    for num in numbers:
                        magic_numbers[num].append(str(file_path))
                    
                    # æŸ¥æ‰¾é‡å¤çš„å­—ç¬¦ä¸²å­—é¢é‡
                    strings = re.findall(r'["\']([^"\']{10,})["\']', content)
                    for string in strings:
                        if not string.islower():  # æ’é™¤æ™®é€šçš„å°å†™å­—ç¬¦ä¸²
                            magic_strings[string].append(str(file_path))
                
                except Exception as e:
                    logger.warning(f"æå–å¸¸é‡å¤±è´¥ {file_path}: {e}")
        
        # åˆ›å»ºå¸¸é‡å®šä¹‰
        if magic_numbers or magic_strings:
            await self._create_constants_file(magic_numbers, magic_strings)
            logger.info(f"        æå–äº† {len(magic_numbers)} ä¸ªæ•°å­—å¸¸é‡å’Œ {len(magic_strings)} ä¸ªå­—ç¬¦ä¸²å¸¸é‡")

    async def _create_constants_file(self, numbers: Dict, strings: Dict):
        """åˆ›å»ºå¸¸é‡æ–‡ä»¶"""
        if self.analysis_mode:
            return
        
        constants_file = self.workspace_path / "constants.py"
        
        try:
            existing_content = ""
            if constants_file.exists():
                with open(constants_file, 'r', encoding='utf-8') as f:
                    existing_content = f.read()
            
            # ç”Ÿæˆæ–°çš„å¸¸é‡å®šä¹‰
            new_constants = ["# è‡ªåŠ¨ç”Ÿæˆçš„å¸¸é‡å®šä¹‰\n"]
            
            for num, files in numbers.items():
                if len(files) > 1:  # åªæå–å¤šæ¬¡ä½¿ç”¨çš„æ•°å­—
                    const_name = f"VALUE_{num}"
                    new_constants.append(f"{const_name} = {num}  # ç”¨äº: {', '.join([Path(f).name for f in files[:3]])}")
            
            for string, files in strings.items():
                if len(files) > 1:  # åªæå–é‡å¤çš„å­—ç¬¦ä¸²
                    const_name = f"TEXT_{hashlib.md5(string.encode()).hexdigest()[:8].upper()}"
                    # è½¬ä¹‰å­—ç¬¦ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                    escaped_string = string.replace('"', '\\"')
                    new_constants.append(f'{const_name} = "{escaped_string}"  # ç”¨äº: {", ".join([Path(f).name for f in files[:3]])}')
            
            # å†™å…¥æ–‡ä»¶
            with open(constants_file, 'w', encoding='utf-8') as f:
                f.write(existing_content + "\n" + "\n".join(new_constants))
        
        except Exception as e:
            logger.error(f"åˆ›å»ºå¸¸é‡æ–‡ä»¶å¤±è´¥: {e}")

    async def _execute_performance_optimization(self):
        """æ‰§è¡Œæ€§èƒ½ä¼˜åŒ– - AIé©±åŠ¨æ¨¡å¼"""
        logger.info("    æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–...")
        
        try:
            # 1. æ£€æµ‹æ€§èƒ½ç“¶é¢ˆ
            performance_issues = await self._detect_performance_issues()
            
            # 2. AIåˆ†æå’Œä¼˜åŒ–ç­–ç•¥
            optimization_plan = await self._ai_analyze_optimization_plan(performance_issues)
            
            # 3. AIæ‰§è¡Œæ€§èƒ½ä¼˜åŒ–
            await self._ai_execute_performance_optimization(optimization_plan)
            
            logger.info("      âœ… æ€§èƒ½ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"      âŒ æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")

    async def _detect_performance_issues(self) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ€§èƒ½é—®é¢˜"""
        logger.info("      æ£€æµ‹æ€§èƒ½é—®é¢˜...")
        
        performance_issues = []
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    file_issues = {
                        "file_path": str(file_path),
                        "issues": []
                    }
                    
                    # æ£€æµ‹åµŒå¥—å¾ªç¯
                    nested_loops = self._detect_nested_loops(content)
                    for loop in nested_loops:
                        file_issues["issues"].append({
                            "type": "nested_loop",
                            "description": "åµŒå¥—å¾ªç¯å¯èƒ½å¯¼è‡´O(nÂ²)å¤æ‚åº¦",
                            "location": loop,
                            "severity": "high",
                            "impact": "algorithmic_complexity"
                        })
                    
                    # æ£€æµ‹ä½æ•ˆæŸ¥æ‰¾
                    inefficient_lookups = self._detect_inefficient_lookups(content)
                    for lookup in inefficient_lookups:
                        file_issues["issues"].append({
                            "type": "inefficient_lookup",
                            "description": "åˆ—è¡¨æŸ¥æ‰¾æ•ˆç‡è¾ƒä½",
                            "location": lookup,
                            "severity": "medium",
                            "impact": "lookup_performance"
                        })
                    
                    # æ£€æµ‹é‡å¤è®¡ç®—
                    repeated_calculations = self._detect_repeated_calculations(content)
                    for calc in repeated_calculations:
                        file_issues["issues"].append({
                            "type": "repeated_calculation",
                            "description": "é‡å¤è®¡ç®—å¯ä»¥ç¼“å­˜",
                            "location": calc,
                            "severity": "medium",
                            "impact": "cpu_usage"
                        })
                    
                    # æ£€æµ‹I/Oæ“ä½œ
                    io_operations = self._detect_io_operations(content)
                    for io_op in io_operations:
                        file_issues["issues"].append({
                            "type": "io_operation",
                            "description": "I/Oæ“ä½œå¯ä»¥ä¼˜åŒ–",
                            "location": io_op,
                            "severity": "medium",
                            "impact": "io_performance"
                        })
                    
                    # æ£€æµ‹å†…å­˜ä½¿ç”¨
                    memory_issues = self._detect_memory_issues(content)
                    for mem_issue in memory_issues:
                        file_issues["issues"].append({
                            "type": "memory_issue",
                            "description": "å†…å­˜ä½¿ç”¨å¯ä»¥ä¼˜åŒ–",
                            "location": mem_issue,
                            "severity": "low",
                            "impact": "memory_usage"
                        })
                    
                    if file_issues["issues"]:
                        performance_issues.append(file_issues)
                
                except Exception as e:
                    logger.warning(f"æ£€æµ‹æ€§èƒ½é—®é¢˜å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"        æ£€æµ‹åˆ° {len(performance_issues)} ä¸ªæ–‡ä»¶çš„æ€§èƒ½é—®é¢˜")
        return performance_issues

    def _detect_nested_loops(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹åµŒå¥—å¾ªç¯"""
        nested_loops = []
        lines = content.split('\n')
        
        for i, line in enumerate(lines):
            if 'for ' in line and i < len(lines) - 1:
                # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æœ‰å¦ä¸€ä¸ªforå¾ªç¯
                next_line = lines[i + 1]
                if 'for ' in next_line and next_line.startswith(lines[i][0] * len(lines[i]) - len(lines[i].lstrip())):
                    nested_loops.append({
                        "line": i + 1,
                        "content": line.strip(),
                        "next_line": next_line.strip()
                    })
        
        return nested_loops

    def _detect_inefficient_lookups(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹ä½æ•ˆæŸ¥æ‰¾"""
        inefficient_lookups = []
        lines = content.split('\n')
        
        # æŸ¥æ‰¾åˆ—è¡¨ä¸­çš„inæ“ä½œ
        for i, line in enumerate(lines):
            if ' in ' in line and not any(keyword in line for keyword in ['set(', 'dict(', 'tuple(']):
                # ç®€å•æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯åˆ—è¡¨æŸ¥æ‰¾
                match = re.search(r'(\w+)\s+in\s+(\w+)', line)
                if match:
                    var_name, collection_name = match.groups()
                    # å¦‚æœé›†åˆåä»¥sç»“å°¾ï¼Œå¯èƒ½æ˜¯åˆ—è¡¨
                    if collection_name.endswith('s') or 'list' in collection_name.lower():
                        inefficient_lookups.append({
                            "line": i + 1,
                            "content": line.strip(),
                            "lookup_var": var_name,
                            "collection": collection_name
                        })
        
        return inefficient_lookups

    def _detect_repeated_calculations(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹é‡å¤è®¡ç®—"""
        repeated_calculations = []
        lines = content.split('\n')
        
        # æŸ¥æ‰¾åœ¨å¾ªç¯ä¸­é‡å¤çš„è®¡ç®—
        function_calls = defaultdict(list)
        
        for i, line in enumerate(lines):
            # æŸ¥æ‰¾å‡½æ•°è°ƒç”¨
            matches = re.findall(r'(\w+)\([^)]*\)', line)
            for func_call in matches:
                function_calls[func_call].append(i + 1)
        
        # å¦‚æœåŒä¸€ä¸ªå‡½æ•°è°ƒç”¨åœ¨é™„è¿‘å¤šæ¬¡å‡ºç°
        for func_name, line_numbers in function_calls.items():
            if len(line_numbers) > 1:
                # æ£€æŸ¥æ˜¯å¦åœ¨å¾ªç¯ä¸­
                for line_num in line_numbers:
                    context_lines = lines[max(0, line_num-5):line_num+5]
                    if any('for ' in ctx_line or 'while ' in ctx_line for ctx_line in context_lines):
                        repeated_calculations.append({
                            "line": line_num,
                            "content": lines[line_num-1].strip(),
                            "function": func_name,
                            "occurrences": line_numbers
                        })
                        break
        
        return repeated_calculations

    def _detect_io_operations(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹I/Oæ“ä½œ"""
        io_operations = []
        lines = content.split('\n')
        
        io_patterns = [
            r'open\(',
            r'\.read\(',
            r'\.write\(',
            r'requests\.',
            r'urllib\.',
            r'subprocess\.'
        ]
        
        for i, line in enumerate(lines):
            for pattern in io_patterns:
                if re.search(pattern, line):
                    io_operations.append({
                        "line": i + 1,
                        "content": line.strip(),
                        "operation_type": pattern.strip('\\')
                    })
                    break
        
        return io_operations

    def _detect_memory_issues(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹å†…å­˜é—®é¢˜"""
        memory_issues = []
        lines = content.split('\n')
        
        # æŸ¥æ‰¾å¯èƒ½å¯¼è‡´å†…å­˜é—®é¢˜çš„æ¨¡å¼
        memory_patterns = [
            r'\[\w+\s+for\s+\w+\s+in\s+\w+\s+if\s+\w+\]',  # åˆ—è¡¨æ¨å¯¼å¼
            r'\.append\(',  # åˆ—è¡¨è¿½åŠ 
            r'list\(',  # åˆ›å»ºåˆ—è¡¨
            r'dict\(',   # åˆ›å»ºå­—å…¸
        ]
        
        for i, line in enumerate(lines):
            for pattern in memory_patterns:
                if re.search(pattern, line):
                    memory_issues.append({
                        "line": i + 1,
                        "content": line.strip(),
                        "pattern": pattern.strip('\\')
                    })
                    break
        
        return memory_issues

    async def _ai_analyze_optimization_plan(self, performance_issues: List[Dict[str, Any]]) -> Dict[str, Any]:
        """AIåˆ†æå’Œä¼˜åŒ–ç­–ç•¥"""
        logger.info("      AIåˆ†æä¼˜åŒ–ç­–ç•¥...")
        
        optimization_plan = {
            "critical_optimizations": [],
            "significant_optimizations": [],
            "minor_optimizations": [],
            "estimated_improvement": {},
            "risk_assessment": {}
        }
        
        # æŒ‰å½±å“å’Œä¸¥é‡ç¨‹åº¦åˆ†ç±»
        for file_issue in performance_issues:
            for issue in file_issue["issues"]:
                optimization = {
                    "file_path": file_issue["file_path"],
                    "issue_type": issue["type"],
                    "description": issue["description"],
                    "severity": issue["severity"],
                    "impact": issue["impact"],
                    "ai_strategy": await self._generate_ai_optimization_strategy(issue),
                    "estimated_improvement": self._estimate_performance_improvement(issue),
                    "confidence": 0.7
                }
                
                # æ ¹æ®ä¸¥é‡ç¨‹åº¦å’Œå½±å“åˆ†ç±»
                if issue["severity"] == "high" or issue["impact"] == "algorithmic_complexity":
                    optimization_plan["critical_optimizations"].append(optimization)
                elif issue["severity"] == "medium":
                    optimization_plan["significant_optimizations"].append(optimization)
                else:
                    optimization_plan["minor_optimizations"].append(optimization)
        
        # ä¼°ç®—æ€»ä½“æ”¹è¿›
        total_improvement = self._calculate_total_improvement(optimization_plan)
        optimization_plan["estimated_improvement"] = total_improvement
        
        logger.info(f"        AIè§„åˆ’å®Œæˆ: {len(optimization_plan['critical_optimizations'])} ä¸ªå…³é”®ä¼˜åŒ–")
        return optimization_plan

    async def _generate_ai_optimization_strategy(self, issue: Dict[str, Any]) -> str:
        """ç”ŸæˆAIä¼˜åŒ–ç­–ç•¥"""
        strategies = {
            "nested_loop": "å»ºè®®ä½¿ç”¨å­—å…¸æŸ¥æ‰¾ã€é›†åˆæ“ä½œæˆ–ç®—æ³•ä¼˜åŒ–æ¥å‡å°‘å¤æ‚åº¦",
            "inefficient_lookup": "å»ºè®®å°†åˆ—è¡¨è½¬æ¢ä¸ºé›†åˆæˆ–ä½¿ç”¨å­—å…¸æ¥æé«˜æŸ¥æ‰¾æ•ˆç‡",
            "repeated_calculation": "å»ºè®®å®ç°ç¼“å­˜æœºåˆ¶æˆ–é¢„è®¡ç®—æ¥é¿å…é‡å¤è®¡ç®—",
            "io_operation": "å»ºè®®ä½¿ç”¨å¼‚æ­¥I/Oã€æ‰¹å¤„ç†æˆ–ç¼“å­˜æ¥ä¼˜åŒ–I/Oæ€§èƒ½",
            "memory_issue": "å»ºè®®ä½¿ç”¨ç”Ÿæˆå™¨ã€åˆ†å—å¤„ç†æˆ–æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„"
        }
        
        return strategies.get(issue["type"], "å»ºè®®è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ä»¥æé«˜æ‰§è¡Œæ•ˆç‡")

    def _estimate_performance_improvement(self, issue: Dict[str, Any]) -> Dict[str, float]:
        """ä¼°ç®—æ€§èƒ½æ”¹è¿›"""
        improvements = {
            "nested_loop": {"speedup": 5.0, "memory_reduction": 0.0},
            "inefficient_lookup": {"speedup": 2.0, "memory_reduction": 0.0},
            "repeated_calculation": {"speedup": 1.5, "memory_reduction": 0.0},
            "io_operation": {"speedup": 2.0, "memory_reduction": 0.0},
            "memory_issue": {"speedup": 1.2, "memory_reduction": 0.3}
        }
        
        return improvements.get(issue["type"], {"speedup": 1.1, "memory_reduction": 0.1})

    def _calculate_total_improvement(self, optimization_plan: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ€»ä½“æ”¹è¿›"""
        total_speedup = 1.0
        total_memory_reduction = 0.0
        
        all_optimizations = (optimization_plan["critical_optimizations"] + 
                           optimization_plan["significant_optimizations"] + 
                           optimization_plan["minor_optimizations"])
        
        for opt in all_optimizations:
            improvement = opt["estimated_improvement"]
            total_speedup *= improvement["speedup"]
            total_memory_reduction += improvement["memory_reduction"]
        
        return {
            "estimated_speedup": min(total_speedup, 10.0),  # é™åˆ¶æœ€å¤§æ”¹è¿›
            "memory_reduction_percent": min(total_memory_reduction * 100, 50.0)
        }

    async def _ai_execute_performance_optimization(self, optimization_plan: Dict[str, Any]):
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®æŠ¥å‘Šï¼ˆä¸æ‰§è¡Œå®é™…ä¼˜åŒ–ï¼‰"""
        logger.info("      ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®æŠ¥å‘Š...")
        
        # æŒ‰ä¼˜å…ˆçº§ç”Ÿæˆå»ºè®®
        all_optimizations = (optimization_plan["critical_optimizations"] + 
                            optimization_plan["significant_optimizations"])
        
        for optimization in all_optimizations:
            logger.info(f"        ğŸ“‹ æ€§èƒ½ä¼˜åŒ–å»ºè®®: {optimization['description']}")
            logger.info(f"           æ–‡ä»¶: {optimization['file_path']}")
            logger.info(f"           AIç­–ç•¥: {optimization['ai_strategy']}")
            logger.info(f"           é¢„æœŸæ”¹è¿›: åŠ é€Ÿ{optimization['estimated_improvement']['speedup']:.1f}x, å†…å­˜å‡å°‘{optimization['estimated_improvement']['memory_reduction']*100:.1f}%")
            
            # è®°å½•åˆ°changelogä½œä¸ºå»ºè®®
            self.changelog.append({
                "timestamp": datetime.now().isoformat(),
                "action": "performance_suggestion",
                "issue": optimization["description"],
                "file": optimization["file_path"],
                "status": "suggested",
                "ai_strategy": optimization["ai_strategy"],
                "estimated_improvement": optimization["estimated_improvement"]
            })
        
        logger.info(f"        ç”Ÿæˆäº† {len(all_optimizations)} ä¸ªæ€§èƒ½ä¼˜åŒ–å»ºè®®")

    async def _ai_apply_optimization(self, optimization: Dict[str, Any]) -> bool:
        """AIåº”ç”¨æ€§èƒ½ä¼˜åŒ–"""
        try:
            file_path = Path(optimization["file_path"])
            
            if not file_path.exists():
                return False
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ ¹æ®é—®é¢˜ç±»å‹åº”ç”¨ä¸åŒçš„AIä¼˜åŒ–ç­–ç•¥
            if optimization["issue_type"] == "nested_loop":
                new_content = await self._ai_optimize_nested_loop(content, optimization)
            elif optimization["issue_type"] == "inefficient_lookup":
                new_content = await self._ai_optimize_lookup(content, optimization)
            elif optimization["issue_type"] == "repeated_calculation":
                new_content = await self._ai_optimize_calculation(content, optimization)
            elif optimization["issue_type"] == "io_operation":
                new_content = await self._ai_optimize_io(content, optimization)
            elif optimization["issue_type"] == "memory_issue":
                new_content = await self._ai_optimize_memory(content, optimization)
            else:
                return False
            
            # å¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼Œå†™å…¥æ–‡ä»¶
            if new_content != content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                return True
            
            return False
        
        except Exception as e:
            logger.error(f"AIåº”ç”¨æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
            return False

    async def _ai_optimize_nested_loop(self, content: str, optimization: Dict[str, Any]) -> str:
        """AIä¼˜åŒ–åµŒå¥—å¾ªç¯"""
        logger.info(f"          AIåˆ†æåµŒå¥—å¾ªç¯ä¼˜åŒ–: {optimization['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›ä¼˜åŒ–åçš„å†…å®¹

    async def _ai_optimize_lookup(self, content: str, optimization: Dict[str, Any]) -> str:
        """AIä¼˜åŒ–æŸ¥æ‰¾æ“ä½œ"""
        logger.info(f"          AIåˆ†ææŸ¥æ‰¾ä¼˜åŒ–: {optimization['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›ä¼˜åŒ–åçš„å†…å®¹

    async def _ai_optimize_calculation(self, content: str, optimization: Dict[str, Any]) -> str:
        """AIä¼˜åŒ–è®¡ç®—"""
        logger.info(f"          AIåˆ†æè®¡ç®—ä¼˜åŒ–: {optimization['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›ä¼˜åŒ–åçš„å†…å®¹

    async def _ai_optimize_io(self, content: str, optimization: Dict[str, Any]) -> str:
        """AIä¼˜åŒ–I/Oæ“ä½œ"""
        logger.info(f"          AIåˆ†æI/Oä¼˜åŒ–: {optimization['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›ä¼˜åŒ–åçš„å†…å®¹

    async def _ai_optimize_memory(self, content: str, optimization: Dict[str, Any]) -> str:
        """AIä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        logger.info(f"          AIåˆ†æå†…å­˜ä¼˜åŒ–: {optimization['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›ä¼˜åŒ–åçš„å†…å®¹

    async def _execute_security_hardening(self):
        """æ‰§è¡Œå®‰å…¨åŠ å›º - AIé©±åŠ¨æ¨¡å¼"""
        logger.info("    æ‰§è¡Œå®‰å…¨åŠ å›º...")
        
        try:
            # 1. æ£€æµ‹å®‰å…¨æ¼æ´
            security_vulnerabilities = await self._detect_security_vulnerabilities()
            
            # 2. AIåˆ†æå’Œå®‰å…¨ç­–ç•¥
            security_plan = await self._ai_analyze_security_plan(security_vulnerabilities)
            
            # 3. AIæ‰§è¡Œå®‰å…¨åŠ å›º
            await self._ai_execute_security_hardening(security_plan)
            
            logger.info("      âœ… å®‰å…¨åŠ å›ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"      âŒ å®‰å…¨åŠ å›ºå¤±è´¥: {e}")

    async def _detect_security_vulnerabilities(self) -> List[Dict[str, Any]]:
        """æ£€æµ‹å®‰å…¨æ¼æ´"""
        logger.info("      æ£€æµ‹å®‰å…¨æ¼æ´...")
        
        security_vulnerabilities = []
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    file_vulnerabilities = {
                        "file_path": str(file_path),
                        "vulnerabilities": []
                    }
                    
                    # æ£€æµ‹ä»£ç æ³¨å…¥
                    injection_vulns = self._detect_code_injection(content)
                    for vuln in injection_vulns:
                        file_vulnerabilities["vulnerabilities"].append({
                            "type": "code_injection",
                            "description": "æ½œåœ¨çš„ä»£ç æ³¨å…¥é£é™©",
                            "location": vuln,
                            "severity": "critical",
                            "cwe": "CWE-94"
                        })
                    
                    # æ£€æµ‹å‘½ä»¤æ³¨å…¥
                    command_injection = self._detect_command_injection(content)
                    for vuln in command_injection:
                        file_vulnerabilities["vulnerabilities"].append({
                            "type": "command_injection",
                            "description": "æ½œåœ¨çš„å‘½ä»¤æ³¨å…¥é£é™©",
                            "location": vuln,
                            "severity": "critical",
                            "cwe": "CWE-78"
                        })
                    
                    # æ£€æµ‹SQLæ³¨å…¥
                    sql_injection = self._detect_sql_injection(content)
                    for vuln in sql_injection:
                        file_vulnerabilities["vulnerabilities"].append({
                            "type": "sql_injection",
                            "description": "æ½œåœ¨çš„SQLæ³¨å…¥é£é™©",
                            "location": vuln,
                            "severity": "high",
                            "cwe": "CWE-89"
                        })
                    
                    # æ£€æµ‹XSS
                    xss_vulns = self._detect_xss(content)
                    for vuln in xss_vulns:
                        file_vulnerabilities["vulnerabilities"].append({
                            "type": "xss",
                            "description": "æ½œåœ¨çš„è·¨ç«™è„šæœ¬æ”»å‡»é£é™©",
                            "location": vuln,
                            "severity": "high",
                            "cwe": "CWE-79"
                        })
                    
                    # æ£€æµ‹æ•æ„Ÿä¿¡æ¯æ³„éœ²
                    sensitive_data = self._detect_sensitive_data(content)
                    for vuln in sensitive_data:
                        file_vulnerabilities["vulnerabilities"].append({
                            "type": "sensitive_data",
                            "description": "æ•æ„Ÿä¿¡æ¯æ³„éœ²é£é™©",
                            "location": vuln,
                            "severity": "medium",
                            "cwe": "CWE-200"
                        })
                    
                    # æ£€æµ‹å¼±åŠ å¯†
                    weak_crypto = self._detect_weak_crypto(content)
                    for vuln in weak_crypto:
                        file_vulnerabilities["vulnerabilities"].append({
                            "type": "weak_crypto",
                            "description": "ä½¿ç”¨å¼±åŠ å¯†ç®—æ³•",
                            "location": vuln,
                            "severity": "medium",
                            "cwe": "CWE-327"
                        })
                    
                    # æ£€æµ‹ç¡¬ç¼–ç å‡­è¯
                    hardcoded_creds = self._detect_hardcoded_credentials(content)
                    for vuln in hardcoded_creds:
                        file_vulnerabilities["vulnerabilities"].append({
                            "type": "hardcoded_credentials",
                            "description": "ç¡¬ç¼–ç å‡­è¯ä¿¡æ¯",
                            "location": vuln,
                            "severity": "high",
                            "cwe": "CWE-798"
                        })
                    
                    if file_vulnerabilities["vulnerabilities"]:
                        security_vulnerabilities.append(file_vulnerabilities)
                
                except Exception as e:
                    logger.warning(f"æ£€æµ‹å®‰å…¨æ¼æ´å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"        æ£€æµ‹åˆ° {len(security_vulnerabilities)} ä¸ªæ–‡ä»¶çš„å®‰å…¨æ¼æ´")
        return security_vulnerabilities

    def _detect_code_injection(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹ä»£ç æ³¨å…¥"""
        code_injection = []
        lines = content.split('\n')
        
        dangerous_functions = ['eval(', 'exec(', 'compile(']
        
        for i, line in enumerate(lines):
            for func in dangerous_functions:
                if func in line:
                    code_injection.append({
                        "line": i + 1,
                        "content": line.strip(),
                        "function": func.strip('(')
                    })
        
        return code_injection

    def _detect_command_injection(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹å‘½ä»¤æ³¨å…¥"""
        command_injection = []
        lines = content.split('\n')
        
        dangerous_patterns = [
            r'subprocess\.',
            r'os\.system',
            r'os\.popen',
            r'commands\.',
            r'popen2\.',
            r'popen4\.',
            r'spawn\.',
            r'call\('
        ]
        
        for i, line in enumerate(lines):
            for pattern in dangerous_patterns:
                if re.search(pattern, line) and 'shell=True' in line:
                    command_injection.append({
                        "line": i + 1,
                        "content": line.strip(),
                        "pattern": pattern
                    })
        
        return command_injection

    def _detect_sql_injection(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹SQLæ³¨å…¥"""
        sql_injection = []
        lines = content.split('\n')
        
        # æŸ¥æ‰¾SQLæ‹¼æ¥æ¨¡å¼
        sql_patterns = [
            r'SELECT.*\+.*',
            r'INSERT.*\+.*',
            r'UPDATE.*\+.*',
            r'DELETE.*\+.*',
            r'WHERE.*\+.*',
            r'".*%s.*".*%.*',
            r"'.*%s.*'.*%.*"
        ]
        
        for i, line in enumerate(lines):
            for pattern in sql_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    sql_injection.append({
                        "line": i + 1,
                        "content": line.strip(),
                        "pattern": pattern
                    })
        
        return sql_injection

    def _detect_xss(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹XSS"""
        xss_vulns = []
        lines = content.split('\n')
        
        xss_patterns = [
            r'innerHTML.*=',
            r'outerHTML.*=',
            r'document\.write',
            r'eval\(',
            r'setTimeout.*eval',
            r'setInterval.*eval'
        ]
        
        for i, line in enumerate(lines):
            for pattern in xss_patterns:
                if re.search(pattern, line):
                    xss_vulns.append({
                        "line": i + 1,
                        "content": line.strip(),
                        "pattern": pattern
                    })
        
        return xss_vulns

    def _detect_sensitive_data(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ•æ„Ÿä¿¡æ¯æ³„éœ²"""
        sensitive_data = []
        lines = content.split('\n')
        
        sensitive_patterns = [
            r'password\s*=\s*["\'][^"\']+["\']',
            r'secret\s*=\s*["\'][^"\']+["\']',
            r'api_key\s*=\s*["\'][^"\']+["\']',
            r'token\s*=\s*["\'][^"\']+["\']',
            r'private_key\s*=\s*["\'][^"\']+["\']',
            r'access_key\s*=\s*["\'][^"\']+["\']'
        ]
        
        for i, line in enumerate(lines):
            for pattern in sensitive_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    sensitive_data.append({
                        "line": i + 1,
                        "content": line.strip()[:50] + "...",  # æˆªæ–­æ˜¾ç¤º
                        "pattern": pattern
                    })
        
        return sensitive_data

    def _detect_weak_crypto(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼±åŠ å¯†"""
        weak_crypto = []
        lines = content.split('\n')
        
        weak_algorithms = [
            'md5(',
            'sha1(',
            'DES(',
            'RC4(',
            'MD5(',
            'SHA1('
        ]
        
        for i, line in enumerate(lines):
            for algo in weak_algorithms:
                if algo in line:
                    weak_crypto.append({
                        "line": i + 1,
                        "content": line.strip(),
                        "algorithm": algo.strip('(')
                    })
        
        return weak_crypto

    def _detect_hardcoded_credentials(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹ç¡¬ç¼–ç å‡­è¯"""
        hardcoded_creds = []
        lines = content.split('\n')
        
        credential_patterns = [
            r'["\'][A-Za-z0-9+/]{20,}["\']',  # Base64ç¼–ç çš„å¯†é’¥
            r'["\'][A-Fa-f0-9]{32,}["\']',   # åå…­è¿›åˆ¶å¯†é’¥
            r'sk_[a-zA-Z0-9]{24,}',            # Stripeå¯†é’¥
            r'ghp_[a-zA-Z0-9]{36}',            # GitHubä¸ªäººè®¿é—®ä»¤ç‰Œ
            r'AIza[0-9A-Za-z_-]{35}'           # Google APIå¯†é’¥
        ]
        
        for i, line in enumerate(lines):
            for pattern in credential_patterns:
                if re.search(pattern, line):
                    hardcoded_creds.append({
                        "line": i + 1,
                        "content": "HARDCODED_CREDENTIAL",  # ä¸æ˜¾ç¤ºå®é™…å†…å®¹
                        "pattern": pattern
                    })
        
        return hardcoded_creds

    async def _ai_analyze_security_plan(self, vulnerabilities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """AIåˆ†æå’Œå®‰å…¨ç­–ç•¥"""
        logger.info("      AIåˆ†æå®‰å…¨ç­–ç•¥...")
        
        security_plan = {
            "critical_fixes": [],
            "high_priority_fixes": [],
            "medium_priority_fixes": [],
            "security_score": 0.0,
            "risk_assessment": {}
        }
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
        for file_vuln in vulnerabilities:
            for vuln in file_vuln["vulnerabilities"]:
                fix = {
                    "file_path": file_vuln["file_path"],
                    "vulnerability_type": vuln["type"],
                    "description": vuln["description"],
                    "severity": vuln["severity"],
                    "cwe": vuln["cwe"],
                    "location": vuln["location"],
                    "ai_fix_strategy": await self._generate_ai_security_fix(vuln),
                    "confidence": 0.8
                }
                
                # æ ¹æ®ä¸¥é‡ç¨‹åº¦åˆ†ç±»
                if vuln["severity"] == "critical":
                    security_plan["critical_fixes"].append(fix)
                elif vuln["severity"] == "high":
                    security_plan["high_priority_fixes"].append(fix)
                else:
                    security_plan["medium_priority_fixes"].append(fix)
        
        # è®¡ç®—å®‰å…¨è¯„åˆ†
        total_vulns = (len(security_plan["critical_fixes"]) + 
                       len(security_plan["high_priority_fixes"]) + 
                       len(security_plan["medium_priority_fixes"]))
        
        if total_vulns > 0:
            critical_weight = len(security_plan["critical_fixes"]) * 10
            high_weight = len(security_plan["high_priority_fixes"]) * 5
            medium_weight = len(security_plan["medium_priority_fixes"]) * 2
            
            security_plan["security_score"] = max(0, 100 - (critical_weight + high_weight + medium_weight))
        
        logger.info(f"        AIå®‰å…¨è§„åˆ’å®Œæˆ: {len(security_plan['critical_fixes'])} ä¸ªå…³é”®ä¿®å¤")
        return security_plan

    async def _generate_ai_security_fix(self, vulnerability: Dict[str, Any]) -> str:
        """ç”ŸæˆAIå®‰å…¨ä¿®å¤ç­–ç•¥"""
        fix_strategies = {
            "code_injection": "å»ºè®®ç§»é™¤eval/execè°ƒç”¨ï¼Œä½¿ç”¨å®‰å…¨çš„æ›¿ä»£æ–¹æ¡ˆå¦‚ast.literal_eval",
            "command_injection": "å»ºè®®é¿å…shell=Trueï¼Œä½¿ç”¨å‚æ•°åŒ–å‘½ä»¤æˆ–subprocess.run without shell",
            "sql_injection": "å»ºè®®ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢æˆ–ORMæ¥é˜²æ­¢SQLæ³¨å…¥",
            "xss": "å»ºè®®å¯¹ç”¨æˆ·è¾“å…¥è¿›è¡ŒHTMLè½¬ä¹‰ï¼Œä½¿ç”¨å®‰å…¨çš„æ¨¡æ¿å¼•æ“",
            "sensitive_data": "å»ºè®®å°†æ•æ„Ÿä¿¡æ¯ç§»è‡³ç¯å¢ƒå˜é‡æˆ–å®‰å…¨çš„é…ç½®ç®¡ç†ç³»ç»Ÿ",
            "weak_crypto": "å»ºè®®ä½¿ç”¨å¼ºåŠ å¯†ç®—æ³•å¦‚SHA-256ã€AES-256",
            "hardcoded_credentials": "å»ºè®®ç§»é™¤ç¡¬ç¼–ç å‡­è¯ï¼Œä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡"
        }
        
        return fix_strategies.get(vulnerability["type"], "å»ºè®®éµå¾ªå®‰å…¨æœ€ä½³å®è·µè¿›è¡Œä¿®å¤")

    async def _ai_execute_security_hardening(self, security_plan: Dict[str, Any]):
        """AIæ‰§è¡Œå®‰å…¨åŠ å›º"""
        logger.info("      AIæ‰§è¡Œå®‰å…¨åŠ å›º...")
        
        # æŒ‰ä¼˜å…ˆçº§æ‰§è¡Œå®‰å…¨ä¿®å¤
        all_fixes = (security_plan["critical_fixes"] + 
                    security_plan["high_priority_fixes"] + 
                    security_plan["medium_priority_fixes"])
        
        for fix in all_fixes:
            try:
                if not self.analysis_mode:
                    # AIæ‰§è¡Œå…·ä½“çš„å®‰å…¨ä¿®å¤
                    success = await self._ai_apply_security_fix(fix)
                    
                    if success:
                        logger.info(f"        âœ… AIå®‰å…¨ä¿®å¤æˆåŠŸ: {fix['description']}")
                        self.changelog.append({
                            "timestamp": datetime.now().isoformat(),
                            "action": "ai_security_fix",
                            "issue": fix["description"],
                            "file": fix["file_path"],
                            "status": "success",
                            "cwe": fix["cwe"],
                            "ai_fix_strategy": fix["ai_fix_strategy"]
                        })
                    else:
                        logger.warning(f"        âš ï¸ AIå®‰å…¨ä¿®å¤å¤±è´¥: {fix['description']}")
                else:
                    logger.info(f"        ğŸ“‹ AIå®‰å…¨ä¿®å¤å»ºè®®: {fix['description']}")
            
            except Exception as e:
                logger.error(f"        âŒ AIå®‰å…¨ä¿®å¤å¼‚å¸¸: {fix['description']} - {e}")

    async def _ai_apply_security_fix(self, fix: Dict[str, Any]) -> bool:
        """AIåº”ç”¨å®‰å…¨ä¿®å¤"""
        try:
            file_path = Path(fix["file_path"])
            
            if not file_path.exists():
                return False
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ ¹æ®æ¼æ´ç±»å‹åº”ç”¨ä¸åŒçš„AIä¿®å¤ç­–ç•¥
            if fix["vulnerability_type"] == "code_injection":
                new_content = await self._ai_fix_code_injection(content, fix)
            elif fix["vulnerability_type"] == "command_injection":
                new_content = await self._ai_fix_command_injection(content, fix)
            elif fix["vulnerability_type"] == "sql_injection":
                new_content = await self._ai_fix_sql_injection(content, fix)
            elif fix["vulnerability_type"] == "sensitive_data":
                new_content = await self._ai_fix_sensitive_data(content, fix)
            elif fix["vulnerability_type"] == "weak_crypto":
                new_content = await self._ai_fix_weak_crypto(content, fix)
            else:
                return False
            
            # å¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼Œå†™å…¥æ–‡ä»¶
            if new_content != content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                return True
            
            return False
        
        except Exception as e:
            logger.error(f"AIåº”ç”¨å®‰å…¨ä¿®å¤å¤±è´¥: {e}")
            return False

    async def _ai_fix_code_injection(self, content: str, fix: Dict[str, Any]) -> str:
        """AIä¿®å¤ä»£ç æ³¨å…¥"""
        logger.info(f"          AIåˆ†æä»£ç æ³¨å…¥ä¿®å¤: {fix['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›ä¿®å¤åçš„å†…å®¹

    async def _ai_fix_command_injection(self, content: str, fix: Dict[str, Any]) -> str:
        """AIä¿®å¤å‘½ä»¤æ³¨å…¥"""
        logger.info(f"          AIåˆ†æå‘½ä»¤æ³¨å…¥ä¿®å¤: {fix['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›ä¿®å¤åçš„å†…å®¹

    async def _ai_fix_sql_injection(self, content: str, fix: Dict[str, Any]) -> str:
        """AIä¿®å¤SQLæ³¨å…¥"""
        logger.info(f"          AIåˆ†æSQLæ³¨å…¥ä¿®å¤: {fix['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›ä¿®å¤åçš„å†…å®¹

    async def _ai_fix_sensitive_data(self, content: str, fix: Dict[str, Any]) -> str:
        """AIä¿®å¤æ•æ„Ÿä¿¡æ¯æ³„éœ²"""
        logger.info(f"          AIåˆ†ææ•æ„Ÿä¿¡æ¯ä¿®å¤: {fix['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›ä¿®å¤åçš„å†…å®¹

    async def _ai_fix_weak_crypto(self, content: str, fix: Dict[str, Any]) -> str:
        """AIä¿®å¤å¼±åŠ å¯†"""
        logger.info(f"          AIåˆ†æå¼±åŠ å¯†ä¿®å¤: {fix['description']}")
        return content  # å®é™…åº”è¯¥è¿”å›ä¿®å¤åçš„å†…å®¹

    async def _execute_architecture_improvements(self):
        """æ‰§è¡Œæ¶æ„æ”¹è¿›"""
        logger.info("    æ‰§è¡Œæ¶æ„æ”¹è¿›...")
        
        # è¿™é‡Œå¯ä»¥å®ç°æ¶æ„æ”¹è¿›é€»è¾‘
        # ç›®å‰åªæ˜¯å ä½ç¬¦
        pass

    async def _phase_validation(self):
        """é˜¶æ®µ4: éªŒè¯æµ‹è¯•"""
        logger.info("ğŸ§ª é˜¶æ®µ4: éªŒè¯æµ‹è¯•...")
        self.current_phase = UpgradePhase.VALIDATION
        
        try:
            # 4.1 è¯­æ³•æ£€æŸ¥
            await self._validate_syntax()
            
            # 4.2 å•å…ƒæµ‹è¯•
            await self._run_unit_tests()
            
            # 4.3 é›†æˆæµ‹è¯•
            await self._run_integration_tests()
            
            # 4.4 æ€§èƒ½æµ‹è¯•
            await self._run_performance_tests()
            
            # 4.5 å®‰å…¨æµ‹è¯•
            await self._run_security_tests()
            
            logger.info("  âœ… éªŒè¯æµ‹è¯•å®Œæˆ")
            
        except Exception as e:
            logger.error(f"  âŒ éªŒè¯æµ‹è¯•å¤±è´¥: {e}")
            raise

    async def _validate_syntax(self):
        """è¯­æ³•æ£€æŸ¥"""
        logger.info("    æ‰§è¡Œè¯­æ³•æ£€æŸ¥...")
        
        syntax_errors = []
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ç¼–è¯‘æ£€æŸ¥è¯­æ³•
                    ast.parse(content)
                
                except SyntaxError as e:
                    syntax_errors.append({
                        "file": str(file_path),
                        "line": e.lineno,
                        "error": str(e)
                    })
                except Exception as e:
                    logger.warning(f"æ— æ³•æ£€æŸ¥è¯­æ³• {file_path}: {e}")
        
        if syntax_errors:
            logger.warning(f"å‘ç° {len(syntax_errors)} ä¸ªè¯­æ³•é”™è¯¯")
            for error in syntax_errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.warning(f"  {error['file']}:{error['line']} - {error['error']}")
        else:
            logger.info("    âœ… æ‰€æœ‰æ–‡ä»¶è¯­æ³•æ­£ç¡®")

    async def _run_unit_tests(self):
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        logger.info("    è¿è¡Œå•å…ƒæµ‹è¯•...")
        
        # å°è¯•è¿è¡Œpytest
        try:
            result = subprocess.run(
                ["python", "-m", "pytest", "-v", "--tb=short"],
                cwd=self.workspace_path,
                capture_output=True,
                text=True,
                timeout=300
            )
            
            if result.returncode == 0:
                logger.info("    âœ… å•å…ƒæµ‹è¯•é€šè¿‡")
            else:
                logger.warning(f"    âš ï¸ å•å…ƒæµ‹è¯•å¤±è´¥: {len(result.stdout.splitlines())} ä¸ªå¤±è´¥")
                if self.verbose:
                    logger.warning(result.stdout)
        
        except subprocess.TimeoutExpired:
            logger.warning("    â° å•å…ƒæµ‹è¯•è¶…æ—¶")
        except FileNotFoundError:
            logger.info("    â„¹ï¸ æœªæ‰¾åˆ°pytestï¼Œè·³è¿‡å•å…ƒæµ‹è¯•")
        except Exception as e:
            logger.warning(f"    âŒ å•å…ƒæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")

    async def _run_integration_tests(self):
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        logger.info("    è¿è¡Œé›†æˆæµ‹è¯•...")
        
        # è¿™é‡Œå¯ä»¥å®ç°é›†æˆæµ‹è¯•é€»è¾‘
        # ç›®å‰åªæ˜¯å ä½ç¬¦
        logger.info("    â„¹ï¸ é›†æˆæµ‹è¯•è·³è¿‡ï¼ˆæœªå®ç°ï¼‰")

    async def _run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        logger.info("    è¿è¡Œæ€§èƒ½æµ‹è¯•...")
        
        # è¿™é‡Œå¯ä»¥å®ç°æ€§èƒ½æµ‹è¯•é€»è¾‘
        # ç›®å‰åªæ˜¯å ä½ç¬¦
        logger.info("    â„¹ï¸ æ€§èƒ½æµ‹è¯•è·³è¿‡ï¼ˆæœªå®ç°ï¼‰")

    async def _run_security_tests(self):
        """è¿è¡Œå®‰å…¨æµ‹è¯•"""
        logger.info("    è¿è¡Œå®‰å…¨æµ‹è¯•...")
        
        # è¿™é‡Œå¯ä»¥å®ç°å®‰å…¨æµ‹è¯•é€»è¾‘
        # ç›®å‰åªæ˜¯å ä½ç¬¦
        logger.info("    â„¹ï¸ å®‰å…¨æµ‹è¯•è·³è¿‡ï¼ˆæœªå®ç°ï¼‰")

    async def _phase_documentation(self):
        """é˜¶æ®µ5: æ–‡æ¡£ç”Ÿæˆ"""
        logger.info("ğŸ“š é˜¶æ®µ5: æ–‡æ¡£ç”Ÿæˆ...")
        self.current_phase = UpgradePhase.DOCUMENTATION
        
        try:
            # 5.1 ç”Ÿæˆå‡çº§æŠ¥å‘Š
            await self._generate_upgrade_report()
            
            # 5.2 ç”ŸæˆAPIæ–‡æ¡£
            await self._generate_api_documentation()
            
            # 5.3 ç”Ÿæˆå˜æ›´æ—¥å¿—
            await self._generate_changelog()
            
            # 5.4 æ›´æ–°README
            await self._update_readme()
            
            logger.info("  âœ… æ–‡æ¡£ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            logger.error(f"  âŒ æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {e}")
            raise

    async def _generate_upgrade_report(self):
        """ç”Ÿæˆå‡çº§æŠ¥å‘Š"""
        logger.info("    ç”Ÿæˆå‡çº§æŠ¥å‘Š...")
        
        report_content = [
            "# é¡¹ç›®å‡çº§æŠ¥å‘Š",
            f"",
            f"**å‡çº§æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**ä¼šè¯ID**: {self.session_id}",
            f"**ç‰ˆæœ¬**: {self.version_info['current']} â†’ {self.version_info['target']}",
            f"",
            "## å‡çº§æ‘˜è¦",
            f"",
            f"- æ€»æ–‡ä»¶æ•°: {self.project_metrics.total_files}",
            f"- ä»£ç æ–‡ä»¶: {self.project_metrics.code_files}",
            f"- æµ‹è¯•æ–‡ä»¶: {self.project_metrics.test_files}",
            f"- æ€»ä»£ç è¡Œæ•°: {self.project_metrics.total_lines}",
            f"- æµ‹è¯•è¦†ç›–ç‡: {self.project_metrics.test_coverage:.1f}%",
            f"- å¯ç»´æŠ¤æ€§æŒ‡æ•°: {self.project_metrics.maintainability_index:.1f}",
            f"",
            "## å‘ç°çš„é—®é¢˜",
            f""
        ]
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡é—®é¢˜
        severity_counts = defaultdict(int)
        for issue in self.issues:
            severity_counts[issue.severity.value] += 1
        
        for severity in ["critical", "high", "medium", "low"]:
            count = severity_counts.get(severity, 0)
            if count > 0:
                report_content.append(f"- {severity.capitalize()}: {count}")
        
        report_content.extend([
            f"",
            "## ä¿®å¤çš„é—®é¢˜",
            f""
        ])
        
        # æ·»åŠ ä¿®å¤çš„é—®é¢˜
        fixed_issues = [c for c in self.changelog if c["status"] == "success"]
        report_content.append(f"- æˆåŠŸä¿®å¤: {len(fixed_issues)} ä¸ªé—®é¢˜")
        
        failed_issues = [c for c in self.changelog if c["status"] == "failed"]
        if failed_issues:
            report_content.append(f"- ä¿®å¤å¤±è´¥: {len(failed_issues)} ä¸ªé—®é¢˜")
        
        report_content.extend([
            f"",
            "## æ¶æ„åˆ†æ",
            f"",
            f"- æ£€æµ‹åˆ°çš„æ¶æ„æ¨¡å¼: {', '.join(self.architecture_analysis.get('detected_patterns', []))}",
            f"- å¹³å‡è€¦åˆåº¦: {self.architecture_analysis.get('average_coupling', 0):.2f}",
            f"",
            "## å»ºè®®å’Œåç»­æ­¥éª¤",
            f"",
            "1. ç»§ç»­ç›‘æ§ä»£ç è´¨é‡æŒ‡æ ‡",
            "2. å®šæœŸè¿è¡Œå®‰å…¨æ‰«æ",
            "3. å¢åŠ æµ‹è¯•è¦†ç›–ç‡",
            "4. ä¼˜åŒ–æ€§èƒ½ç“¶é¢ˆ",
            "5. å®šæœŸæ›´æ–°ä¾èµ–",
            f"",
            "---",
            f"*æŠ¥å‘Šç”± iFlow CLI è‡ªåŠ¨ç”Ÿæˆ*"
        ])
        
        # ä¿å­˜æŠ¥å‘Š
        report_dir = self.workspace_path / ".iflow" / "reports"
        report_dir.mkdir(exist_ok=True)
        
        report_file = report_dir / f"upgrade_report_{self.session_id}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        logger.info(f"      âœ… å‡çº§æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    async def _generate_api_documentation(self):
        """ç”ŸæˆAPIæ–‡æ¡£"""
        logger.info("    ç”ŸæˆAPIæ–‡æ¡£...")
        
        # è¿™é‡Œå¯ä»¥å®ç°APIæ–‡æ¡£ç”Ÿæˆé€»è¾‘
        # ç›®å‰åªæ˜¯å ä½ç¬¦
        pass

    async def _generate_changelog(self):
        """ç”Ÿæˆå˜æ›´æ—¥å¿—"""
        logger.info("    ç”Ÿæˆå˜æ›´æ—¥å¿—...")
        
        changelog_content = [
            "# å˜æ›´æ—¥å¿—",
            f"",
            f"## [{self.version_info['target']}] - {datetime.now().strftime('%Y-%m-%d')}",
            f""
        ]
        
        # æŒ‰ç±»å‹åˆ†ç»„å˜æ›´
        changes_by_type = defaultdict(list)
        for change in self.changelog:
            if change["status"] == "success":
                changes_by_type[change["action"]].append(change["issue"])
        
        for change_type, issues in changes_by_type.items():
            changelog_content.append(f"### {change_type.title()}")
            changelog_content.append("")
            
            for issue in issues:
                changelog_content.append(f"- {issue}")
            
            changelog_content.append("")
        
        # ä¿å­˜å˜æ›´æ—¥å¿—
        changelog_file = self.workspace_path / "CHANGELOG.md"
        
        # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–ç°æœ‰å†…å®¹
        existing_content = ""
        if changelog_file.exists():
            with open(changelog_file, 'r', encoding='utf-8') as f:
                existing_content = f.read()
        
        # å†™å…¥æ–°å†…å®¹
        with open(changelog_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(changelog_content))
            if existing_content:
                f.write('\n')
                f.write(existing_content)
        
        logger.info(f"      âœ… å˜æ›´æ—¥å¿—å·²æ›´æ–°: {changelog_file}")

    async def _update_readme(self):
        """æ›´æ–°README"""
        logger.info("    æ›´æ–°README...")
        
        readme_file = self.workspace_path / "README.md"
        
        if not readme_file.exists():
            # åˆ›å»ºåŸºæœ¬README
            readme_content = [
                f"# {self.workspace_path.name}",
                f"",
                f"## é¡¹ç›®ä¿¡æ¯",
                f"",
                f"- å‡çº§æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                f"- ç‰ˆæœ¬: {self.version_info['target']}",
                f"- æ–‡ä»¶æ•°: {self.project_metrics.total_files}",
                f"- æµ‹è¯•è¦†ç›–ç‡: {self.project_metrics.test_coverage:.1f}%",
                f"",
                f"## å¿«é€Ÿå¼€å§‹",
                f"",
                f"```bash",
                f"# å®‰è£…ä¾èµ–",
                f"# è¿è¡Œæµ‹è¯•",
                f"# å¯åŠ¨é¡¹ç›®",
                f"```",
                f"",
                f"## æ–‡æ¡£",
                f"",
                f"- [å‡çº§æŠ¥å‘Š](.iflow/reports/upgrade_report_{self.session_id}.md)",
                f"- [å˜æ›´æ—¥å¿—](CHANGELOG.md)",
                f"",
                f"---",
                f"*æ­¤READMEç”± iFlow CLI è‡ªåŠ¨ç”Ÿæˆ*"
            ]
            
            with open(readme_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(readme_content))
            
            logger.info(f"      âœ… READMEå·²åˆ›å»º: {readme_file}")

    async def _phase_cleanup(self):
        """é˜¶æ®µ6: æ¸…ç†ä¼˜åŒ–"""
        logger.info("ğŸ—‘ï¸ é˜¶æ®µ6: æ¸…ç†ä¼˜åŒ–...")
        self.current_phase = UpgradePhase.CLEANUP
        
        try:
            # 6.1 æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            await self._cleanup_temp_files()
            
            # 6.2 æ¸…ç†æ—§ä»£ç 
            await self._cleanup_old_code()
            
            # 6.3 ä¼˜åŒ–å¯¼å…¥
            await self._optimize_imports()
            
            # 6.4 æ¸…ç†ç¼“å­˜
            await self._cleanup_cache()
            
            logger.info("  âœ… æ¸…ç†ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"  âŒ æ¸…ç†ä¼˜åŒ–å¤±è´¥: {e}")
            raise

    async def _cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        logger.info("    æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        
        temp_patterns = [
            "**/__pycache__",
            "**/*.pyc",
            "**/*.pyo",
            "**/.pytest_cache",
            "**/.coverage",
            "**/.mypy_cache"
        ]
        
        cleaned_count = 0
        
        for pattern in temp_patterns:
            for path in self.workspace_path.rglob(pattern.split('/')[-1]):
                if path.is_dir():
                    try:
                        shutil.rmtree(path)
                        cleaned_count += 1
                    except:
                        pass
                elif path.is_file():
                    try:
                        path.unlink()
                        cleaned_count += 1
                    except:
                        pass
        
        logger.info(f"      æ¸…ç†äº† {cleaned_count} ä¸ªä¸´æ—¶æ–‡ä»¶/ç›®å½•")

    async def _cleanup_old_code(self):
        """æ¸…ç†æ—§ä»£ç """
        logger.info("    æ¸…ç†æ—§ä»£ç ...")
        
        # è¯†åˆ«æœªä½¿ç”¨çš„ä»£ç 
        unused_imports = []
        unused_functions = []
        
        for file_path in self.workspace_path.rglob("*.py"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ç®€å•çš„æœªä½¿ç”¨å¯¼å…¥æ£€æµ‹
                    imports = re.findall(r'^import\s+(\w+)|^from\s+(\w+)', content, re.MULTILINE)
                    for import_match in imports:
                        module = import_match[0] or import_match[1]
                        if module and module not in content:
                            unused_imports.append((str(file_path), module))
                
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ†ææ—§ä»£ç  {file_path}: {e}")
        
        if unused_imports and not self.dry_run:
            logger.info(f"      å‘ç° {len(unused_imports)} ä¸ªæœªä½¿ç”¨çš„å¯¼å…¥")
            # è¿™é‡Œå¯ä»¥å®ç°è‡ªåŠ¨æ¸…ç†é€»è¾‘
        else:
            logger.info("      æœªå‘ç°éœ€è¦æ¸…ç†çš„æ—§ä»£ç ")

    async def _optimize_imports(self):
        """ä¼˜åŒ–å¯¼å…¥"""
        logger.info("    ä¼˜åŒ–å¯¼å…¥...")
        
        # è¿™é‡Œå¯ä»¥å®ç°å¯¼å…¥ä¼˜åŒ–é€»è¾‘
        # ç›®å‰åªæ˜¯å ä½ç¬¦
        pass

    async def _cleanup_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        logger.info("    æ¸…ç†ç¼“å­˜...")
        
        # æ¸…ç†.iflow/cache
        cache_dir = self.workspace_path / ".iflow" / "cache"
        if cache_dir.exists():
            try:
                shutil.rmtree(cache_dir)
                cache_dir.mkdir(exist_ok=True)
                logger.info("      âœ… ç¼“å­˜å·²æ¸…ç†")
            except Exception as e:
                logger.warning(f"      æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")

    async def _generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Šï¼ˆçº¯æ£€æµ‹æ¨¡å¼ï¼‰"""
        logger.info("ğŸ“Š ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š...")
        
        try:
            # è®¡ç®—åˆ†æç»Ÿè®¡
            total_issues = len(self.issues)
            
            # æŒ‰ç±»å‹åˆ†ç±»é—®é¢˜
            issues_by_type = defaultdict(int)
            issues_by_severity = defaultdict(int)
            
            for issue in self.issues:
                issues_by_type[issue.category] += 1
                issues_by_severity[issue.severity.value] += 1
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            quality_score = self._calculate_quality_score()
            
            # æ„å»ºæœ€ç»ˆæŠ¥å‘Š
            final_report = {
                "analysis_summary": {
                    "session_id": self.session_id,
                    "start_time": datetime.fromtimestamp(self.start_time).isoformat(),
                    "end_time": datetime.now().isoformat(),
                    "duration_minutes": (time.time() - self.start_time) / 60,
                    "analysis_mode": "detection_only",
                    "total_issues_detected": total_issues,
                    "issues_by_type": dict(issues_by_type),
                    "issues_by_severity": dict(issues_by_severity)
                },
                "project_metrics": asdict(self.project_metrics),
                "quality_metrics": {
                    "quality_score": quality_score,
                    "maintainability_index": self.project_metrics.maintainability_index,
                    "test_coverage": self.project_metrics.test_coverage,
                    "complexity_score": self.project_metrics.complexity_score,
                    "security_score": self._calculate_security_score()
                },
                "architecture_analysis": self.architecture_analysis,
                "security_findings": [asdict(finding) for finding in self.security_findings],
                "detailed_issues": [asdict(issue) for issue in self.issues],
                "analysis_suggestions": await self._generate_comprehensive_suggestions(),
                "ai_training_data": await self._generate_ai_training_data(),
                "recommendations": await self._generate_recommendations()
            }
            
            return final_report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {e}")
            return {"error": str(e)}

    def _calculate_security_score(self) -> float:
        """è®¡ç®—å®‰å…¨è¯„åˆ†"""
        if not self.security_findings:
            return 100.0
        
        total_findings = len(self.security_findings)
        critical_count = len([f for f in self.security_findings if f.severity == Severity.CRITICAL])
        high_count = len([f for f in self.security_findings if f.severity == Severity.HIGH])
        
        # å®‰å…¨è¯„åˆ†è®¡ç®—
        score = 100.0
        score -= (critical_count * 25)  # ä¸¥é‡é—®é¢˜æ‰£25åˆ†
        score -= (high_count * 15)     # é«˜å±é—®é¢˜æ‰£15åˆ†
        score -= ((total_findings - critical_count - high_count) * 5)  # å…¶ä»–é—®é¢˜æ‰£5åˆ†
        
        return max(0.0, score)

    async def _generate_comprehensive_suggestions(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç»¼åˆå»ºè®®"""
        suggestions = []
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„é—®é¢˜
        critical_issues = [issue for issue in self.issues if issue.severity == Severity.CRITICAL]
        high_issues = [issue for issue in self.issues if issue.severity == Severity.HIGH]
        medium_issues = [issue for issue in self.issues if issue.severity == Severity.MEDIUM]
        low_issues = [issue for issue in self.issues if issue.severity == Severity.LOW]
        
        # ç”Ÿæˆä¼˜å…ˆçº§å»ºè®®
        if critical_issues:
            suggestions.append({
                "priority": "critical",
                "title": "ç«‹å³ä¿®å¤å…³é”®é—®é¢˜",
                "description": f"å‘ç° {len(critical_issues)} ä¸ªå…³é”®é—®é¢˜éœ€è¦ç«‹å³å¤„ç†",
                "issues": [asdict(issue) for issue in critical_issues[:5]],
                "estimated_effort": f"{len(critical_issues) * 4} å°æ—¶",
                "risk_level": "high"
            })
        
        if high_issues:
            suggestions.append({
                "priority": "high",
                "title": "ä¼˜å…ˆå¤„ç†é«˜é£é™©é—®é¢˜",
                "description": f"å‘ç° {len(high_issues)} ä¸ªé«˜é£é™©é—®é¢˜å»ºè®®ä¼˜å…ˆå¤„ç†",
                "issues": [asdict(issue) for issue in high_issues[:5]],
                "estimated_effort": f"{len(high_issues) * 2} å°æ—¶",
                "risk_level": "medium"
            })
        
        if medium_issues:
            suggestions.append({
                "priority": "medium",
                "title": "è®¡åˆ’å¤„ç†ä¸­ç­‰é—®é¢˜",
                "description": f"å‘ç° {len(medium_issues)} ä¸ªä¸­ç­‰é—®é¢˜å¯ä»¥è®¡åˆ’å¤„ç†",
                "issues": [asdict(issue) for issue in medium_issues[:5]],
                "estimated_effort": f"{len(medium_issues) * 1} å°æ—¶",
                "risk_level": "low"
            })
        
        return suggestions

    async def _generate_ai_training_data(self) -> Dict[str, Any]:
        """ç”ŸæˆAIè®­ç»ƒæ•°æ®é›†"""
        training_data = {
            "session_metadata": {
                "session_id": self.session_id,
                "timestamp": datetime.now().isoformat(),
                "project_path": str(self.workspace_path),
                "total_files_analyzed": self.project_metrics.total_files,
                "analysis_duration": (time.time() - self.start_time) / 60
            },
            "detected_patterns": {
                "code_quality_patterns": [],
                "security_patterns": [],
                "performance_patterns": [],
                "architecture_patterns": []
            },
            "fix_strategies": {},
            "user_preferences": asdict(self.ai_profile),
            "success_criteria": {}
        }
        
        # æå–æ£€æµ‹åˆ°çš„æ¨¡å¼
        for issue in self.issues:
            pattern = {
                "type": issue.type,
                "category": issue.category,
                "severity": issue.severity.value,
                "description": issue.description,
                "file_path": issue.file_path,
                "line_number": issue.line_number,
                "evidence": issue.evidence,
                "fix_suggestion": issue.fix_suggestion,
                "auto_fixable": issue.auto_fixable
            }
            
            if issue.category == "security":
                training_data["detected_patterns"]["security_patterns"].append(pattern)
            elif issue.category == "performance":
                training_data["detected_patterns"]["performance_patterns"].append(pattern)
            elif issue.category == "architecture":
                training_data["detected_patterns"]["architecture_patterns"].append(pattern)
            else:
                training_data["detected_patterns"]["code_quality_patterns"].append(pattern)
        
        # ç”Ÿæˆä¿®å¤ç­–ç•¥æ˜ å°„
        for issue in self.issues:
            if issue.type not in training_data["fix_strategies"]:
                training_data["fix_strategies"][issue.type] = []
            
            training_data["fix_strategies"][issue.type].append({
                "suggestion": issue.fix_suggestion,
                "auto_fixable": issue.auto_fixable,
                "context": f"æ–‡ä»¶: {issue.file_path}, è¡Œ: {issue.line_number}",
                "confidence": 0.8
            })
        
        # ç”ŸæˆæˆåŠŸæ ‡å‡†
        training_data["success_criteria"] = {
            "quality_threshold": 80.0,
            "security_threshold": 90.0,
            "performance_threshold": 85.0,
            "test_coverage_threshold": 70.0
        }
        
        return training_data

    def _calculate_quality_score(self) -> float:
        """è®¡ç®—è´¨é‡åˆ†æ•°"""
        scores = []
        
        # å¯ç»´æŠ¤æ€§åˆ†æ•° (0-100)
        scores.append(self.project_metrics.maintainability_index)
        
        # æµ‹è¯•è¦†ç›–ç‡åˆ†æ•° (0-100)
        scores.append(self.project_metrics.test_coverage)
        
        # å¤æ‚åº¦åˆ†æ•° (åå‘ï¼Œå¤æ‚åº¦è¶Šä½åˆ†æ•°è¶Šé«˜)
        complexity_score = max(0, 100 - (self.project_metrics.complexity_score * 2))
        scores.append(complexity_score)
        
        # å®‰å…¨åˆ†æ•° (åŸºäºå®‰å…¨é—®é¢˜æ•°é‡)
        security_issues = len([i for i in self.issues if i.category == "security"])
        security_score = max(0, 100 - (security_issues * 10))
        scores.append(security_score)
        
        return sum(scores) / len(scores)

    async def _generate_recommendations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        if self.project_metrics.test_coverage < 50:
            recommendations.append({
                "category": "testing",
                "priority": "high",
                "title": "å¢åŠ æµ‹è¯•è¦†ç›–ç‡",
                "description": f"å½“å‰æµ‹è¯•è¦†ç›–ç‡ä¸º {self.project_metrics.test_coverage:.1f}%ï¼Œå»ºè®®å¢åŠ åˆ°80%ä»¥ä¸Š",
                "effort": "high"
            })
        
        if self.project_metrics.maintainability_index < 60:
            recommendations.append({
                "category": "maintainability",
                "priority": "medium",
                "title": "æé«˜ä»£ç å¯ç»´æŠ¤æ€§",
                "description": f"å½“å‰å¯ç»´æŠ¤æ€§æŒ‡æ•°ä¸º {self.project_metrics.maintainability_index:.1f}ï¼Œå»ºè®®é‡æ„å¤æ‚ä»£ç ",
                "effort": "medium"
            })
        
        critical_security_issues = [i for i in self.security_findings if i.severity == Severity.CRITICAL]
        if critical_security_issues:
            recommendations.append({
                "category": "security",
                "priority": "critical",
                "title": "ä¿®å¤å…³é”®å®‰å…¨é—®é¢˜",
                "description": f"å‘ç° {len(critical_security_issues)} ä¸ªå…³é”®å®‰å…¨é—®é¢˜ï¼Œéœ€è¦ç«‹å³ä¿®å¤",
                "effort": "high"
            })
        
        return recommendations

    async def _save_upgrade_history(self, final_report: Dict[str, Any]):
        """ä¿å­˜å‡çº§å†å²"""
        logger.info("ğŸ’¾ ä¿å­˜å‡çº§å†å²...")
        
        try:
            # æ·»åŠ åˆ°å†å²è®°å½•
            history_entry = {
                "session_id": self.session_id,
                "timestamp": datetime.now().isoformat(),
                "version": self.version_info,
                "summary": final_report["upgrade_summary"],
                "quality_metrics": final_report["quality_metrics"]
            }
            
            self.upgrade_history.append(history_entry)
            
            # ä¿å­˜å†å²æ–‡ä»¶
            history_file = self.workspace_path / ".iflow" / "data" / "upgrade_history.json"
            history_file.parent.mkdir(exist_ok=True)
            
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(self.upgrade_history, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜AIåå¥½æ¡£æ¡ˆ
            profile_file = self.workspace_path / ".iflow" / "data" / "ai_profile.json"
            with open(profile_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(self.ai_profile), f, indent=2, ensure_ascii=False)
            
            logger.info("  âœ… å‡çº§å†å²å·²ä¿å­˜")
            
        except Exception as e:
            logger.error(f"  ä¿å­˜å‡çº§å†å²å¤±è´¥: {e}")

    async def _analyze_coding_patterns(self):
        """åˆ†æç¼–ç æ¨¡å¼"""
        # è¿™é‡Œå¯ä»¥å®ç°ç¼–ç æ¨¡å¼åˆ†æé€»è¾‘
        # ç”¨äºå­¦ä¹ ç”¨æˆ·åå¥½
        pass

    async def _learn_architectural_preferences(self):
        """å­¦ä¹ æ¶æ„åå¥½"""
        # è¿™é‡Œå¯ä»¥å®ç°æ¶æ„åå¥½å­¦ä¹ é€»è¾‘
        # ç”¨äºç†è§£ç”¨æˆ·çš„æ¶æ„é€‰æ‹©
        pass

    async def _understand_documentation_style(self):
        """ç†è§£æ–‡æ¡£é£æ ¼"""
        # è¿™é‡Œå¯ä»¥å®ç°æ–‡æ¡£é£æ ¼åˆ†æé€»è¾‘
        # ç”¨äºå­¦ä¹ ç”¨æˆ·çš„æ–‡æ¡£åå¥½
        pass

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å…¨è‡ªåŠ¨åŒ–é¡¹ç›®å®¡æŸ¥å’Œå‡çº§å·¥ä½œæµ")
    parser.add_argument("--workspace", "-w", default=".", help="å·¥ä½œç©ºé—´è·¯å¾„")
    parser.add_argument("--auto-fix", action="store_true", default=True, help="è‡ªåŠ¨ä¿®å¤é—®é¢˜")
    parser.add_argument("--no-backup", action="store_true", help="ä¸åˆ›å»ºå¤‡ä»½")
    parser.add_argument("--dry-run", action="store_true", help="åˆ†ææ¨¡å¼ï¼Œä»…ç”ŸæˆæŠ¥å‘Šä¸ä¿®æ”¹æ–‡ä»¶")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = {}
    if args.config and Path(args.config).exists():
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                config = json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return 1
    
    # è®¾ç½®é…ç½®
    config.update({
        "auto_fix": args.auto_fix,
        "backup_enabled": not args.no_backup,
        "analysis_mode": args.dry_run,
        "verbose": args.verbose
    })
    
    # åˆ›å»ºå¹¶æ‰§è¡Œå·¥ä½œæµ
    workflow = ComprehensiveProjectUpgradeWorkflow(args.workspace, config)
    
    try:
        await workflow.initialize()
        report = await workflow.execute_comprehensive_upgrade()
        
        # è¾“å‡ºç»“æœæ‘˜è¦
        summary = report["upgrade_summary"]
        print(f"\nğŸ‰ é¡¹ç›®å‡çº§å®Œæˆ!")
        print(f"ğŸ“Š æ€»é—®é¢˜æ•°: {summary['total_issues']}")
        print(f"âœ… å·²ä¿®å¤: {summary['fixed_issues']}")
        print(f"âŒ ä¿®å¤å¤±è´¥: {summary['failed_issues']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        print(f"â±ï¸ è€—æ—¶: {summary['duration_minutes']:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: .iflow/reports/upgrade_report_{workflow.session_id}.md")
        
        return 0
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)