#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”— ARQæ•°æ®é›†æˆå‘½ä»¤ - è‡ªåŠ¨æ•°æ®ç®¡ç†é›†æˆå·¥å…·
==============================================

è¿™ä¸ªå‘½ä»¤å°†ARQæ•°æ®ç®¡ç†åŠŸèƒ½æ— ç¼é›†æˆåˆ°å·¥ä½œæµä¸­ï¼Œç¡®ä¿ï¼š
- ğŸ”„ è‡ªåŠ¨è¯»å–å’Œè°ƒç”¨æœ¬åœ°æ•°æ®é›†
- ğŸ“Š è‡ªåŠ¨è®°å½•å’Œåˆ†æä¼šè¯æ•°æ®
- ğŸ§  æ™ºèƒ½æ€»ç»“å’ŒæŸ¥çœ‹å†å²ä¼šè¯
- ğŸ’¾ è‡ªåŠ¨åŒæ­¥çŸ¥è¯†åº“å’Œåå¥½æ•°æ®
- ğŸ¯ æ— éœ€æ‰‹åŠ¨å‘½ä»¤çš„å…¨è‡ªåŠ¨è¿è¡Œ

ä½¿ç”¨æ–¹æ³•:
    python arq-data-integration.py [--auto-start] [--config config.json]

ç‰¹æ€§:
- é›¶é…ç½®è‡ªåŠ¨å¯åŠ¨
- æ™ºèƒ½æ•°æ®å‘ç°å’Œé›†æˆ
- å®æ—¶ç›‘æ§å’Œä¼˜åŒ–
- å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ¢å¤

ä½œè€…: AIæ¶æ„å¸ˆå›¢é˜Ÿ
ç‰ˆæœ¬: 17.0.0 Hyperdimensional Singularity
æ—¥æœŸ: 2025-11-17
"""

import os
import sys
import json
import asyncio
import logging
import argparse
import signal
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta

# é¡¹ç›®æ ¹è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / ".iflow" / "core"))

# å¯¼å…¥ARQç»„ä»¶
try:
    from arq_data_manager_v17 import get_arq_data_manager, DataType, DataPriority
    from arq_data_analyzer_v17 import get_arq_data_analyzer
    ARQ_COMPONENTS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ARQç»„ä»¶ä¸å¯ç”¨: {e}")
    ARQ_COMPONENTS_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ARQDataIntegration:
    """ARQæ•°æ®é›†æˆä¸»ç±»"""
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–æ•°æ®é›†æˆ"""
        self.config = config or {}
        self.running = False
        
        # ARQç»„ä»¶
        self.data_manager = None
        self.data_analyzer = None
        
        # é›†æˆçŠ¶æ€
        self.integration_status = {
            "started_at": None,
            "last_sync": None,
            "data_processed": 0,
            "errors_count": 0,
            "auto_operations": []
        }
        
        # ç›‘æ§ä»»åŠ¡
        self.monitoring_tasks = []
        
        logger.info("ğŸ”— ARQæ•°æ®é›†æˆåˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self):
        """åˆå§‹åŒ–ARQç»„ä»¶"""
        if not ARQ_COMPONENTS_AVAILABLE:
            raise RuntimeError("ARQç»„ä»¶ä¸å¯ç”¨ï¼Œæ— æ³•åˆå§‹åŒ–")
        
        logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ARQç»„ä»¶...")
        
        # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        self.data_manager = get_arq_data_manager()
        await self.data_manager.start_auto_sync()
        
        # åˆå§‹åŒ–æ•°æ®åˆ†æå™¨
        self.data_analyzer = get_arq_data_analyzer()
        
        # åˆ›å»ºé»˜è®¤é¡¹ç›®ä¼šè¯
        await self._create_default_session()
        
        # åŠ è½½å†å²æ•°æ®
        await self._load_historical_data()
        
        logger.info("âœ… ARQç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    async def start_auto_integration(self):
        """å¯åŠ¨è‡ªåŠ¨é›†æˆ"""
        if self.running:
            logger.warning("âš ï¸ è‡ªåŠ¨é›†æˆå·²åœ¨è¿è¡Œ")
            return
        
        self.running = True
        self.integration_status["started_at"] = datetime.now()
        
        logger.info("ğŸ”„ å¯åŠ¨ARQè‡ªåŠ¨æ•°æ®é›†æˆ...")
        
        # å¯åŠ¨ç›‘æ§ä»»åŠ¡
        await self._start_monitoring_tasks()
        
        # æ‰§è¡Œåˆå§‹æ•°æ®åŒæ­¥
        await self._perform_initial_sync()
        
        # å¯åŠ¨ä¸»å¾ªç¯
        await self._main_integration_loop()
    
    async def stop_integration(self):
        """åœæ­¢é›†æˆ"""
        logger.info("â¹ï¸ æ­£åœ¨åœæ­¢ARQæ•°æ®é›†æˆ...")
        
        self.running = False
        
        # å–æ¶ˆç›‘æ§ä»»åŠ¡
        for task in self.monitoring_tasks:
            task.cancel()
        
        # æ¸…ç†èµ„æº
        if self.data_manager:
            await self.data_manager.cleanup()
        
        # ä¿å­˜é›†æˆçŠ¶æ€
        await self._save_integration_status()
        
        logger.info("âœ… ARQæ•°æ®é›†æˆå·²åœæ­¢")
    
    async def _create_default_session(self):
        """åˆ›å»ºé»˜è®¤ä¼šè¯"""
        try:
            session_id = await self.data_manager.create_session(
                project_id="arq_auto_integration",
                user_id="system",
                goals=["è‡ªåŠ¨æ•°æ®é›†æˆ", "æ™ºèƒ½åˆ†æ", "æŒç»­ä¼˜åŒ–"]
            )
            
            # å­˜å‚¨ä¼šè¯IDåˆ°é…ç½®
            self.config["default_session_id"] = session_id
            
            logger.info(f"âœ… åˆ›å»ºé»˜è®¤ä¼šè¯: {session_id}")
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºé»˜è®¤ä¼šè¯å¤±è´¥: {e}")
    
    async def _load_historical_data(self):
        """åŠ è½½å†å²æ•°æ®"""
        try:
            # æ‰«ææ•°æ®ç›®å½•
            data_dir = PROJECT_ROOT / "data"
            if not data_dir.exists():
                logger.info("ğŸ“ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡å†å²æ•°æ®åŠ è½½")
                return
            
            # åŠ è½½çŸ¥è¯†åº“æ•°æ®
            await self._load_knowledge_base(data_dir)
            
            # åŠ è½½æŸ¥è¯¢å†å²
            await self._load_query_history(data_dir)
            
            # åŠ è½½ä¼šè¯æ•°æ®
            await self._load_session_data(data_dir)
            
            logger.info("âœ… å†å²æ•°æ®åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å†å²æ•°æ®å¤±è´¥: {e}")
    
    async def _load_knowledge_base(self, data_dir: Path):
        """åŠ è½½çŸ¥è¯†åº“æ•°æ®"""
        kb_file = data_dir / "knowledge_base.json"
        if kb_file.exists():
            try:
                with open(kb_file, 'r', encoding='utf-8') as f:
                    kb_data = json.load(f)
                
                # å­˜å‚¨çŸ¥è¯†åº“æ•°æ®
                for kb_id, kb_item in kb_data.items():
                    await self.data_manager.store_data(
                        data=kb_item,
                        data_type=DataType.KNOWLEDGE_BASE,
                        priority=DataPriority.HIGH,
                        tags={"knowledge_base", "auto_imported"}
                    )
                
                logger.info(f"ğŸ“š åŠ è½½çŸ¥è¯†åº“æ•°æ®: {len(kb_data)} é¡¹")
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
    
    async def _load_query_history(self, data_dir: Path):
        """åŠ è½½æŸ¥è¯¢å†å²"""
        query_file = data_dir / "query_history.json"
        if query_file.exists():
            try:
                with open(query_file, 'r', encoding='utf-8') as f:
                    query_data = json.load(f)
                
                # å­˜å‚¨æŸ¥è¯¢å†å²
                for query_item in query_data:
                    session_id = self.config.get("default_session_id")
                    if session_id:
                        await self.data_manager.record_query(
                            session_id=session_id,
                            query=query_item.get("question", ""),
                            context=query_item.get("context", ""),
                            response=query_item.get("response", {}).get("answer", ""),
                            response_time=query_item.get("response_time", 0.0),
                            confidence=query_item.get("response", {}).get("confidence", 0.0)
                        )
                
                logger.info(f"ğŸ“ åŠ è½½æŸ¥è¯¢å†å²: {len(query_data)} æ¡")
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½æŸ¥è¯¢å†å²å¤±è´¥: {e}")
    
    async def _load_session_data(self, data_dir: Path):
        """åŠ è½½ä¼šè¯æ•°æ®"""
        sessions_dir = data_dir / "sessions"
        if sessions_dir.exists():
            try:
                session_files = list(sessions_dir.glob("*.json"))
                
                for session_file in session_files:
                    with open(session_file, 'r', encoding='utf-8') as f:
                        session_data = json.load(f)
                    
                    # å­˜å‚¨ä¼šè¯æ•°æ®
                    await self.data_manager.store_data(
                        data=session_data,
                        data_type=DataType.SESSION_DATA,
                        tags={"session", "auto_imported"}
                    )
                
                logger.info(f"ğŸ”„ åŠ è½½ä¼šè¯æ•°æ®: {len(session_files)} ä¸ªæ–‡ä»¶")
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½ä¼šè¯æ•°æ®å¤±è´¥: {e}")
    
    async def _start_monitoring_tasks(self):
        """å¯åŠ¨ç›‘æ§ä»»åŠ¡"""
        # æ•°æ®åŒæ­¥ç›‘æ§
        sync_task = asyncio.create_task(self._monitor_data_sync())
        self.monitoring_tasks.append(sync_task)
        
        # æ€§èƒ½ç›‘æ§
        perf_task = asyncio.create_task(self._monitor_performance())
        self.monitoring_tasks.append(perf_task)
        
        # æ•°æ®è´¨é‡ç›‘æ§
        quality_task = asyncio.create_task(self._monitor_data_quality())
        self.monitoring_tasks.append(quality_task)
        
        logger.info("ğŸ“Š ç›‘æ§ä»»åŠ¡å·²å¯åŠ¨")
    
    async def _monitor_data_sync(self):
        """ç›‘æ§æ•°æ®åŒæ­¥"""
        while self.running:
            try:
                # æ£€æŸ¥åŒæ­¥çŠ¶æ€
                summary = await self.data_manager.get_performance_summary()
                
                # è®°å½•åŒæ­¥æŒ‡æ ‡
                sync_info = {
                    "timestamp": datetime.now().isoformat(),
                    "cache_hit_rate": summary.get("cache_hit_rate", 0),
                    "active_sessions": summary.get("active_sessions", 0),
                    "memory_usage": summary.get("memory_usage", 0)
                }
                
                await self.data_manager.store_data(
                    data=sync_info,
                    data_type=DataType.SYSTEM_METRICS,
                    tags={"monitoring", "sync_status"}
                )
                
                await asyncio.sleep(300)  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ æ•°æ®åŒæ­¥ç›‘æ§å¤±è´¥: {e}")
                await asyncio.sleep(60)
    
    async def _monitor_performance(self):
        """ç›‘æ§æ€§èƒ½"""
        while self.running:
            try:
                # è·å–å®æ—¶æ´å¯Ÿ
                insights = await self.data_analyzer.get_real_time_insights()
                
                # æ£€æŸ¥æ€§èƒ½é—®é¢˜
                if insights["system_status"] != "healthy":
                    logger.warning(f"âš ï¸ ç³»ç»Ÿæ€§èƒ½è­¦å‘Š: {insights['system_status']}")
                    
                    # è®°å½•æ€§èƒ½é—®é¢˜
                    await self.data_manager.store_data(
                        data=insights,
                        data_type=DataType.SYSTEM_METRICS,
                        priority=DataPriority.HIGH,
                        tags={"performance", "warning"}
                    )
                
                await asyncio.sleep(180)  # 3åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ æ€§èƒ½ç›‘æ§å¤±è´¥: {e}")
                await asyncio.sleep(60)
    
    async def _monitor_data_quality(self):
        """ç›‘æ§æ•°æ®è´¨é‡"""
        while self.running:
            try:
                # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡æ•°æ®è´¨é‡æ£€æŸ¥
                await self._perform_quality_check()
                
                await asyncio.sleep(3600)  # 1å°æ—¶æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ æ•°æ®è´¨é‡ç›‘æ§å¤±è´¥: {e}")
                await asyncio.sleep(300)
    
    async def _perform_quality_check(self):
        """æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥"""
        try:
            # åˆ†æä½¿ç”¨æ¨¡å¼
            usage_analysis = await self.data_analyzer.analyze_usage_patterns(
                time_range=timedelta(hours=1)
            )
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            quality_score = usage_analysis.confidence
            
            if quality_score < 0.8:
                logger.warning(f"âš ï¸ æ•°æ®è´¨é‡ä¸‹é™: {quality_score:.1%}")
                
                # è®°å½•è´¨é‡é—®é¢˜
                await self.data_manager.store_data(
                    data={
                        "quality_score": quality_score,
                        "analysis": asdict(usage_analysis),
                        "timestamp": datetime.now().isoformat()
                    },
                    data_type=DataType.SYSTEM_METRICS,
                    tags={"quality", "warning"}
                )
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
    
    async def _perform_initial_sync(self):
        """æ‰§è¡Œåˆå§‹åŒæ­¥"""
        try:
            logger.info("ğŸ”„ æ‰§è¡Œåˆå§‹æ•°æ®åŒæ­¥...")
            
            # åŒæ­¥æ‰€æœ‰ç¼“å­˜æ•°æ®
            await self.data_manager._sync_memory_to_db()
            
            # æ‰§è¡Œæ•°æ®åº“ä¼˜åŒ–
            await self.data_manager._optimize_database()
            
            # ç”Ÿæˆåˆå§‹æŠ¥å‘Š
            await self._generate_integration_report()
            
            self.integration_status["last_sync"] = datetime.now()
            
            logger.info("âœ… åˆå§‹æ•°æ®åŒæ­¥å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒæ­¥å¤±è´¥: {e}")
    
    async def _main_integration_loop(self):
        """ä¸»é›†æˆå¾ªç¯"""
        logger.info("ğŸ”„ è¿›å…¥ä¸»é›†æˆå¾ªç¯...")
        
        while self.running:
            try:
                # è‡ªåŠ¨æ•°æ®æ”¶é›†
                await self._auto_collect_data()
                
                # æ™ºèƒ½æ•°æ®åˆ†æ
                await self._auto_analyze_data()
                
                # è‡ªåŠ¨ä¼˜åŒ–
                await self._auto_optimize()
                
                # æ›´æ–°é›†æˆçŠ¶æ€
                self.integration_status["last_sync"] = datetime.now()
                
                # ç­‰å¾…ä¸‹ä¸€ä¸ªå¾ªç¯
                await asyncio.sleep(self.config.get("integration_interval", 600))  # é»˜è®¤10åˆ†é’Ÿ
                
            except Exception as e:
                logger.error(f"âŒ ä¸»é›†æˆå¾ªç¯é”™è¯¯: {e}")
                self.integration_status["errors_count"] += 1
                await asyncio.sleep(60)
    
    async def _auto_collect_data(self):
        """è‡ªåŠ¨æ”¶é›†æ•°æ®"""
        try:
            # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
            system_metrics = await self._collect_system_metrics()
            
            # æ”¶é›†ç”¨æˆ·è¡Œä¸ºæ•°æ®
            behavior_data = await self._collect_behavior_data()
            
            # æ”¶é›†æ€§èƒ½æ•°æ®
            performance_data = await self._collect_performance_data()
            
            # å­˜å‚¨æ”¶é›†çš„æ•°æ®
            for data_type, data in [
                (DataType.SYSTEM_METRICS, system_metrics),
                (DataType.USER_PREFERENCES, behavior_data),
                (DataType.SYSTEM_METRICS, performance_data)
            ]:
                if data:
                    await self.data_manager.store_data(
                        data=data,
                        data_type=data_type,
                        tags={"auto_collected"}
                    )
            
            self.integration_status["data_processed"] += 1
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨æ•°æ®æ”¶é›†å¤±è´¥: {e}")
    
    async def _auto_analyze_data(self):
        """è‡ªåŠ¨åˆ†ææ•°æ®"""
        try:
            # å®šæœŸæ‰§è¡Œåˆ†æ
            current_time = datetime.now()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œåˆ†æ
            last_analysis = self.config.get("last_analysis")
            if not last_analysis or (current_time - datetime.fromisoformat(last_analysis)).hours >= 1:
                
                # æ‰§è¡Œä½¿ç”¨æ¨¡å¼åˆ†æ
                await self.data_analyzer.analyze_usage_patterns(
                    time_range=timedelta(hours=24)
                )
                
                # æ‰§è¡Œæ€§èƒ½åˆ†æ
                await self.data_analyzer.analyze_performance_metrics(
                    time_range=timedelta(hours=24)
                )
                
                # æ›´æ–°æœ€ååˆ†ææ—¶é—´
                self.config["last_analysis"] = current_time.isoformat()
                
                logger.info("ğŸ“Š è‡ªåŠ¨æ•°æ®åˆ†æå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨æ•°æ®åˆ†æå¤±è´¥: {e}")
    
    async def _auto_optimize(self):
        """è‡ªåŠ¨ä¼˜åŒ–"""
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¼˜åŒ–
            summary = await self.data_manager.get_performance_summary()
            
            # ç¼“å­˜å‘½ä¸­ç‡ä½æ—¶ä¼˜åŒ–
            if summary.get("cache_hit_rate", 0) < 0.7:
                await self._optimize_cache()
            
            # å†…å­˜ä½¿ç”¨é«˜æ—¶ä¼˜åŒ–
            if summary.get("memory_usage", 0) > 512:  # 512MB
                await self._optimize_memory()
            
            # æ•°æ®åº“éœ€è¦ä¼˜åŒ–æ—¶
            if self.integration_status["data_processed"] % 100 == 0:
                await self.data_manager._optimize_database()
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨ä¼˜åŒ–å¤±è´¥: {e}")
    
    async def _optimize_cache(self):
        """ä¼˜åŒ–ç¼“å­˜"""
        try:
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            await self.data_manager._cleanup_expired_data()
            
            # è°ƒæ•´ç¼“å­˜ç­–ç•¥
            logger.info("âš¡ ç¼“å­˜ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜ä¼˜åŒ–å¤±è´¥: {e}")
    
    async def _optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜"""
        try:
            import gc
            
            # æ‰§è¡Œåƒåœ¾å›æ”¶
            collected = gc.collect()
            
            # æ¸…ç†å†…å­˜ç¼“å­˜
            if hasattr(self.data_manager, 'memory_cache'):
                with self.data_manager.cache_lock:
                    # ä¿ç•™æœ€è¿‘ä½¿ç”¨çš„50%æ•°æ®
                    cache_items = list(self.data_manager.memory_cache.items())
                    keep_count = len(cache_items) // 2
                    
                    # æŒ‰è®¿é—®æ—¶é—´æ’åº
                    cache_items.sort(key=lambda x: x[1].last_accessed, reverse=True)
                    
                    # ä¿ç•™çƒ­é—¨æ•°æ®
                    self.data_manager.memory_cache = dict(cache_items[:keep_count])
            
            logger.info(f"ğŸ§  å†…å­˜ä¼˜åŒ–å®Œæˆï¼Œå›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
            
        except Exception as e:
            logger.error(f"âŒ å†…å­˜ä¼˜åŒ–å¤±è´¥: {e}")
    
    async def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            import psutil
            
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # å†…å­˜ä½¿ç”¨
            memory = psutil.virtual_memory()
            
            # ç£ç›˜ä½¿ç”¨
            disk = psutil.disk_usage(PROJECT_ROOT)
            
            return {
                "timestamp": datetime.now().isoformat(),
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "memory_used_mb": memory.used / (1024 * 1024),
                "disk_percent": disk.percent,
                "disk_free_gb": disk.free / (1024 * 1024 * 1024)
            }
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
            return None
    
    async def _collect_behavior_data(self):
        """æ”¶é›†è¡Œä¸ºæ•°æ®"""
        try:
            # è·å–æ´»è·ƒä¼šè¯ç»Ÿè®¡
            summary = await self.data_manager.get_performance_summary()
            
            return {
                "timestamp": datetime.now().isoformat(),
                "active_sessions": summary.get("active_sessions", 0),
                "cache_hit_rate": summary.get("cache_hit_rate", 0),
                "data_reads": summary.get("performance_metrics", {}).get("data_reads", 0),
                "data_writes": summary.get("performance_metrics", {}).get("data_writes", 0)
            }
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†è¡Œä¸ºæ•°æ®å¤±è´¥: {e}")
            return None
    
    async def _collect_performance_data(self):
        """æ”¶é›†æ€§èƒ½æ•°æ®"""
        try:
            # è·å–å®æ—¶æ´å¯Ÿ
            insights = await self.data_analyzer.get_real_time_insights()
            
            return {
                "timestamp": datetime.now().isoformat(),
                "system_status": insights.get("system_status"),
                "current_metrics": insights.get("current_metrics", {}),
                "insights_count": len(insights.get("insights", [])),
                "anomalies_count": len(insights.get("anomalies", []))
            }
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†æ€§èƒ½æ•°æ®å¤±è´¥: {e}")
            return None
    
    async def _generate_integration_report(self):
        """ç”Ÿæˆé›†æˆæŠ¥å‘Š"""
        try:
            report = {
                "integration_id": str(int(time.time())),
                "generated_at": datetime.now().isoformat(),
                "status": "active",
                "components": {
                    "data_manager": "active",
                    "data_analyzer": "active"
                },
                "statistics": {
                    "started_at": self.integration_status["started_at"].isoformat(),
                    "data_processed": self.integration_status["data_processed"],
                    "errors_count": self.integration_status["errors_count"],
                    "last_sync": self.integration_status["last_sync"].isoformat() if self.integration_status["last_sync"] else None
                },
                "configuration": self.config
            }
            
            # ä¿å­˜æŠ¥å‘Š
            await self.data_manager.store_data(
                data=report,
                data_type=DataType.SYSTEM_METRICS,
                priority=DataPriority.HIGH,
                tags={"integration", "report"}
            )
            
            logger.info("ğŸ“‹ é›†æˆæŠ¥å‘Šå·²ç”Ÿæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆé›†æˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    async def _save_integration_status(self):
        """ä¿å­˜é›†æˆçŠ¶æ€"""
        try:
            status_file = PROJECT_ROOT / "data" / "integration_status.json"
            status_file.parent.mkdir(exist_ok=True)
            
            with open(status_file, 'w', encoding='utf-8') as f:
                json.dump(self.integration_status, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info("ğŸ’¾ é›†æˆçŠ¶æ€å·²ä¿å­˜")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é›†æˆçŠ¶æ€å¤±è´¥: {e}")

# å…¨å±€é›†æˆå®ä¾‹
_global_integration: Optional[ARQDataIntegration] = None

def get_integration() -> ARQDataIntegration:
    """è·å–å…¨å±€é›†æˆå®ä¾‹"""
    global _global_integration
    if _global_integration is None:
        _global_integration = ARQDataIntegration()
    return _global_integration

# ä¿¡å·å¤„ç†
def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…åœæ­¢...")
    
    integration = get_integration()
    asyncio.create_task(integration.stop_integration())

# ä¸»å‡½æ•°
async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ARQæ•°æ®é›†æˆå·¥å…·")
    parser.add_argument("--auto-start", action="store_true", help="è‡ªåŠ¨å¯åŠ¨é›†æˆ")
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--daemon", action="store_true", help="å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = {}
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
    
    # åˆ›å»ºé›†æˆå®ä¾‹
    integration = ARQDataIntegration(config)
    
    # è®¾ç½®ä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        # åˆå§‹åŒ–
        await integration.initialize()
        
        if args.auto_start or args.daemon:
            # å¯åŠ¨è‡ªåŠ¨é›†æˆ
            await integration.start_auto_integration()
        else:
            # äº¤äº’æ¨¡å¼
            print("ğŸ”— ARQæ•°æ®é›†æˆå·²å‡†å¤‡å°±ç»ª")
            print("å¯ç”¨å‘½ä»¤:")
            print("  start    - å¯åŠ¨è‡ªåŠ¨é›†æˆ")
            print("  status   - æŸ¥çœ‹é›†æˆçŠ¶æ€")
            print("  stop     - åœæ­¢é›†æˆ")
            print("  report   - ç”ŸæˆæŠ¥å‘Š")
            print("  exit     - é€€å‡ºç¨‹åº")
            
            while True:
                try:
                    command = input("\n> ").strip().lower()
                    
                    if command == "start":
                        await integration.start_auto_integration()
                    elif command == "status":
                        summary = await integration.data_manager.get_performance_summary()
                        print(f"çŠ¶æ€: {summary}")
                    elif command == "stop":
                        await integration.stop_integration()
                    elif command == "report":
                        report = await integration.data_analyzer.generate_comprehensive_report()
                        print(f"æŠ¥å‘ŠID: {report['report_id']}")
                    elif command == "exit":
                        break
                    else:
                        print("æœªçŸ¥å‘½ä»¤")
                        
                except EOFError:
                    break
                except Exception as e:
                    print(f"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")
        
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·")
    except Exception as e:
        logger.error(f"ç¨‹åºé”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        await integration.stop_integration()
        logger.info("ğŸ‘‹ ARQæ•°æ®é›†æˆå·²é€€å‡º")

if __name__ == "__main__":
    asyncio.run(main())