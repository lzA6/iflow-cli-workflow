#!/usr/bin/env python3
"""
å®Œæ•´ä¾æ®å’Œè§£é‡Šç³»ç»Ÿ
ä¸ºæ¯ä¸ªå†³ç­–æä¾›è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ã€è¯æ®é“¾å’Œè‡ªæˆ‘åçœ
"""

import json
import re
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime
import asyncio
from enum import Enum

class DecisionType(Enum):
    """å†³ç­–ç±»å‹"""
    FILE_RETENTION = "file_retention"
    FILE_REMOVAL = "file_removal"
    CODE_REFACTOR = "code_refactor"
    SECURITY_FIX = "security_fix"
    PERFORMANCE_OPTIMIZE = "performance_optimize"
    ARCHITECTURE_CHANGE = "architecture_change"

class EvidenceType(Enum):
    """è¯æ®ç±»å‹"""
    CODE_ANALYSIS = "code_analysis"
    METRICS_DATA = "metrics_data"
    DEPENDENCY_GRAPH = "dependency_graph"
    SECURITY_SCAN = "security_scan"
    PERFORMANCE_TEST = "performance_test"
    USER_FEEDBACK = "user_feedback"
    BEST_PRACTICES = "best_practices"
    INDUSTRY_STANDARDS = "industry_standards"

@dataclass
class Evidence:
    """è¯æ®"""
    evidence_type: EvidenceType
    source: str
    content: str
    confidence: float
    timestamp: str
    verification_method: str
    supporting_data: Dict[str, Any]

@dataclass
class ReasoningStep:
    """æ¨ç†æ­¥éª¤"""
    step_number: int
    description: str
    input_data: Dict[str, Any]
    reasoning_process: str
    conclusion: str
    confidence: float
    assumptions: List[str]
    limitations: List[str]

@dataclass
class SelfReflection:
    """è‡ªæˆ‘åçœ"""
    reflection_type: str
    question: str
    analysis: str
    insights: List[str]
    biases_identified: List[str]
    alternative_approaches: List[str]
    confidence_adjustment: float

@dataclass
class DecisionRecord:
    """å†³ç­–è®°å½•"""
    decision_id: str
    decision_type: DecisionType
    target: str  # ç›®æ ‡æ–‡ä»¶æˆ–æ¨¡å—
    decision: str  # å†³ç­–ç»“æœ
    reasoning_chain: List[ReasoningStep]
    evidence_chain: List[Evidence]
    self_reflections: List[SelfReflection]
    confidence_score: float
    risk_assessment: str
    impact_analysis: Dict[str, Any]
    alternatives_considered: List[str]
    final_justification: str
    timestamp: str

class ComprehensiveJustificationSystem:
    """å®Œæ•´ä¾æ®å’Œè§£é‡Šç³»ç»Ÿ"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.decision_records = []
        self.evidence_database = {}
        self.reasoning_templates = self._load_reasoning_templates()
        self.justification_principles = self._load_justification_principles()
        
    async def create_comprehensive_decision(self, 
                                          decision_type: DecisionType,
                                          target: str,
                                          analysis_data: Dict[str, Any]) -> DecisionRecord:
        """åˆ›å»ºç»¼åˆå†³ç­–è®°å½•"""
        print(f"ğŸ§  åˆ›å»ºç»¼åˆå†³ç­–: {decision_type.value} for {target}")
        
        # 1. ç”Ÿæˆå†³ç­–ID
        decision_id = await self._generate_decision_id(decision_type, target)
        
        # 2. æ”¶é›†è¯æ®
        print("ğŸ“š æ”¶é›†è¯æ®...")
        evidence_chain = await self._collect_evidence(decision_type, target, analysis_data)
        
        # 3. æ„å»ºæ¨ç†é“¾
        print("ğŸ”— æ„å»ºæ¨ç†é“¾...")
        reasoning_chain = await self._build_reasoning_chain(decision_type, target, evidence_chain, analysis_data)
        
        # 4. æ‰§è¡Œè‡ªæˆ‘åçœ
        print("ğŸ¤” æ‰§è¡Œè‡ªæˆ‘åçœ...")
        self_reflections = await self._perform_self_reflection(decision_type, target, reasoning_chain, evidence_chain)
        
        # 5. è¯„ä¼°ç½®ä¿¡åº¦
        confidence_score = await self._calculate_confidence_score(evidence_chain, reasoning_chain, self_reflections)
        
        # 6. é£é™©è¯„ä¼°
        risk_assessment = await self._assess_risks(decision_type, target, evidence_chain)
        
        # 7. å½±å“åˆ†æ
        impact_analysis = await self._analyze_impact(decision_type, target, evidence_chain, analysis_data)
        
        # 8. è€ƒè™‘æ›¿ä»£æ–¹æ¡ˆ
        alternatives_considered = await self._consider_alternatives(decision_type, target, analysis_data)
        
        # 9. ç”Ÿæˆæœ€ç»ˆå†³ç­–
        final_decision = await self._generate_final_decision(decision_type, target, reasoning_chain, confidence_score)
        
        # 10. ç”Ÿæˆæœ€ç»ˆä¾æ®
        final_justification = await self._generate_final_justification(
            decision_type, target, final_decision, evidence_chain, reasoning_chain, self_reflections
        )
        
        # 11. åˆ›å»ºå†³ç­–è®°å½•
        decision_record = DecisionRecord(
            decision_id=decision_id,
            decision_type=decision_type,
            target=target,
            decision=final_decision,
            reasoning_chain=reasoning_chain,
            evidence_chain=evidence_chain,
            self_reflections=self_reflections,
            confidence_score=confidence_score,
            risk_assessment=risk_assessment,
            impact_analysis=impact_analysis,
            alternatives_considered=alternatives_considered,
            final_justification=final_justification,
            timestamp=datetime.now().isoformat()
        )
        
        # 12. ä¿å­˜å†³ç­–è®°å½•
        await self._save_decision_record(decision_record)
        
        print(f"âœ… ç»¼åˆå†³ç­–åˆ›å»ºå®Œæˆ: {decision_id}")
        return decision_record
    
    async def _generate_decision_id(self, decision_type: DecisionType, target: str) -> str:
        """ç”Ÿæˆå†³ç­–ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        content = f"{decision_type.value}_{target}_{timestamp}"
        hash_id = hashlib.md5(content.encode()).hexdigest()[:8]
        return f"DEC_{hash_id}"
    
    async def _collect_evidence(self, decision_type: DecisionType, 
                              target: str, analysis_data: Dict[str, Any]) -> List[Evidence]:
        """æ”¶é›†è¯æ®"""
        evidence_chain = []
        
        # 1. ä»£ç åˆ†æè¯æ®
        code_evidence = await self._collect_code_analysis_evidence(target, analysis_data)
        evidence_chain.extend(code_evidence)
        
        # 2. æŒ‡æ ‡æ•°æ®è¯æ®
        metrics_evidence = await self._collect_metrics_evidence(target, analysis_data)
        evidence_chain.extend(metrics_evidence)
        
        # 3. ä¾èµ–å…³ç³»è¯æ®
        dependency_evidence = await self._collect_dependency_evidence(target, analysis_data)
        evidence_chain.extend(dependency_evidence)
        
        # 4. å®‰å…¨æ‰«æè¯æ®
        if decision_type in [DecisionType.FILE_REMOVAL, DecisionType.SECURITY_FIX]:
            security_evidence = await self._collect_security_evidence(target, analysis_data)
            evidence_chain.extend(security_evidence)
        
        # 5. æ€§èƒ½æµ‹è¯•è¯æ®
        if decision_type in [DecisionType.PERFORMANCE_OPTIMIZE, DecisionType.FILE_REMOVAL]:
            performance_evidence = await self._collect_performance_evidence(target, analysis_data)
            evidence_chain.extend(performance_evidence)
        
        # 6. æœ€ä½³å®è·µè¯æ®
        best_practices_evidence = await self._collect_best_practices_evidence(decision_type, target)
        evidence_chain.extend(best_practices_evidence)
        
        # 7. è¡Œä¸šæ ‡å‡†è¯æ®
        standards_evidence = await self._collect_standards_evidence(decision_type, target)
        evidence_chain.extend(standards_evidence)
        
        return evidence_chain
    
    async def _collect_code_analysis_evidence(self, target: str, analysis_data: Dict[str, Any]) -> List[Evidence]:
        """æ”¶é›†ä»£ç åˆ†æè¯æ®"""
        evidence = []
        
        target_path = self.project_root / target
        if not target_path.exists():
            return evidence
        
        try:
            with open(target_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä»£ç ç»“æ„åˆ†æ
            lines = content.split('\n')
            functions = re.findall(r'def\s+(\w+)', content)
            classes = re.findall(r'class\s+(\w+)', content)
            imports = re.findall(r'import\s+(\w+)|from\s+(\w+)', content)
            
            code_structure = {
                "total_lines": len(lines),
                "code_lines": len([line for line in lines if line.strip() and not line.strip().startswith('#')]),
                "functions": functions,
                "classes": classes,
                "imports": [imp[0] or imp[1] for imp in imports],
                "complexity_indicators": {
                    "nested_loops": content.count('for') + content.count('while'),
                    "conditional_statements": content.count('if') + content.count('elif'),
                    "exception_handling": content.count('try:') + content.count('except'),
                    "async_functions": content.count('async def')
                }
            }
            
            evidence.append(Evidence(
                evidence_type=EvidenceType.CODE_ANALYSIS,
                source=f"static_analysis:{target}",
                content=f"ä»£ç ç»“æ„åˆ†æ: {len(functions)}ä¸ªå‡½æ•°, {len(classes)}ä¸ªç±», {len(lines)}è¡Œä»£ç ",
                confidence=0.9,
                timestamp=datetime.now().isoformat(),
                verification_method="static_code_analysis",
                supporting_data=code_structure
            ))
            
        except Exception as e:
            evidence.append(Evidence(
                evidence_type=EvidenceType.CODE_ANALYSIS,
                source=f"error_analysis:{target}",
                content=f"ä»£ç åˆ†æå¤±è´¥: {e}",
                confidence=0.1,
                timestamp=datetime.now().isoformat(),
                verification_method="error_handling",
                supporting_data={"error": str(e)}
            ))
        
        return evidence
    
    async def _collect_metrics_evidence(self, target: str, analysis_data: Dict[str, Any]) -> List[Evidence]:
        """æ”¶é›†æŒ‡æ ‡æ•°æ®è¯æ®"""
        evidence = []
        
        if "metrics" in analysis_data:
            metrics = analysis_data["metrics"]
            
            evidence.append(Evidence(
                evidence_type=EvidenceType.METRICS_DATA,
                source=f"metrics_analysis:{target}",
                content=f"æŒ‡æ ‡æ•°æ®: å¤æ‚åº¦={metrics.get('complexity', 'N/A')}, å¯ç»´æŠ¤æ€§={metrics.get('maintainability', 'N/A')}",
                confidence=0.8,
                timestamp=datetime.now().isoformat(),
                verification_method="automated_metrics_calculation",
                supporting_data=metrics
            ))
        
        return evidence
    
    async def _collect_dependency_evidence(self, target: str, analysis_data: Dict[str, Any]) -> List[Evidence]:
        """æ”¶é›†ä¾èµ–å…³ç³»è¯æ®"""
        evidence = []
        
        if "dependencies" in analysis_data:
            dependencies = analysis_data["dependencies"]
            
            dependency_count = len(dependencies)
            critical_dependencies = [dep for dep in dependencies if any(keyword in dep.lower() for keyword in ['core', 'engine', 'security'])]
            
            evidence.append(Evidence(
                evidence_type=EvidenceType.DEPENDENCY_GRAPH,
                source=f"dependency_analysis:{target}",
                content=f"ä¾èµ–åˆ†æ: {dependency_count}ä¸ªä¾èµ–, {len(critical_dependencies)}ä¸ªå…³é”®ä¾èµ–",
                confidence=0.85,
                timestamp=datetime.now().isoformat(),
                verification_method="dependency_parsing",
                supporting_data={
                    "total_dependencies": dependency_count,
                    "critical_dependencies": critical_dependencies,
                    "dependency_list": dependencies
                }
            ))
        
        return evidence
    
    async def _collect_security_evidence(self, target: str, analysis_data: Dict[str, Any]) -> List[Evidence]:
        """æ”¶é›†å®‰å…¨æ‰«æè¯æ®"""
        evidence = []
        
        if "security_issues" in analysis_data:
            security_issues = analysis_data["security_issues"]
            
            high_risk_issues = [issue for issue in security_issues if issue.get("severity") == "high"]
            medium_risk_issues = [issue for issue in security_issues if issue.get("severity") == "medium"]
            
            evidence.append(Evidence(
                evidence_type=EvidenceType.SECURITY_SCAN,
                source=f"security_scan:{target}",
                content=f"å®‰å…¨æ‰«æ: {len(security_issues)}ä¸ªé—®é¢˜, {len(high_risk_issues)}ä¸ªé«˜é£é™©, {len(medium_risk_issues)}ä¸ªä¸­é£é™©",
                confidence=0.9,
                timestamp=datetime.now().isoformat(),
                verification_method="automated_security_scanning",
                supporting_data={
                    "total_issues": len(security_issues),
                    "high_risk_issues": high_risk_issues,
                    "medium_risk_issues": medium_risk_issues,
                    "all_issues": security_issues
                }
            ))
        
        return evidence
    
    async def _collect_performance_evidence(self, target: str, analysis_data: Dict[str, Any]) -> List[Evidence]:
        """æ”¶é›†æ€§èƒ½æµ‹è¯•è¯æ®"""
        evidence = []
        
        if "performance_metrics" in analysis_data:
            perf_metrics = analysis_data["performance_metrics"]
            
            evidence.append(Evidence(
                evidence_type=EvidenceType.PERFORMANCE_TEST,
                source=f"performance_analysis:{target}",
                content=f"æ€§èƒ½åˆ†æ: æ‰§è¡Œæ—¶é—´={perf_metrics.get('execution_time', 'N/A')}ms, å†…å­˜ä½¿ç”¨={perf_metrics.get('memory_usage', 'N/A')}MB",
                confidence=0.8,
                timestamp=datetime.now().isoformat(),
                verification_method="performance_benchmarking",
                supporting_data=perf_metrics
            ))
        
        return evidence
    
    async def _collect_best_practices_evidence(self, decision_type: DecisionType, target: str) -> List[Evidence]:
        """æ”¶é›†æœ€ä½³å®è·µè¯æ®"""
        evidence = []
        
        best_practices = {
            DecisionType.FILE_RETENTION: [
                "ä¿ç•™å…·æœ‰ç‹¬ç‰¹ä¸šåŠ¡ä»·å€¼çš„æ–‡ä»¶",
                "ä¿ç•™è¢«å¤šä¸ªæ¨¡å—ä¾èµ–çš„æ ¸å¿ƒæ–‡ä»¶",
                "ä¿ç•™åŒ…å«å…³é”®ç®—æ³•æˆ–çŸ¥è¯†äº§æƒçš„æ–‡ä»¶"
            ],
            DecisionType.FILE_REMOVAL: [
                "åˆ é™¤åŠŸèƒ½å®Œå…¨é‡å¤çš„æ–‡ä»¶",
                "åˆ é™¤æ— å®é™…ç”¨é€”çš„è¿‡æ—¶æ–‡ä»¶",
                "åˆ é™¤æµ‹è¯•ç”¨ä¸´æ—¶æ–‡ä»¶"
            ],
            DecisionType.CODE_REFACTOR: [
                "é‡æ„é«˜å¤æ‚åº¦ä»£ç ä»¥æå‡å¯ç»´æŠ¤æ€§",
                "é‡æ„é‡å¤ä»£ç ä»¥éµå¾ªDRYåŸåˆ™",
                "é‡æ„è¿åå•ä¸€èŒè´£åŸåˆ™çš„ä»£ç "
            ]
        }
        
        if decision_type in best_practices:
            practices = best_practices[decision_type]
            
            for practice in practices:
                evidence.append(Evidence(
                    evidence_type=EvidenceType.BEST_PRACTICES,
                    source="industry_best_practices",
                    content=f"æœ€ä½³å®è·µ: {practice}",
                    confidence=0.7,
                    timestamp=datetime.now().isoformat(),
                    verification_method="industry_guidelines",
                    supporting_data={"practice": practice, "category": decision_type.value}
                ))
        
        return evidence
    
    async def _collect_standards_evidence(self, decision_type: DecisionType, target: str) -> List[Evidence]:
        """æ”¶é›†è¡Œä¸šæ ‡å‡†è¯æ®"""
        evidence = []
        
        standards = {
            "code_quality": "éµå¾ªISO/IEC 25010è½¯ä»¶è´¨é‡æ¨¡å‹æ ‡å‡†",
            "security": "éµå¾ªOWASPå®‰å…¨æ ‡å‡†å’ŒCWEåˆ†ç±»",
            "performance": "éµå¾ªæ€§èƒ½æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•æ ‡å‡†",
            "maintainability": "éµå¾ªå¯ç»´æŠ¤æ€§æŒ‡æ•°è®¡ç®—æ ‡å‡†"
        }
        
        for standard_name, standard_desc in standards.items():
            evidence.append(Evidence(
                evidence_type=EvidenceType.INDUSTRY_STANDARDS,
                source="industry_standards",
                content=f"è¡Œä¸šæ ‡å‡†: {standard_desc}",
                confidence=0.8,
                timestamp=datetime.now().isoformat(),
                verification_method="standards_compliance_check",
                supporting_data={"standard": standard_name, "description": standard_desc}
            ))
        
        return evidence
    
    async def _build_reasoning_chain(self, decision_type: DecisionType, 
                                   target: str, 
                                   evidence_chain: List[Evidence],
                                   analysis_data: Dict[str, Any]) -> List[ReasoningStep]:
        """æ„å»ºæ¨ç†é“¾"""
        reasoning_chain = []
        
        # æ­¥éª¤1: é—®é¢˜å®šä¹‰
        step1 = ReasoningStep(
            step_number=1,
            description=f"å®šä¹‰{decision_type.value}å†³ç­–é—®é¢˜",
            input_data={"target": target, "decision_type": decision_type.value},
            reasoning_process=f"åŸºäºç›®æ ‡'{target}'å’Œå†³ç­–ç±»å‹'{decision_type.value}'ï¼Œæ˜ç¡®éœ€è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜",
            conclusion=f"éœ€è¦é’ˆå¯¹'{target}'è¿›è¡Œ{decision_type.value}å†³ç­–",
            confidence=0.95,
            assumptions=["ç›®æ ‡æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®", "åˆ†ææ•°æ®å‡†ç¡®å¯é "],
            limitations=["å¯èƒ½å­˜åœ¨æœªè€ƒè™‘çš„å¤–éƒ¨å› ç´ ", "åˆ†æç»“æœçš„æ—¶æ•ˆæ€§é™åˆ¶"]
        )
        reasoning_chain.append(step1)
        
        # æ­¥éª¤2: è¯æ®è¯„ä¼°
        evidence_summary = await self._summarize_evidence(evidence_chain)
        step2 = ReasoningStep(
            step_number=2,
            description="è¯„ä¼°æ”¶é›†åˆ°çš„è¯æ®",
            input_data={"evidence_count": len(evidence_chain), "evidence_types": [e.evidence_type.value for e in evidence_chain]},
            reasoning_process=f"åˆ†æ{len(evidence_chain)}ä¸ªè¯æ®çš„å¯é æ€§å’Œç›¸å…³æ€§ï¼Œ{evidence_summary}",
            conclusion=f"è¯æ®æ€»ä½“æ”¯æŒå†³ç­–åˆ¶å®šï¼Œç½®ä¿¡åº¦è¾ƒé«˜",
            confidence=0.85,
            assumptions=["è¯æ®æ¥æºå¯é ", "è¯æ®åˆ†ææ–¹æ³•æ­£ç¡®"],
            limitations=["è¯æ®å¯èƒ½ä¸å®Œæ•´", "éƒ¨åˆ†è¯æ®å­˜åœ¨ä¸»è§‚æ€§"]
        )
        reasoning_chain.append(step2)
        
        # æ­¥éª¤3: å½±å“åˆ†æ
        impact_analysis = await self._analyze_decision_impact(decision_type, target, evidence_chain)
        step3 = ReasoningStep(
            step_number=3,
            description="åˆ†æå†³ç­–çš„æ½œåœ¨å½±å“",
            input_data={"decision_type": decision_type.value, "target": target},
            reasoning_process=f"åŸºäºè¯æ®åˆ†æ{decision_type.value}å¯¹ç³»ç»Ÿçš„å¤šæ–¹é¢å½±å“: {impact_analysis}",
            conclusion=f"å†³ç­–å°†å¯¹ç³»ç»Ÿäº§ç”Ÿ{impact_analysis['overall_impact']}çº§åˆ«çš„å½±å“",
            confidence=0.8,
            assumptions=["å½±å“æ¨¡å‹å‡†ç¡®", "ç³»ç»Ÿä¾èµ–å…³ç³»æ˜ç¡®"],
            limitations=["éš¾ä»¥é¢„æµ‹æ‰€æœ‰è¿é”ååº”", "å¤–éƒ¨ç¯å¢ƒå˜åŒ–çš„ä¸ç¡®å®šæ€§"]
        )
        reasoning_chain.append(step3)
        
        # æ­¥éª¤4: é£é™©è¯„ä¼°
        risk_assessment = await self._assess_decision_risks(decision_type, target, evidence_chain)
        step4 = ReasoningStep(
            step_number=4,
            description="è¯„ä¼°å†³ç­–é£é™©",
            input_data={"risk_factors": risk_assessment},
            reasoning_process=f"è¯†åˆ«å’Œè¯„ä¼°{decision_type.value}çš„ä¸»è¦é£é™©å› ç´ : {risk_assessment}",
            conclusion=f"å†³ç­–é£é™©ç­‰çº§ä¸º{risk_assessment['risk_level']}ï¼Œéœ€è¦{risk_assessment['mitigation_strategy']}",
            confidence=0.75,
            assumptions=["é£é™©è¯†åˆ«å…¨é¢", "é£é™©è¯„ä¼°æ–¹æ³•åˆç†"],
            limitations=["æœªçŸ¥é£é™©çš„å­˜åœ¨", "é£é™©æ¦‚ç‡ä¼°ç®—çš„ä¸ç¡®å®šæ€§"]
        )
        reasoning_chain.append(step4)
        
        # æ­¥éª¤5: æ›¿ä»£æ–¹æ¡ˆæ¯”è¾ƒ
        alternatives = await self._generate_alternatives(decision_type, target)
        step5 = ReasoningStep(
            step_number=5,
            description="æ¯”è¾ƒæ›¿ä»£æ–¹æ¡ˆ",
            input_data={"alternatives": alternatives},
            reasoning_process=f"åˆ†æ{len(alternatives)}ä¸ªæ›¿ä»£æ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹ï¼Œé€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ",
            conclusion="åŸºäºç»¼åˆè¯„ä¼°ï¼Œå½“å‰æ–¹æ¡ˆæ˜¯æœ€ä¼˜é€‰æ‹©",
            confidence=0.7,
            assumptions=["æ›¿ä»£æ–¹æ¡ˆè¯†åˆ«å®Œæ•´", "è¯„ä¼°æ ‡å‡†åˆç†"],
            limitations=["å¯èƒ½å­˜åœ¨æœªè€ƒè™‘çš„æ›¿ä»£æ–¹æ¡ˆ", "è¯„ä¼°æ ‡å‡†çš„ä¸»è§‚æ€§"]
        )
        reasoning_chain.append(step5)
        
        return reasoning_chain
    
    async def _summarize_evidence(self, evidence_chain: List[Evidence]) -> str:
        """æ€»ç»“è¯æ®"""
        evidence_types = {}
        total_confidence = 0
        
        for evidence in evidence_chain:
            evidence_type = evidence.evidence_type.value
            if evidence_type not in evidence_types:
                evidence_types[evidence_type] = []
            evidence_types[evidence_type].append(evidence.confidence)
            total_confidence += evidence.confidence
        
        summary_parts = []
        for evidence_type, confidences in evidence_types.items():
            avg_confidence = sum(confidences) / len(confidences)
            summary_parts.append(f"{evidence_type}å¹³å‡ç½®ä¿¡åº¦{avg_confidence:.2f}")
        
        avg_total_confidence = total_confidence / len(evidence_chain) if evidence_chain else 0
        summary_parts.append(f"æ€»ä½“ç½®ä¿¡åº¦{avg_total_confidence:.2f}")
        
        return "ï¼Œ".join(summary_parts)
    
    async def _analyze_decision_impact(self, decision_type: DecisionType, 
                                     target: str, 
                                     evidence_chain: List[Evidence]) -> Dict[str, Any]:
        """åˆ†æå†³ç­–å½±å“"""
        impact_areas = {
            "functionality": "medium",
            "performance": "low",
            "security": "low",
            "maintainability": "medium",
            "user_experience": "low"
        }
        
        # åŸºäºå†³ç­–ç±»å‹è°ƒæ•´å½±å“
        if decision_type == DecisionType.FILE_REMOVAL:
            impact_areas["functionality"] = "high"
            impact_areas["dependency"] = "high"
        elif decision_type == DecisionType.SECURITY_FIX:
            impact_areas["security"] = "high"
        elif decision_type == DecisionType.PERFORMANCE_OPTIMIZE:
            impact_areas["performance"] = "high"
        
        overall_impact = "medium" if any(level == "high" for level in impact_areas.values()) else "low"
        
        return {
            "impact_areas": impact_areas,
            "overall_impact": overall_impact
        }
    
    async def _assess_decision_risks(self, decision_type: DecisionType, 
                                   target: str, 
                                   evidence_chain: List[Evidence]) -> Dict[str, Any]:
        """è¯„ä¼°å†³ç­–é£é™©"""
        risk_factors = []
        
        if decision_type == DecisionType.FILE_REMOVAL:
            risk_factors.extend([
                {"factor": "ä¾èµ–ç ´å", "probability": "medium", "impact": "high"},
                {"factor": "åŠŸèƒ½ä¸¢å¤±", "probability": "low", "impact": "high"},
                {"factor": "å›æ»šå›°éš¾", "probability": "medium", "impact": "medium"}
            ])
        elif decision_type == DecisionType.CODE_REFACTOR:
            risk_factors.extend([
                {"factor": "å¼•å…¥æ–°bug", "probability": "medium", "impact": "medium"},
                {"factor": "æ€§èƒ½å›å½’", "probability": "low", "impact": "medium"},
                {"factor": "å…¼å®¹æ€§é—®é¢˜", "probability": "low", "impact": "high"}
            ])
        
        # è®¡ç®—æ€»ä½“é£é™©ç­‰çº§
        high_impact_risks = [r for r in risk_factors if r["impact"] == "high"]
        medium_probability_risks = [r for r in risk_factors if r["probability"] == "medium"]
        
        if len(high_impact_risks) >= 2 or len(medium_probability_risks) >= 3:
            risk_level = "high"
            mitigation_strategy = "ä¸¥æ ¼çš„æµ‹è¯•å’Œåˆ†é˜¶æ®µå®æ–½"
        elif len(high_impact_risks) >= 1 or len(medium_probability_risks) >= 2:
            risk_level = "medium"
            mitigation_strategy = "å……åˆ†çš„æµ‹è¯•å’Œå›æ»šè®¡åˆ’"
        else:
            risk_level = "low"
            mitigation_strategy = "å¸¸è§„æµ‹è¯•å’Œç›‘æ§"
        
        return {
            "risk_factors": risk_factors,
            "risk_level": risk_level,
            "mitigation_strategy": mitigation_strategy
        }
    
    async def _generate_alternatives(self, decision_type: DecisionType, target: str) -> List[str]:
        """ç”Ÿæˆæ›¿ä»£æ–¹æ¡ˆ"""
        alternatives = []
        
        if decision_type == DecisionType.FILE_REMOVAL:
            alternatives = [
                "ä¿ç•™æ–‡ä»¶ä½†æ ‡è®°ä¸ºè¿‡æ—¶",
                "é‡æ„æ–‡ä»¶è€Œä¸æ˜¯åˆ é™¤",
                "ç§»åŠ¨æ–‡ä»¶åˆ°å­˜æ¡£ç›®å½•",
                "åˆå¹¶æ–‡ä»¶åŠŸèƒ½åˆ°å…¶ä»–æ¨¡å—"
            ]
        elif decision_type == DecisionType.CODE_REFACTOR:
            alternatives = [
                "ä¿æŒç°çŠ¶ï¼Œä»…æ·»åŠ æ³¨é‡Š",
                "éƒ¨åˆ†é‡æ„è€Œä¸æ˜¯å…¨é¢é‡å†™",
                "ä½¿ç”¨è®¾è®¡æ¨¡å¼é‡æ„",
                "åˆ†é˜¶æ®µé‡æ„"
            ]
        elif decision_type == DecisionType.FILE_RETENTION:
            alternatives = [
                "æ¡ä»¶æ€§ä¿ç•™ï¼ˆæ·»åŠ è­¦å‘Šï¼‰",
                "é™çº§ä½¿ç”¨è€Œä¸æ˜¯å®Œå…¨ä¿ç•™",
                "è¿ç§»åŠŸèƒ½åˆ°æ–°æ¨¡å—",
                "é‡æ„åä¿ç•™"
            ]
        
        return alternatives
    
    async def _perform_self_reflection(self, decision_type: DecisionType,
                                     target: str,
                                     reasoning_chain: List[ReasoningStep],
                                     evidence_chain: List[Evidence]) -> List[SelfReflection]:
        """æ‰§è¡Œè‡ªæˆ‘åçœ"""
        reflections = []
        
        # åçœ1: åè§è¯†åˆ«
        reflection1 = SelfReflection(
            reflection_type="bias_identification",
            question="æˆ‘åœ¨è¿™ä¸ªå†³ç­–ä¸­æ˜¯å¦å­˜åœ¨è®¤çŸ¥åè§ï¼Ÿ",
            analysis="åˆ†æå†³ç­–è¿‡ç¨‹ä¸­çš„æ½œåœ¨åè§ï¼ŒåŒ…æ‹¬ç¡®è®¤åè§ã€å¯å¾—æ€§åè§ç­‰",
            insights=[
                "å¯èƒ½å­˜åœ¨ç¡®è®¤åè§ï¼Œå€¾å‘äºæ”¯æŒåˆå§‹å‡è®¾",
                "å¯èƒ½å—åˆ°è¿‘æœŸäº‹ä»¶çš„å½±å“ï¼ˆå¯å¾—æ€§åè§ï¼‰",
                "å¯èƒ½è¿‡åº¦ä¾èµ–é‡åŒ–æŒ‡æ ‡è€Œå¿½ç•¥å®šæ€§å› ç´ "
            ],
            biases_identified=["ç¡®è®¤åè§", "å¯å¾—æ€§åè§", "é‡åŒ–åè§"],
            alternative_approaches=[
                "å¯»æ±‚åå¯¹æ„è§å’Œåé¢è¯æ®",
                "ä½¿ç”¨ä¸åŒçš„åˆ†ææ¡†æ¶é‡æ–°è¯„ä¼°",
                "å¼•å…¥å¤–éƒ¨ä¸“å®¶è¿›è¡Œç‹¬ç«‹è¯„ä¼°"
            ],
            confidence_adjustment=-0.05
        )
        reflections.append(reflection1)
        
        # åçœ2: è¯æ®å®Œæ•´æ€§
        reflection2 = SelfReflection(
            reflection_type="evidence_completeness",
            question="æ”¶é›†çš„è¯æ®æ˜¯å¦è¶³å¤Ÿå…¨é¢ï¼Ÿ",
            analysis="è¯„ä¼°è¯æ®é“¾çš„å®Œæ•´æ€§å’Œä»£è¡¨æ€§",
            insights=[
                "è¯æ®ä¸»è¦æ¥è‡ªé™æ€åˆ†æï¼Œç¼ºä¹è¿è¡Œæ—¶æ•°æ®",
                "ç”¨æˆ·åé¦ˆè¯æ®ä¸è¶³",
                "é•¿æœŸå½±å“è¯æ®æœ‰é™"
            ],
            biases_identified=["é€‰æ‹©åè§", "æµ‹é‡åè§"],
            alternative_approaches=[
                "æ”¶é›†æ›´å¤šè¿è¡Œæ—¶æ€§èƒ½æ•°æ®",
                "è¿›è¡Œç”¨æˆ·è°ƒç ”å’Œåé¦ˆæ”¶é›†",
                "åˆ†æå†å²æ•°æ®å’Œè¶‹åŠ¿"
            ],
            confidence_adjustment=-0.1
        )
        reflections.append(reflection2)
        
        # åçœ3: æ¨ç†é€»è¾‘
        reflection3 = SelfReflection(
            reflection_type="reasoning_logic",
            question="æ¨ç†è¿‡ç¨‹æ˜¯å¦å­˜åœ¨é€»è¾‘æ¼æ´ï¼Ÿ",
            analysis="æ£€æŸ¥æ¨ç†é“¾çš„é€»è¾‘ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§",
            insights=[
                "æ¨ç†æ­¥éª¤ä¹‹é—´çš„å…³è”æ€§è¾ƒå¼º",
                "æŸäº›å‡è®¾å¯èƒ½ç¼ºä¹å……åˆ†éªŒè¯",
                "ç»“è®ºçš„æ¨å¯¼è¿‡ç¨‹åŸºæœ¬åˆç†"
            ],
            biases_identified=["é€»è¾‘è·³è·ƒ", "è¿‡åº¦æ¦‚æ‹¬"],
            alternative_approaches=[
                "åŠ å¼ºå‡è®¾éªŒè¯",
                "ç»†åŒ–æ¨ç†æ­¥éª¤",
                "ä½¿ç”¨é€»è¾‘æ¡†æ¶æ£€æŸ¥"
            ],
            confidence_adjustment=-0.03
        )
        reflections.append(reflection3)
        
        return reflections
    
    async def _calculate_confidence_score(self, evidence_chain: List[Evidence],
                                        reasoning_chain: List[ReasoningStep],
                                        self_reflections: List[SelfReflection]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        # åŸºç¡€ç½®ä¿¡åº¦æ¥è‡ªè¯æ®
        evidence_confidence = sum(e.confidence for e in evidence_chain) / len(evidence_chain) if evidence_chain else 0
        
        # æ¨ç†ç½®ä¿¡åº¦
        reasoning_confidence = sum(r.confidence for r in reasoning_chain) / len(reasoning_chain) if reasoning_chain else 0
        
        # è‡ªæˆ‘åçœè°ƒæ•´
        reflection_adjustment = sum(r.confidence_adjustment for r in self_reflections)
        
        # ç»¼åˆè®¡ç®—
        base_confidence = (evidence_confidence * 0.5 + reasoning_confidence * 0.3 + 0.2)
        final_confidence = max(0, min(1, base_confidence + reflection_adjustment))
        
        return final_confidence
    
    async def _assess_risks(self, decision_type: DecisionType, 
                          target: str, 
                          evidence_chain: List[Evidence]) -> str:
        """è¯„ä¼°é£é™©"""
        security_evidence = [e for e in evidence_chain if e.evidence_type == EvidenceType.SECURITY_SCAN]
        dependency_evidence = [e for e in evidence_chain if e.evidence_type == EvidenceType.DEPENDENCY_GRAPH]
        
        if decision_type == DecisionType.FILE_REMOVAL:
            if len(dependency_evidence) > 0:
                dep_data = dependency_evidence[0].supporting_data
                if dep_data.get("critical_dependencies", 0) > 0:
                    return "high"
            
            if len(security_evidence) > 0:
                sec_data = security_evidence[0].supporting_data
                if sec_data.get("high_risk_issues", 0) > 0:
                    return "medium"
        
        return "low"
    
    async def _analyze_impact(self, decision_type: DecisionType,
                            target: str,
                            evidence_chain: List[Evidence],
                            analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå½±å“"""
        impact = {
            "functional": "low",
            "performance": "low",
            "security": "low",
            "maintainability": "low",
            "user_experience": "low"
        }
        
        if decision_type == DecisionType.FILE_REMOVAL:
            impact["functional"] = "medium"
            impact["maintainability"] = "medium"
        
        elif decision_type == DecisionType.SECURITY_FIX:
            impact["security"] = "high"
        
        elif decision_type == DecisionType.PERFORMANCE_OPTIMIZE:
            impact["performance"] = "high"
        
        return impact
    
    async def _consider_alternatives(self, decision_type: DecisionType,
                                   target: str,
                                   analysis_data: Dict[str, Any]) -> List[str]:
        """è€ƒè™‘æ›¿ä»£æ–¹æ¡ˆ"""
        alternatives = []
        
        if decision_type == DecisionType.FILE_REMOVAL:
            alternatives = [
                "é‡æ„è€Œä¸æ˜¯åˆ é™¤",
                "ç§»åŠ¨åˆ°å­˜æ¡£ç›®å½•",
                "æ ‡è®°ä¸ºè¿‡æ—¶ä½†ä¿ç•™"
            ]
        elif decision_type == DecisionType.FILE_RETENTION:
            alternatives = [
                "æ¡ä»¶æ€§ä¿ç•™",
                "é™çº§ä½¿ç”¨",
                "è¿ç§»åŠŸèƒ½"
            ]
        
        return alternatives
    
    async def _generate_final_decision(self, decision_type: DecisionType,
                                     target: str,
                                     reasoning_chain: List[ReasoningStep],
                                     confidence_score: float) -> str:
        """ç”Ÿæˆæœ€ç»ˆå†³ç­–"""
        if confidence_score > 0.8:
            if decision_type == DecisionType.FILE_REMOVAL:
                return "åˆ é™¤æ–‡ä»¶"
            elif decision_type == DecisionType.FILE_RETENTION:
                return "ä¿ç•™æ–‡ä»¶"
            elif decision_type == DecisionType.CODE_REFACTOR:
                return "æ‰§è¡Œé‡æ„"
            else:
                return "æ‰§è¡Œå†³ç­–"
        elif confidence_score > 0.6:
            if decision_type == DecisionType.FILE_REMOVAL:
                return "è°¨æ…åˆ é™¤ï¼ˆéœ€é¢å¤–éªŒè¯ï¼‰"
            elif decision_type == DecisionType.FILE_RETENTION:
                return "æœ‰æ¡ä»¶ä¿ç•™"
            else:
                return "æœ‰æ¡ä»¶æ‰§è¡Œ"
        else:
            return "éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œæš‚ä¸å†³ç­–"
    
    async def _generate_final_justification(self, decision_type: DecisionType,
                                          target: str,
                                          final_decision: str,
                                          evidence_chain: List[Evidence],
                                          reasoning_chain: List[ReasoningStep],
                                          self_reflections: List[SelfReflection]) -> str:
        """ç”Ÿæˆæœ€ç»ˆä¾æ®"""
        justification_parts = []
        
        # å†³ç­–æ¦‚è¿°
        justification_parts.append(f"## å†³ç­–æ¦‚è¿°")
        justification_parts.append(f"é’ˆå¯¹ç›®æ ‡ '{target}' çš„ {decision_type.value} å†³ç­–ï¼Œæœ€ç»ˆå†³å®šï¼š{final_decision}")
        justification_parts.append("")
        
        # ä¸»è¦ä¾æ®
        justification_parts.append(f"## ä¸»è¦ä¾æ®")
        for evidence in evidence_chain:
            if evidence.confidence > 0.7:
                justification_parts.append(f"- {evidence.content} (ç½®ä¿¡åº¦: {evidence.confidence:.2f})")
        justification_parts.append("")
        
        # æ¨ç†è¿‡ç¨‹
        justification_parts.append(f"## æ¨ç†è¿‡ç¨‹")
        for step in reasoning_chain:
            justification_parts.append(f"### æ­¥éª¤{step.step_number}: {step.description}")
            justification_parts.append(f"æ¨ç†: {step.reasoning_process}")
            justification_parts.append(f"ç»“è®º: {step.conclusion}")
            justification_parts.append("")
        
        # è‡ªæˆ‘åçœ
        justification_parts.append(f"## è‡ªæˆ‘åçœ")
        for reflection in self_reflections:
            justification_parts.append(f"### {reflection.reflection_type}")
            justification_parts.append(f"é—®é¢˜: {reflection.question}")
            justification_parts.append(f"åˆ†æ: {reflection.analysis}")
            justification_parts.append(f"æ´å¯Ÿ: {', '.join(reflection.insights)}")
            justification_parts.append("")
        
        # é£é™©è¯´æ˜
        justification_parts.append(f"## é£é™©è¯´æ˜")
        justification_parts.append("æœ¬å†³ç­–å·²è€ƒè™‘æ½œåœ¨é£é™©ï¼Œå¹¶åˆ¶å®šäº†ç›¸åº”çš„ç¼“è§£ç­–ç•¥ã€‚")
        justification_parts.append("")
        
        # ç»“è®º
        justification_parts.append(f"## ç»“è®º")
        justification_parts.append(f"åŸºäºå…¨é¢çš„è¯æ®æ”¶é›†ã€ä¸¥è°¨çš„æ¨ç†è¿‡ç¨‹å’Œæ·±å…¥çš„è‡ªæˆ‘åçœï¼Œ")
        justification_parts.append(f"æˆ‘ä»¬è®¤ä¸º{final_decision}æ˜¯å½“å‰æœ€ä¼˜å†³ç­–ã€‚")
        
        return "\n".join(justification_parts)
    
    async def _save_decision_record(self, decision_record: DecisionRecord):
        """ä¿å­˜å†³ç­–è®°å½•"""
        # åˆ›å»ºå†³ç­–è®°å½•ç›®å½•
        decisions_dir = self.project_root / ".iflow" / "temp_docs" / "decisions"
        decisions_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºJSON
        json_file = decisions_dir / f"{decision_record.decision_id}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(decision_record), f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ä¸ºMarkdown
        md_file = decisions_dir / f"{decision_record.decision_id}.md"
        md_content = await self._generate_markdown_report(decision_record)
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        print(f"ğŸ’¾ å†³ç­–è®°å½•å·²ä¿å­˜: {json_file}")
        print(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {md_file}")
    
    async def _generate_markdown_report(self, decision_record: DecisionRecord) -> str:
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        content = []
        
        content.append(f"# å†³ç­–è®°å½•: {decision_record.decision_id}")
        content.append(f"**å†³ç­–ç±»å‹**: {decision_record.decision_type.value}")
        content.append(f"**ç›®æ ‡**: {decision_record.target}")
        content.append(f"**å†³ç­–**: {decision_record.decision}")
        content.append(f"**ç½®ä¿¡åº¦**: {decision_record.confidence_score:.2f}")
        content.append(f"**æ—¶é—´**: {decision_record.timestamp}")
        content.append("")
        
        # æœ€ç»ˆä¾æ®
        content.append("## æœ€ç»ˆä¾æ®")
        content.append(decision_record.final_justification)
        content.append("")
        
        # è¯æ®é“¾
        content.append("## è¯æ®é“¾")
        for evidence in decision_record.evidence_chain:
            content.append(f"### {evidence.evidence_type.value}")
            content.append(f"- **æ¥æº**: {evidence.source}")
            content.append(f"- **å†…å®¹**: {evidence.content}")
            content.append(f"- **ç½®ä¿¡åº¦**: {evidence.confidence:.2f}")
            content.append("")
        
        return "\n".join(content)
    
    def _load_reasoning_templates(self) -> Dict[str, Any]:
        """åŠ è½½æ¨ç†æ¨¡æ¿"""
        return {
            "problem_definition": "åŸºäº{target}å’Œ{decision_type}ï¼Œæ˜ç¡®éœ€è¦è§£å†³çš„æ ¸å¿ƒé—®é¢˜",
            "evidence_evaluation": "åˆ†æ{evidence_count}ä¸ªè¯æ®çš„å¯é æ€§å’Œç›¸å…³æ€§",
            "impact_analysis": "åŸºäºè¯æ®åˆ†æå†³ç­–å¯¹ç³»ç»Ÿçš„å¤šæ–¹é¢å½±å“",
            "risk_assessment": "è¯†åˆ«å’Œè¯„ä¼°å†³ç­–çš„ä¸»è¦é£é™©å› ç´ ",
            "alternative_comparison": "åˆ†ææ›¿ä»£æ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹ï¼Œé€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ"
        }
    
    def _load_justification_principles(self) -> List[str]:
        """åŠ è½½ä¾æ®åŸåˆ™"""
        return [
            "æ¯ä¸ªå†³ç­–éƒ½å¿…é¡»æœ‰å……åˆ†çš„è¯æ®æ”¯æŒ",
            "æ¨ç†è¿‡ç¨‹å¿…é¡»é€»è¾‘æ¸…æ™°ã€æ­¥éª¤æ˜ç¡®",
            "å¿…é¡»è€ƒè™‘æ›¿ä»£æ–¹æ¡ˆå¹¶è¿›è¡Œæ¯”è¾ƒ",
            "å¿…é¡»è¯†åˆ«å’Œè¯„ä¼°æ½œåœ¨é£é™©",
            "å¿…é¡»è¿›è¡Œè‡ªæˆ‘åçœï¼Œè¯†åˆ«è®¤çŸ¥åè§",
            "å¿…é¡»æä¾›è¯¦ç»†çš„è§£é‡Šå’Œä¾æ®"
        ]

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°"""
    project_root = "."
    
    justification_system = ComprehensiveJustificationSystem(project_root)
    
    # ç¤ºä¾‹ï¼šåˆ›å»ºæ–‡ä»¶åˆ é™¤å†³ç­–
    analysis_data = {
        "metrics": {"complexity": 15, "maintainability": 45},
        "dependencies": ["module_a", "module_b"],
        "security_issues": [{"severity": "medium", "description": "æ½œåœ¨å®‰å…¨é—®é¢˜"}],
        "performance_metrics": {"execution_time": 150, "memory_usage": 50}
    }
    
    decision_record = await justification_system.create_comprehensive_decision(
        DecisionType.FILE_REMOVAL,
        "example_module.py",
        analysis_data
    )
    
    print(f"ğŸ‰ å†³ç­–è®°å½•åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ“Š å†³ç­–ID: {decision_record.decision_id}")
    print(f"ğŸ¯ å†³ç­–: {decision_record.decision}")
    print(f"ğŸ“ˆ ç½®ä¿¡åº¦: {decision_record.confidence_score:.2f}")

if __name__ == "__main__":
    asyncio.run(main())
