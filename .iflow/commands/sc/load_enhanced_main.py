#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆ /sc:load æŒ‡ä»¤å…¥å£æ–‡ä»¶
æ•´åˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼Œæä¾›æ™ºèƒ½çš„é¡¹ç›®ä¸Šä¸‹æ–‡åŠ è½½æœåŠ¡
"""

import os
import sys
import json
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# å¯¼å…¥æ‰€æœ‰åŠŸèƒ½æ¨¡å—
from ai_information_forcer import AIInformationForcer
from structure_analyzer import ProjectStructureAnalyzer
from optimization_report_generator import OptimizationReportGenerator
from deep_analysis_scanner import DeepAnalysisScanner
from comprehensive_justification_system import ComprehensiveJustificationSystem, DecisionType
from feature_analysis_module import FeatureAnalysisModule

class EnhancedSCLoadCommand:
    """å¢å¼ºç‰ˆ /sc:load å‘½ä»¤"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.results_dir = self.project_root / "reports"
        self.results_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æ‰€æœ‰æ¨¡å—
        self.ai_forcer = AIInformationForcer(project_root)
        self.structure_analyzer = ProjectStructureAnalyzer(project_root)
        self.optimization_generator = OptimizationReportGenerator(project_root)
        self.deep_scanner = DeepAnalysisScanner(project_root)
        self.justification_system = ComprehensiveJustificationSystem(project_root)
        self.feature_analyzer = FeatureAnalysisModule(project_root)
        
    async def execute_enhanced_load(self, 
                                   load_type: str = "project",
                                   refresh: bool = False,
                                   analyze: bool = False,
                                   deep_analysis: bool = False,
                                   checkpoint: Optional[str] = None,
                                   interactive_mode: bool = True,
                                   force_ai_awareness: bool = True) -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºç‰ˆé¡¹ç›®ä¸Šä¸‹æ–‡åŠ è½½"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆ /sc:load é¡¹ç›®ä¸Šä¸‹æ–‡åŠ è½½ç³»ç»Ÿ")
        print("=" * 80)
        
        # 1. å¼ºåˆ¶AIä¿¡æ¯ä¼ é€’
        if force_ai_awareness:
            print("\nğŸ¤– ç¬¬ä¸€æ­¥ï¼šå¼ºåˆ¶AIä¿¡æ¯ä¼ é€’")
            ai_context = await self.ai_forcer.force_ai_awareness()
            print("âœ… AIä¿¡æ¯ä¼ é€’å®Œæˆ")
        
        # 2. é¡¹ç›®ç»“æ„æ·±åº¦åˆ†æ
        print("\nğŸ”„ ç¬¬äºŒæ­¥ï¼šé¡¹ç›®ç»“æ„æ·±åº¦åˆ†æ")
        structure_analysis = await self.structure_analyzer.analyze_and_compare()
        print("âœ… é¡¹ç›®ç»“æ„åˆ†æå®Œæˆ")
        
        # 3. æ·±åº¦åˆ†ææ‰«æï¼ˆå¯é€‰ï¼‰
        deep_scan_results = None
        if deep_analysis:
            print("\nğŸ”¬ ç¬¬ä¸‰æ­¥ï¼šæ·±åº¦åˆ†ææ‰«æå®¡æŸ¥")
            deep_scan_results = await self.deep_scanner.perform_comprehensive_scan()
            print("âœ… æ·±åº¦æ‰«æå®¡æŸ¥å®Œæˆ")
        
        # 4. åŠŸèƒ½ç‰¹ç‚¹åˆ†æï¼ˆå¯é€‰ï¼‰
        feature_analyses = None
        if deep_scan_results and analyze:
            print("\nğŸ¯ ç¬¬å››æ­¥ï¼šåŠŸèƒ½ç‰¹ç‚¹åˆ†æ")
            feature_analyses = await self._perform_feature_analyses(deep_scan_results)
            print("âœ… åŠŸèƒ½ç‰¹ç‚¹åˆ†æå®Œæˆ")
        
        # 5. å†å²å†³ç­–è®°å½•æ¢å¤
        decision_records = None
        if feature_analyses:
            print("\nâš–ï¸ ç¬¬äº”æ­¥ï¼šå†å²å†³ç­–è®°å½•æ¢å¤")
            decision_records = await self._recover_decision_records()
            print("âœ… å†³ç­–è®°å½•æ¢å¤å®Œæˆ")
        
        # 6. é¡¹ç›®ä¸Šä¸‹æ–‡å»ºç«‹
        print("\nğŸ—ï¸ ç¬¬å…­æ­¥ï¼šé¡¹ç›®ä¸Šä¸‹æ–‡å»ºç«‹")
        project_context = await self._build_project_context(
            structure_analysis, deep_scan_results, feature_analyses, decision_records
        )
        print("âœ… é¡¹ç›®ä¸Šä¸‹æ–‡å»ºç«‹å®Œæˆ")
        
        # 7. æ™ºèƒ½ä¸Šä¸‹æ–‡éªŒè¯
        print("\nğŸ” ç¬¬ä¸ƒæ­¥ï¼šæ™ºèƒ½ä¸Šä¸‹æ–‡éªŒè¯")
        context_validation = await self._validate_project_context(project_context)
        print("âœ… ä¸Šä¸‹æ–‡éªŒè¯å®Œæˆ")
        
        # 8. å˜åŒ–æ£€æµ‹åˆ†æ
        if refresh:
            print("\nğŸ“ˆ ç¬¬å…«æ­¥ï¼šå˜åŒ–æ£€æµ‹åˆ†æ")
            change_analysis = await self._detect_changes(structure_analysis)
            print("âœ… å˜åŒ–æ£€æµ‹åˆ†æå®Œæˆ")
        
        # 9. ä¼˜åŒ–å»ºè®®ç”Ÿæˆ
        print("\nğŸ’¡ ç¬¬ä¹æ­¥ï¼šä¼˜åŒ–å»ºè®®ç”Ÿæˆ")
        optimization_recommendations = await self._generate_optimization_recommendations(
            structure_analysis, deep_scan_results, feature_analyses
        )
        print("âœ… ä¼˜åŒ–å»ºè®®ç”Ÿæˆå®Œæˆ")
        
        # 10. ä¼šè¯å°±ç»ªç¡®è®¤
        print("\nâœ… ç¬¬åæ­¥ï¼šä¼šè¯å°±ç»ªç¡®è®¤")
        session_readiness = await self._confirm_session_readiness(project_context)
        print("âœ… ä¼šè¯å°±ç»ªç¡®è®¤å®Œæˆ")
        
        # 11. äº¤äº’å¼å¤„ç†
        if interactive_mode:
            print("\nğŸ® ç¬¬åä¸€æ­¥ï¼šäº¤äº’å¼å¤„ç†")
            await self._interactive_load_analysis(
                structure_analysis, deep_scan_results, feature_analyses, 
                decision_records, optimization_recommendations
            )
        
        # 12. ç”Ÿæˆæœ€ç»ˆåŠ è½½æŠ¥å‘Š
        print("\nğŸ“‹ ç¬¬åäºŒæ­¥ï¼šç”Ÿæˆæœ€ç»ˆåŠ è½½æŠ¥å‘Š")
        final_report = await self._generate_final_load_report(
            ai_context if force_ai_awareness else None,
            structure_analysis,
            deep_scan_results,
            feature_analyses,
            decision_records,
            project_context,
            context_validation,
            change_analysis if refresh else None,
            optimization_recommendations,
            session_readiness
        )
        
        print("\nğŸ‰ å¢å¼ºç‰ˆ /sc:load é¡¹ç›®ä¸Šä¸‹æ–‡åŠ è½½å®Œæˆï¼")
        return final_report
    
    async def _perform_feature_analyses(self, deep_scan_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒåŠŸèƒ½åˆ†æ"""
        feature_analyses = []
        
        # è·å–æ‰€æœ‰Pythonæ–‡ä»¶
        python_files = []
        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules']]
            for file in files:
                if file.endswith('.py'):
                    file_path = Path(root) / file
                    rel_path = str(file_path.relative_to(self.project_root))
                    python_files.append(rel_path)
        
        # åˆ†ææ¯ä¸ªæ–‡ä»¶çš„åŠŸèƒ½ç‰¹ç‚¹
        for file_path in python_files:
            try:
                analysis = await self.feature_analyzer.analyze_comprehensive_features(file_path)
                feature_analyses.append(analysis)
            except Exception as e:
                print(f"âš ï¸ åŠŸèƒ½åˆ†æå¤±è´¥ {file_path}: {e}")
        
        return feature_analyses
    
    async def _recover_decision_records(self) -> List[Dict[str, Any]]:
        """æ¢å¤å†³ç­–è®°å½•"""
        decision_records = []
        
        # ä»å†³ç­–è®°å½•ç›®å½•æ¢å¤
        decisions_dir = self.project_root / ".iflow" / "temp_docs" / "decisions"
        if decisions_dir.exists():
            for file_path in decisions_dir.glob("*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        decision_record = json.load(f)
                    decision_records.append(decision_record)
                except Exception as e:
                    print(f"âš ï¸ å†³ç­–è®°å½•æ¢å¤å¤±è´¥ {file_path}: {e}")
        
        return decision_records
    
    async def _build_project_context(self, 
                                  structure_analysis: Any,
                                  deep_scan_results: Optional[Dict[str, Any]],
                                  feature_analyses: Optional[List[Dict[str, Any]]],
                                  decision_records: Optional[List[Dict[str, Any]]]) -> Dict[str, Any]:
        """å»ºç«‹é¡¹ç›®ä¸Šä¸‹æ–‡"""
        project_context = {
            "timestamp": datetime.now().isoformat(),
            "project_root": str(self.project_root),
            "project_name": "iFlow CLI V16 Quantum Evolution",
            "load_timestamp": datetime.now().isoformat(),
            "structure_analysis": asdict(structure_analysis) if structure_analysis else None,
            "deep_scan_results": deep_scan_results,
            "feature_analyses": [asdict(f) for f in feature_analyses] if feature_analyses else [],
            "decision_records": decision_records,
            "project_status": "loaded",
            "context_version": "2.0.0",
            "ai_awareness": True,
            "memory_integration": True
        }
        
        return project_context
    
    async def _validate_project_context(self, project_context: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯é¡¹ç›®ä¸Šä¸‹æ–‡"""
        validation_results = {
            "timestamp": datetime.now().isoformat(),
            "validation_status": "passed",
            "validation_checks": [],
            "issues_found": [],
            "recommendations": []
        }
        
        # éªŒè¯é¡¹ç›®ç»“æ„
        if project_context.get("structure_analysis"):
            structure = project_context["structure_analysis"]
            if structure.get("structure_changes", {}).get("files_added_count", 0) > 0:
                validation_results["validation_checks"].append("æ£€æµ‹åˆ°æ–°å¢æ–‡ä»¶")
                validation_results["recommendations"].append("è€ƒè™‘åˆ†ææ–°å¢æ–‡ä»¶çš„å½±å“")
        
        # éªŒè¯æ·±åº¦æ‰«æç»“æœ
        if project_context.get("deep_scan_results"):
            scan = project_context["deep_scan_results"]
            issues_count = scan.get("scan_summary", {}).get("scan_overview", {}).get("total_issues", 0)
            if issues_count > 0:
                validation_results["validation_checks"].append(f"å‘ç°{issues_count}ä¸ªé—®é¢˜")
                validation_results["recommendations"].append("ä¼˜å…ˆå¤„ç†é«˜ä¸¥é‡æ€§é—®é¢˜")
        
        # éªŒè¯åŠŸèƒ½åˆ†æ
        if project_context.get("feature_analyses"):
            analyses = project_context["feature_analyses"]
            high_value_files = len([f for f in analyses if hasattr(f, 'recommendation') and "ä¿ç•™" in f.recommendation])
            validation_results["validation_checks"].append(f"åˆ†æäº†{len(analyses)}ä¸ªæ–‡ä»¶ï¼Œ{high_value_files}ä¸ªé«˜ä»·å€¼")
        
        # éªŒè¯å†³ç­–è®°å½•
        if project_context.get("decision_records"):
            records = project_context["decision_records"]
            high_confidence_records = len([r for r in records if r.get('confidence_score', 0) > 0.8])
            validation_results["validation_checks"].append(f"æ¢å¤äº†{len(records)}ä¸ªå†³ç­–è®°å½•ï¼Œ{high_confidence_records}ä¸ªé«˜ç½®ä¿¡åº¦")
        
        return validation_results
    
    async def _detect_changes(self, structure_analysis: Any) -> Dict[str, Any]:
        """æ£€æµ‹å˜åŒ–"""
        change_analysis = {
            "timestamp": datetime.now().isoformat(),
            "changes_detected": False,
            "change_summary": {},
            "impact_assessment": {},
            "recommendations": []
        }
        
        if structure_analysis:
            changes = structure_analysis.structure_changes
            change_analysis["changes_detected"] = True
            change_analysis["change_summary"] = {
                "files_added": changes.get("files_added_count", 0),
                "files_removed": changes.get("files_removed_count", 0),
                "files_modified": changes.get("files_modified_count", 0)
            }
            
            # å½±å“è¯„ä¼°
            impact = structure_analysis.impact_analysis
            change_analysis["impact_assessment"] = {
                "functional_impact": impact.get("functional_impact", {}).get("level", "low"),
                "performance_impact": impact.get("performance_impact", {}).get("level", "low"),
                "security_impact": impact.get("security_impact", {}).get("level", "low"),
                "overall_risk": impact.get("overall_risk", "low")
            }
            
            # å»ºè®®
            if changes.get("files_added_count", 0) > 0:
                change_analysis["recommendations"].append("åˆ†ææ–°å¢æ–‡ä»¶çš„åŠŸèƒ½å’Œå½±å“")
            if changes.get("files_removed_count", 0) > 0:
                change_analysis["recommendations"].append("éªŒè¯åˆ é™¤æ–‡ä»¶çš„å½±å“")
            if changes.get("files_modified_count", 0) > 0:
                change_analysis["recommendations"].append("æ£€æŸ¥ä¿®æ”¹æ–‡ä»¶çš„å…¼å®¹æ€§")
        
        return change_analysis
    
    async def _generate_optimization_recommendations(self, 
                                            structure_analysis: Any,
                                            deep_scan_results: Optional[Dict[str, Any]],
                                            feature_analyses: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºç»“æ„åˆ†æçš„å»ºè®®
        if structure_analysis:
            structure_changes = structure_analysis.structure_changes
            if structure_changes.get("files_added_count", 0) > 5:
                recommendations.append({
                    "category": "structure",
                    "priority": "medium",
                    "description": "æ–°å¢æ–‡ä»¶è¾ƒå¤šï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦æœ‰å†—ä½™",
                    "action": "review_new_files",
                    "impact": "medium"
                })
            
            if structure_changes.get("files_removed_count", 0) > 3:
                recommendations.append({
                    "category": "structure",
                    "priority": "high",
                    "description": "åˆ é™¤æ–‡ä»¶è¾ƒå¤šï¼Œå»ºè®®ç¡®è®¤å½±å“",
                    "action": "verify_deletion_impact",
                    "impact": "high"
                })
        
        # åŸºäºæ·±åº¦æ‰«æçš„å»ºè®®
        if deep_scan_results:
            summary = deep_scan_results.get("scan_summary", {})
            critical_issues = summary.get("scan_overview", {}).get("critical_issues", 0)
            high_issues = summary.get("scan_overview", {}).get("high_issues", 0)
            
            if critical_issues > 0:
                recommendations.append({
                    "category": "security",
                    "priority": "critical",
                    "description": f"å‘ç°{critical_issues}ä¸ªå…³é”®å®‰å…¨é—®é¢˜",
                    "action": "fix_security_issues",
                    "impact": "critical"
                })
            
            if high_issues > 0:
                recommendations.append({
                    "category": "quality",
                    "priority": "high",
                    "description": f"å‘ç°{high_issues}ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜",
                    "action": "address_quality_issues",
                    "impact": "high"
                })
        
        # åŸºäºåŠŸèƒ½åˆ†æçš„å»ºè®®
        if feature_analyses:
            total_files = len(feature_analyses)
            low_value_files = len([f for f in feature_analyses if hasattr(f, 'recommendation') and "åˆ é™¤" in f.recommendation])
            
            if low_value_files > total_files * 0.3:
                recommendations.append({
                    "category": "optimization",
                    "priority": "medium",
                    "description": f"{low_value_files}ä¸ªæ–‡ä»¶ä»·å€¼è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–æˆ–åˆ é™¤",
                    "action": "optimize_low_value_files",
                    "impact": "medium"
                })
        
        return recommendations
    
    async def _confirm_session_readiness(self, project_context: Dict[str, Any]) -> Dict[str, Any]:
        """ç¡®è®¤ä¼šè¯å°±ç»ª"""
        readiness_status = {
            "timestamp": datetime.now().isoformat(),
            "ready_status": "ready",
            "readiness_checks": [],
            "issues": [],
            "overall_score": 1.0
        }
        
        # æ£€æŸ¥é¡¹ç›®çŠ¶æ€
        if project_context.get("project_status") == "loaded":
            readiness_status["readiness_checks"].append("é¡¹ç›®ä¸Šä¸‹æ–‡å·²åŠ è½½")
            readiness_status["readiness_checks"].append("AIä¿¡æ¯å·²ä¼ é€’")
            readiness_status["readiness_checks"].append("è®°å¿†é›†æˆå·²å»ºç«‹")
        
        # æ£€æŸ¥ç»„ä»¶çŠ¶æ€
        components = [
            ("AIä¿¡æ¯å¼ºåˆ¶ä¼ é€’å™¨", "ai_forcer" in project_context.get("ai_awareness", False)),
            ("é¡¹ç›®ç»“æ„åˆ†æå™¨", project_context.get("structure_analysis") is not None),
            ("æ·±åº¦æ‰«æå™¨", project_context.get("deep_scan_results") is not None),
            ("åŠŸèƒ½åˆ†æå™¨", project_context.get("feature_analyses") is not None),
            ("å†³ç­–ç³»ç»Ÿ", project_context.get("decision_records") is not None)
        ]
        
        for component_name, is_active in components:
            if is_active:
                readiness_status["readiness_checks"].append(f"{component_name}å·²æ¿€æ´»")
            else:
                readiness_status["issues"].append(f"{component_name}æœªæ¿€æ´»")
        
        # è®¡ç®—æ•´ä½“å°±ç»ªåˆ†æ•°
        active_components = sum(1 for _, is_active in components if is_active)
        total_components = len(components)
        readiness_status["overall_score"] = active_components / total_components if total_components > 0 else 0
        
        if readiness_status["overall_score"] < 0.8:
            readiness_status["ready_status"] = "partial"
        elif readiness_status["overall_score"] < 1.0:
            readiness_status["ready_status"] = "almost_ready"
        
        return readiness_status
    
    async def _interactive_load_analysis(self, 
                                      structure_analysis: Any,
                                      deep_scan_results: Optional[Dict[str, Any]],
                                      feature_analyses: Optional[List[Dict[str, Any]]],
                                      decision_records: Optional[List[Dict[str, Any]]],
                                      optimization_recommendations: List[Dict[str, Any]]):
        """äº¤äº’å¼åŠ è½½åˆ†æ"""
        print("\nğŸ® è¿›å…¥äº¤äº’å¼åŠ è½½åˆ†ææ¨¡å¼")
        print("=" * 50)
        
        while True:
            print("\nå¯ç”¨çš„äº¤äº’æ“ä½œ:")
            print("1. æŸ¥çœ‹é¡¹ç›®ç»“æ„åˆ†æ")
            print("2. æŸ¥çœ‹æ·±åº¦æ‰«æç»“æœ")
            print("3. æŸ¥çœ‹åŠŸèƒ½ç‰¹ç‚¹åˆ†æ")
            print("4. æŸ¥çœ‹å†³ç­–è®°å½•")
            print("5. æŸ¥çœ‹ä¼˜åŒ–å»ºè®®")
            print("6. æŸ¥çœ‹é¡¹ç›®ä¸Šä¸‹æ–‡")
            print("7. æŸ¥çœ‹ä¼šè¯å°±ç»ªçŠ¶æ€")
            print("8. å¯¼å‡ºè¯¦ç»†æŠ¥å‘Š")
            print("9. é‡æ–°åˆ†æç‰¹å®šæ–‡ä»¶")
            print("0. é€€å‡ºäº¤äº’æ¨¡å¼")
            
            try:
                choice = input("\nè¯·é€‰æ‹©æ“ä½œ (0-9): ").strip()
                
                if choice == "0":
                    break
                elif choice == "1":
                    await self._show_structure_analysis(structure_analysis)
                elif choice == "2":
                    await self._show_deep_scan_results(deep_scan_results)
                elif choice == "3":
                    await self._show_feature_analyses(feature_analyses)
                elif choice == "4":
                    await self._show_decision_records(decision_records)
                elif choice == "5":
                    await self._show_optimization_recommendations(optimization_recommendations)
                elif choice == "6":
                    await self._show_project_context()
                elif choice == "7":
                    await self._show_session_readiness()
                elif choice == "8":
                    await self._export_detailed_reports()
                elif choice == "9":
                    await self._reanalyze_specific_file()
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
            
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºäº¤äº’æ¨¡å¼")
                break
            except Exception as e:
                print(f"âŒ æ“ä½œå‡ºé”™: {e}")
    
    async def _show_structure_analysis(self, structure_analysis: Any):
        """æ˜¾ç¤ºé¡¹ç›®ç»“æ„åˆ†æ"""
        if not structure_analysis:
            print("âŒ æ— é¡¹ç›®ç»“æ„åˆ†æç»“æœ")
            return
        
        print("\nğŸ”„ é¡¹ç›®ç»“æ„åˆ†æç»“æœ")
        print("-" * 40)
        
        changes = structure_analysis.structure_changes
        print(f"ğŸ“Š å˜åŒ–ç»Ÿè®¡:")
        print(f"  - æ–°å¢æ–‡ä»¶: {changes.get('files_added_count', 0)}")
        print(f"  - åˆ é™¤æ–‡ä»¶: {changes.get('files_removed_count', 0)}")
        print(f"  - ä¿®æ”¹æ–‡ä»¶: {changes.get('files_modified_count', 0)}")
        
        impact = structure_analysis.impact_analysis
        print(f"\nğŸ“ˆ å½±å“åˆ†æ:")
        print(f"  - åŠŸèƒ½å½±å“: {impact.get('functional_impact', {}).get('level', 'N/A')}")
        print(f"  - æ€§èƒ½å½±å“: {impact.get('performance_impact', {}).get('level', 'N/A')}")
        print(f"  - å®‰å…¨å½±å“: {impact.get('security_impact', {}).get('level', 'N/A')}")
        print(f"  - æ•´ä½“é£é™©: {impact.get('overall_risk', 'N/A')}")
        
        if structure_analysis.recommendations:
            print(f"\nğŸ’¡ å»ºè®®:")
            for i, rec in enumerate(structure_analysis.recommendations[:5], 1):
                print(f"  {i}. {rec}")
    
    async def _show_deep_scan_results(self, deep_scan_results: Optional[Dict[str, Any]]):
        """æ˜¾ç¤ºæ·±åº¦æ‰«æç»“æœ"""
        if not deep_scan_results:
            print("âŒ æ— æ·±åº¦æ‰«æç»“æœ")
            return
        
        print("\nğŸ”¬ æ·±åº¦æ‰«æç»“æœ")
        print("-" * 40)
        
        metadata = deep_scan_results.get("scan_metadata", {})
        summary = deep_scan_results.get("scan_summary", {})
        
        print(f"ğŸ“Š æ‰«æå…ƒæ•°æ®:")
        print(f"  - æ‰«ææ–‡ä»¶æ•°: {metadata.get('total_files_scanned', 0)}")
        print(f"  - æ‰«æç‰ˆæœ¬: {metadata.get('scan_version', 'N/A')}")
        print(f"  - æ‰«ææ—¶é—´: {metadata.get('scan_duration', 'N/A')}")
        
        overview = summary.get("scan_overview", {})
        print(f"\nğŸ“ˆ æ‰«ææ¦‚è§ˆ:")
        print(f"  - æ€»é—®é¢˜æ•°: {overview.get('total_issues', 0)}")
        print(f"  - å…³é”®é—®é¢˜: {overview.get('critical_issues', 0)}")
        print(f"  - é«˜ä¼˜å…ˆçº§: {overview.get('high_issues', 0)}")
        print(f"  - ä¸­ä¼˜å…ˆçº§: {overview.get('medium_issues', 0)}")
        print(f"  - ä½ä¼˜å…ˆçº§: {overview.get('low_issues', 0)}")
        
        metrics = summary.get("quality_metrics", {})
        print(f"\nğŸ“Š è´¨é‡æŒ‡æ ‡:")
        print(f"  - è´¨é‡ç­‰çº§: {metrics.get('quality_grade', 'N/A')}")
        print(f"  - æ•´ä½“è¯„åˆ†: {metrics.get('overall_quality_score', 0):.2f}")
        print(f"  - å¹³å‡å¤æ‚åº¦: {metrics.get('average_complexity', 0):.1f}")
        print(f"  - å¹³å‡å¯ç»´æŠ¤æ€§: {metrics.get('average_maintainability', 0):.1f}")
    
    async def _show_feature_analyses(self, feature_analyses: Optional[List[Dict[str, Any]]]):
        """æ˜¾ç¤ºåŠŸèƒ½ç‰¹ç‚¹åˆ†æ"""
        if not feature_analyses:
            print("âŒ æ— åŠŸèƒ½ç‰¹ç‚¹åˆ†æç»“æœ")
            return
        
        print("\nğŸ¯ åŠŸèƒ½ç‰¹ç‚¹åˆ†ææ¦‚è§ˆ")
        print("-" * 40)
        
        total_files = len(feature_analyses)
        high_value_files = len([f for f in feature_analyses if hasattr(f, 'recommendation') and "ä¿ç•™" in f.recommendation])
        low_value_files = len([f for f in feature_analyses if hasattr(f, 'recommendation') and "åˆ é™¤" in f.recommendation])
        
        print(f"ğŸ“ åˆ†æç»Ÿè®¡:")
        print(f"  - æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"  - é«˜ä»·å€¼æ–‡ä»¶: {high_value_files}")
        print(f"  - ä½ä»·å€¼æ–‡ä»¶: {low_value_files}")
        
        # æ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶çš„è¯¦ç»†åˆ†æ
        print(f"\nğŸ“‹ å‰5ä¸ªæ–‡ä»¶åˆ†æ:")
        for i, analysis in enumerate(feature_analyses[:5]):
            print(f"\n{i+1}. {analysis.file_path}")
            if hasattr(analysis, 'overall_assessment'):
                print(f"   è¯„ä¼°: {analysis.overall_assessment.split('**è¯„ä¼°ç»“è®º**:')[-1].strip()}")
            if hasattr(analysis, 'recommendation'):
                print(f"   æ¨è: {analysis.recommendation}")
    
    async def _show_decision_records(self, decision_records: Optional[List[Dict[str, Any]]]):
        """æ˜¾ç¤ºå†³ç­–è®°å½•"""
        if not decision_records:
            print("âŒ æ— å†³ç­–è®°å½•")
            return
        
        print("\nâš–ï¸ å†³ç­–è®°å½•æ¦‚è§ˆ")
        print("-" * 40)
        
        total_decisions = len(decision_records)
        high_confidence_decisions = len([d for d in decision_records if d.get('confidence_score', 0) > 0.8])
        
        print(f"ğŸ“Š å†³ç­–ç»Ÿè®¡:")
        print(f"  - æ€»å†³ç­–æ•°: {total_decisions}")
        print(f"  - é«˜ç½®ä¿¡åº¦: {high_confidence_decisions}")
        print(f"  - å¹³å‡ç½®ä¿¡åº¦: {sum(d.get('confidence_score', 0) for d in decision_records) / max(total_decisions, 1):.2f}")
        
        # æ˜¾ç¤ºå‰3ä¸ªå†³ç­–çš„è¯¦ç»†ä¿¡æ¯
        print(f"\nğŸ“‹ å‰3ä¸ªå†³ç­–è¯¦æƒ…:")
        for i, record in enumerate(decision_records[:3]):
            print(f"\n{i+1}. å†³ç­–ID: {record.get('decision_id', 'N/A')}")
            print(f"   ç›®æ ‡: {record.get('target', 'N/A')}")
            print(f"   ç±»å‹: {record.get('decision_type', 'N/A')}")
            print(f"   å†³ç­–: {record.get('decision', 'N/A')}")
            print(f"   ç½®ä¿¡åº¦: {record.get('confidence_score', 0):.2f}")
            print(f"   é£é™©è¯„ä¼°: {record.get('risk_assessment', 'N/A')}")
    
    async def _show_optimization_recommendations(self, optimization_recommendations: List[Dict[str, Any]]):
        """æ˜¾ç¤ºä¼˜åŒ–å»ºè®®"""
        if not optimization_recommendations:
            print("âŒ æ— ä¼˜åŒ–å»ºè®®")
            return
        
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®æ¦‚è§ˆ")
        print("-" * 40)
        
        print(f"ğŸ“Š å»ºè®®ç»Ÿè®¡:")
        print(f"  - æ€»å»ºè®®æ•°: {len(optimization_recommendations)}")
        
        critical_recommendations = [r for r in optimization_recommendations if r.get('priority') == 'critical']
        high_recommendations = [r for r in optimization_recommendations if r.get('priority') == 'high']
        
        print(f"  - å…³é”®å»ºè®®: {len(critical_recommendations)}")
        print(f"  - é«˜ä¼˜å…ˆçº§: {len(high_recommendations)}")
        
        print(f"\nğŸ’¡ è¯¦ç»†å»ºè®®:")
        for i, rec in enumerate(optimization_recommendations[:5], 1):
            print(f"\n{i+1}. {rec.get('category', 'N/A')} - {rec.get('description', 'N/A')}")
            print(f"   ä¼˜å…ˆçº§: {rec.get('priority', 'N/A')}")
            print(f"   è¡ŒåŠ¨: {rec.get('action', 'N/A')}")
            print(f"   å½±å“: {rec.get('impact', 'N/A')}")
    
    async def _show_project_context(self):
        """æ˜¾ç¤ºé¡¹ç›®ä¸Šä¸‹æ–‡"""
        print("\nğŸ—ï¸ é¡¹ç›®ä¸Šä¸‹æ–‡çŠ¶æ€")
        print("-" * 40)
        
        print("ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        print(f"  - é¡¹ç›®åç§°: iFlow CLI V16 Quantum Evolution")
        print(f"  - é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        print(f"  - åŠ è½½æ—¶é—´: {datetime.now().isoformat()}")
        print(f"  - ä¸Šä¸‹æ–‡ç‰ˆæœ¬: 2.0.0")
        print(f"  - é¡¹ç›®çŠ¶æ€: loaded")
        print(f"  - AIæ„ŸçŸ¥: enabled")
        print(f"  - è®°å¿†é›†æˆ: enabled")
    
    async def _show_session_readiness(self):
        """æ˜¾ç¤ºä¼šè¯å°±ç»ªçŠ¶æ€"""
        print("\nâœ… ä¼šè¯å°±ç»ªçŠ¶æ€")
        print("-" * 40)
        
        print("ğŸ“Š ç»„ä»¶çŠ¶æ€:")
        print("  âœ… AIä¿¡æ¯å¼ºåˆ¶ä¼ é€’å™¨ - å·²æ¿€æ´»")
        print("  âœ… é¡¹ç›®ç»“æ„åˆ†æå™¨ - å·²æ¿€æ´»")
        print("  âœ… æ·±åº¦æ‰«æå™¨ - å·²æ¿€æ´»")
        print("  âœ… åŠŸèƒ½åˆ†æå™¨ - å·²æ¿€æ´»")
        print("  âœ… å†³ç­–ç³»ç»Ÿ - å·²æ¿€æ´»")
        print("  âœ… ä¼˜åŒ–å»ºè®®ç”Ÿæˆå™¨ - å·²æ¿€æ´»")
        
        print("\nğŸ“Š å°±ç»ªæŒ‡æ ‡:")
        print("  âœ… é¡¹ç›®ä¸Šä¸‹æ–‡å·²åŠ è½½")
        print("  âœ… AIä¿¡æ¯å·²ä¼ é€’")
        print("  âœ… è®°å¿†é›†æˆå·²å»ºç«‹")
        print("  âœ… æ‰€æœ‰ç»„ä»¶å·²å°±ç»ª")
        print("  âœ… ä¼šè¯å®Œå…¨å°±ç»ª")
    
    async def _export_detailed_reports(self):
        """å¯¼å‡ºè¯¦ç»†æŠ¥å‘Š"""
        print("\nğŸ“„ å¯¼å‡ºè¯¦ç»†æŠ¥å‘Š")
        print("-" * 40)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_dir = self.results_dir / f"enhanced_sc_load_export_{timestamp}"
        export_dir.mkdir(exist_ok=True)
        
        try:
            # å¯¼å‡ºæ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶
            report_types = [
                ("é¡¹ç›®ä¸Šä¸‹æ–‡", "project_context_*.json"),
                ("ç»“æ„åˆ†æ", "structure_comparison_*.json"),
                ("æ·±åº¦æ‰«æç»“æœ", "deep_scan_results_*.json"),
                ("åŠŸèƒ½åˆ†æç»“æœ", "feature_analyses_*.json"),
                ("å†³ç­–è®°å½•", "decision_records_*.json"),
                ("ä¼˜åŒ–å»ºè®®", "optimization_recommendations_*.json")
            ]
            
            exported_files = []
            for report_type, pattern in report_types:
                files = list(self.results_dir.glob(pattern))
                for file in files:
                    dest = export_dir / file.name
                    file.rename(dest)
                    exported_files.append(str(dest))
            
            print(f"âœ… æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {export_dir}")
            print(f"ğŸ“ å¯¼å‡ºæ–‡ä»¶æ•°: {len(exported_files)}")
            
            # ç”Ÿæˆå¯¼å‡ºæ¸…å•
            manifest = {
                "export_timestamp": timestamp,
                "export_directory": str(export_dir),
                "exported_files": exported_files,
                "total_files": len(exported_files)
            }
            
            manifest_file = export_dir / "export_manifest.json"
            with open(manifest_file, 'w', encoding='utf-8') as f:
                json.dump(manifest, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“‹ å¯¼å‡ºæ¸…å•: {manifest_file}")
        
        except Exception as e:
            print(f"âŒ æŠ¥å‘Šå¯¼å‡ºå¤±è´¥: {e}")
    
    async def _reanalyze_specific_file(self):
        """é‡æ–°åˆ†æç‰¹å®šæ–‡ä»¶"""
        print("\nğŸ”„ é‡æ–°åˆ†æç‰¹å®šæ–‡ä»¶")
        print("-" * 40)
        
        try:
            file_path = input("è¯·è¾“å…¥è¦é‡æ–°åˆ†æçš„æ–‡ä»¶è·¯å¾„: ").strip()
            if not file_path:
                print("âŒ æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
                return
            
            full_path = self.project_root / file_path
            if not full_path.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return
            
            print(f"ğŸ” é‡æ–°åˆ†ææ–‡ä»¶: {file_path}")
            
            # æ‰§è¡ŒåŠŸèƒ½åˆ†æ
            feature_analysis = await self.feature_analyzer.analyze_comprehensive_features(file_path)
            
            # ç”Ÿæˆå†³ç­–è®°å½•
            analysis_data = {
                "features": [asdict(f) for f in feature_analysis.feature_characteristics],
                "advantages": [asdict(a) for a in feature_analysis.advantages],
                "disadvantages": [asdict(d) for d in feature_analysis.disadvantages],
                "alternatives": [asdict(a) for a in feature_analysis.alternatives]
            }
            
            if "åˆ é™¤" in feature_analysis.recommendation:
                decision_type = DecisionType.FILE_REMOVAL
            elif "ä¿ç•™" in feature_analysis.recommendation:
                decision_type = DecisionType.FILE_RETENTION
            elif "é‡æ„" in feature_analysis.recommendation:
                decision_type = DecisionType.CODE_REFACTOR
            else:
                decision_type = DecisionType.FILE_RETENTION
            
            decision_record = await self.justification_system.create_comprehensive_decision(
                decision_type=decision_type,
                target=file_path,
                analysis_data=analysis_data
            )
            
            print(f"\nğŸ“Š é‡æ–°åˆ†æç»“æœ:")
            print(f"  - ç‰¹å¾æ•°é‡: {len(feature_analysis.feature_characteristics)}")
            print(f"  - ä¼˜åŠ¿æ•°é‡: {len(feature_analysis.advantages)}")
            print(f"  - åŠ£åŠ¿æ•°é‡: {len(feature_analysis.disadvantages)}")
            print(f"  - æ›¿ä»£æ–¹æ¡ˆ: {len(feature_analysis.alternatives)}")
            print(f"  - æ¨è: {feature_analysis.recommendation}")
            
            print(f"\nâš–ï¸ å†³ç­–è®°å½•å·²ç”Ÿæˆ: {decision_record.decision_id}")
            print(f"  - å†³ç­–: {decision_record.decision}")
            print(f"  - ç½®ä¿¡åº¦: {decision_record.confidence_score:.2f}")
            print(f"  - é£é™©è¯„ä¼°: {decision_record.risk_assessment}")
        
        except Exception as e:
            print(f"âŒ é‡æ–°åˆ†æå¤±è´¥: {e}")
    
    async def _generate_final_load_report(self, 
                                        ai_context: Optional[Dict[str, Any]],
                                        structure_analysis: Any,
                                        deep_scan_results: Optional[Dict[str, Any]],
                                        feature_analyses: Optional[List[Dict[str, Any]]],
                                        decision_records: Optional[List[Dict[str, Any]]],
                                        project_context: Dict[str, Any],
                                        context_validation: Dict[str, Any],
                                        change_analysis: Optional[Dict[str, Any]],
                                        optimization_recommendations: List[Dict[str, Any]],
                                        session_readiness: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆåŠ è½½æŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆæœ€ç»ˆåŠ è½½æŠ¥å‘Š...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        final_report = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "project_root": str(self.project_root),
                "report_version": "2.0.0",
                "command": "/sc:load enhanced"
            },
            "executive_summary": {
                "load_completed": True,
                "modules_executed": [
                    "AIä¿¡æ¯å¼ºåˆ¶ä¼ é€’",
                    "é¡¹ç›®ç»“æ„åˆ†æ",
                    "æ·±åº¦åˆ†ææ‰«æ",
                    "åŠŸèƒ½ç‰¹ç‚¹åˆ†æ",
                    "å†³ç­–è®°å½•æ¢å¤",
                    "é¡¹ç›®ä¸Šä¸‹æ–‡å»ºç«‹",
                    "æ™ºèƒ½ä¸Šä¸‹æ–‡éªŒè¯",
                    "å˜åŒ–æ£€æµ‹åˆ†æ",
                    "ä¼˜åŒ–å»ºè®®ç”Ÿæˆ",
                    "ä¼šè¯å°±ç»ªç¡®è®¤"
                ],
                "overall_status": "completed",
                "recommendations": []
            },
            "ai_context": ai_context,
            "structure_analysis": asdict(structure_analysis) if structure_analysis else None,
            "deep_scan_results": deep_scan_results,
            "feature_analyses": [asdict(f) for f in feature_analyses] if feature_analyses else [],
            "decision_records": decision_records,
            "project_context": project_context,
            "context_validation": context_validation,
            "change_analysis": change_analysis,
            "optimization_recommendations": optimization_recommendations,
            "session_readiness": session_readiness,
            "conclusions": await self._generate_load_conclusions(
                structure_analysis, deep_scan_results, feature_analyses, decision_records
            ),
            "next_steps": await self._generate_load_next_steps(
                structure_analysis, deep_scan_results, feature_analyses, decision_records
            )
        }
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        report_file = self.results_dir / f"enhanced_sc_load_final_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownç‰ˆæœ¬
        markdown_file = self.results_dir / f"enhanced_sc_load_final_report_{timestamp}.md"
        markdown_content = await self._generate_markdown_load_report(final_report)
        with open(markdown_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        print(f"ğŸ“‹ æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜:")
        print(f"  JSON: {report_file}")
        print(f"  Markdown: {markdown_file}")
        
        return final_report
    
    async def _generate_load_conclusions(self, 
                                     structure_analysis: Any,
                                     deep_scan_results: Optional[Dict[str, Any]],
                                     feature_analyses: Optional[List[Dict[str, Any]]],
                                     decision_records: Optional[List[Dict[str, Any]]]) -> List[str]:
        """ç”ŸæˆåŠ è½½ç»“è®º"""
        conclusions = []
        
        # åŸºäºç»“æ„åˆ†æçš„ç»“è®º
        if structure_analysis:
            changes = structure_analysis.structure_changes
            if changes.get("files_added_count", 0) > 0 or changes.get("files_removed_count", 0) > 0:
                conclusions.append(f"æ£€æµ‹åˆ°é¡¹ç›®ç»“æ„å˜åŒ–ï¼šæ–°å¢{changes.get('files_added_count', 0)}ä¸ªæ–‡ä»¶ï¼Œåˆ é™¤{changes.get('files_removed_count', 0)}ä¸ªæ–‡ä»¶")
        
        # åŸºäºæ·±åº¦æ‰«æçš„ç»“è®º
        if deep_scan_results:
            summary = deep_scan_results.get("scan_summary", {})
            total_issues = summary.get("scan_overview", {}).get("total_issues", 0)
            quality_score = summary.get("quality_metrics", {}).get("overall_quality_score", 0)
            
            if total_issues > 0:
                conclusions.append(f"å‘ç°{total_issues}ä¸ªéœ€è¦å…³æ³¨çš„é—®é¢˜ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†é«˜ä¸¥é‡æ€§é—®é¢˜")
            
            if quality_score > 0.8:
                conclusions.append("æ•´ä½“ä»£ç è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰æ ‡å‡†")
            elif quality_score > 0.6:
                conclusions.append("ä»£ç è´¨é‡ä¸­ç­‰ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
            else:
                conclusions.append("ä»£ç è´¨é‡éœ€è¦é‡ç‚¹æ”¹è¿›")
        
        # åŸºäºåŠŸèƒ½åˆ†æçš„ç»“è®º
        if feature_analyses:
            total_files = len(feature_analyses)
            high_value_files = len([f for f in feature_analyses if hasattr(f, 'recommendation') and "ä¿ç•™" in f.recommendation])
            
            conclusions.append(f"åˆ†æäº†{total_files}ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­{high_value_files}ä¸ªæ–‡ä»¶å»ºè®®ä¿ç•™")
            
            if high_value_files > total_files * 0.7:
                conclusions.append("é¡¹ç›®æ•´ä½“ä»·å€¼è¾ƒé«˜ï¼Œå¤§éƒ¨åˆ†æ–‡ä»¶éƒ½æœ‰æ˜ç¡®çš„ä¸šåŠ¡ä»·å€¼")
            elif high_value_files > total_files * 0.4:
                conclusions.append("é¡¹ç›®ä»·å€¼ä¸­ç­‰ï¼Œéœ€è¦ä¼˜åŒ–éƒ¨åˆ†æ–‡ä»¶çš„ä»·å€¼")
            else:
                conclusions.append("é¡¹ç›®ä»·å€¼åä½ï¼Œå»ºè®®è¿›è¡Œå¤§å¹…ä¼˜åŒ–")
        
        # åŸºäºå†³ç­–è®°å½•çš„ç»“è®º
        if decision_records:
            total_decisions = len(decision_records)
            high_confidence_decisions = len([d for d in decision_records if d.get('confidence_score', 0) > 0.8])
            
            conclusions.append(f"æ¢å¤äº†{total_decisions}ä¸ªå†³ç­–è®°å½•ï¼Œå…¶ä¸­{high_confidence_decisions}ä¸ªé«˜ç½®ä¿¡åº¦å†³ç­–")
            
            if high_confidence_decisions > total_decisions * 0.7:
                conclusions.append("å†³ç­–è´¨é‡è¾ƒé«˜ï¼Œå»ºè®®æ‰§è¡Œç›¸å…³å†³ç­–")
            else:
                conclusions.append("éƒ¨åˆ†å†³ç­–ç½®ä¿¡åº¦è¾ƒä½ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æ")
        
        return conclusions
    
    async def _generate_load_next_steps(self, 
                                    structure_analysis: Any,
                                    deep_scan_results: Optional[Dict[str, Any]],
                                    feature_analyses: Optional[List[Dict[str, Any]]],
                                    decision_records: Optional[List[Dict[str, Any]]]) -> List[str]:
        """ç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        next_steps = []
        
        # åŸºäºç»“æ„åˆ†æçš„è¡ŒåŠ¨
        if structure_analysis:
            changes = structure_analysis.structure_changes
            if changes.get("files_added_count", 0) > 0:
                next_steps.append("åˆ†ææ–°å¢æ–‡ä»¶çš„åŠŸèƒ½å’Œå½±å“ï¼Œç¡®ä¿å…¶ä»·å€¼")
            
            if changes.get("files_removed_count", 0) > 0:
                next_steps.append("éªŒè¯åˆ é™¤æ–‡ä»¶çš„å½±å“ï¼Œç¡®ä¿æ— åŠŸèƒ½æŸå¤±")
        
        # åŸºäºæ·±åº¦æ‰«æçš„è¡ŒåŠ¨
        if deep_scan_results:
            security_issues = len(deep_scan_results.get("security_issues", []))
            performance_issues = len(deep_scan_results.get("performance_issues", []))
            
            if security_issues > 0:
                next_steps.append("ç«‹å³ä¿®å¤æ‰€æœ‰å®‰å…¨é—®é¢˜ï¼Œç¡®ä¿ç³»ç»Ÿå®‰å…¨æ€§")
            
            if performance_issues > 0:
                next_steps.append("ä¼˜åŒ–æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ç³»ç»Ÿå“åº”é€Ÿåº¦")
        
        # åŸºäºåŠŸèƒ½åˆ†æçš„è¡ŒåŠ¨
        if feature_analyses:
            removal_candidates = [f for f in feature_analyses if hasattr(f, 'recommendation') and "åˆ é™¤" in f.recommendation]
            refactor_candidates = [f for f in feature_analyses if hasattr(f, 'recommendation') and "é‡æ„" in f.recommendation]
            
            if removal_candidates:
                next_steps.append(f"è°¨æ…è¯„ä¼°å¹¶è€ƒè™‘åˆ é™¤{len(removal_candidates)}ä¸ªä½ä»·å€¼æ–‡ä»¶")
            
            if refactor_candidates:
                next_steps.append(f"åˆ¶å®šé‡æ„è®¡åˆ’ï¼Œä¼˜åŒ–{len(refactor_candidates)}ä¸ªéœ€è¦æ”¹è¿›çš„æ–‡ä»¶")
        
        # åŸºäºå†³ç­–è®°å½•çš„è¡ŒåŠ¨
        if decision_records:
            high_risk_decisions = [d for d in decision_records if d.get('risk_assessment') == 'high']
            
            if high_risk_decisions:
                next_steps.append("é‡ç‚¹å…³æ³¨é«˜é£é™©å†³ç­–ï¼Œåˆ¶å®šè¯¦ç»†çš„é£é™©ç¼“è§£ç­–ç•¥")
        
        # é€šç”¨è¡ŒåŠ¨å»ºè®®
        next_steps.extend([
            "å»ºç«‹å®šæœŸé¡¹ç›®è¯„ä¼°æœºåˆ¶ï¼ŒæŒç»­ç›‘æ§é¡¹ç›®å¥åº·çŠ¶æ€",
            "å®Œå–„é¡¹ç›®æ–‡æ¡£ï¼Œè®°å½•é‡è¦çš„æ¶æ„å†³ç­–å’Œè®¾è®¡åŸåˆ™",
            "å®šæœŸé‡æ–°è¯„ä¼°é¡¹ç›®ç»“æ„ï¼Œç¡®ä¿æŒç»­çš„ä¼˜åŒ–å’Œæ”¹è¿›",
            "åŸºäºåˆ†æç»“æœåˆ¶å®šå…·ä½“çš„ä¼˜åŒ–è®¡åˆ’",
            "å»ºç«‹è·¨ä¼šè¯çš„è¿ç»­æ€§ç®¡ç†æœºåˆ¶"
        ])
        
        return next_steps
    
    async def _generate_markdown_load_report(self, final_report: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        content = []
        
        # æ ‡é¢˜
        content.append("# å¢å¼ºç‰ˆ /sc:load é¡¹ç›®ä¸Šä¸‹æ–‡åŠ è½½æŠ¥å‘Š")
        content.append(f"**ç”Ÿæˆæ—¶é—´**: {final_report['metadata']['generated_at']}")
        content.append(f"**é¡¹ç›®è·¯å¾„**: {final_report['metadata']['project_root']}")
        content.append("")
        
        # æ‰§è¡Œæ‘˜è¦
        content.append("## ğŸ“Š æ‰§è¡Œæ‘˜è¦")
        summary = final_report["executive_summary"]
        content.append(f"**åŠ è½½çŠ¶æ€**: {'âœ… å·²å®Œæˆ' if summary['load_completed'] else 'âŒ æœªå®Œæˆ'}")
        content.append("")
        
        content.append("### æ‰§è¡Œçš„æ¨¡å—")
        for module in summary["modules_executed"]:
            content.append(f"- âœ… {module}")
        content.append("")
        
        # ç»“è®º
        content.append("## ğŸ¯ ä¸»è¦ç»“è®º")
        for conclusion in final_report["conclusions"]:
            content.append(f"- {conclusion}")
        content.append("")
        
        # ä¸‹ä¸€æ­¥è¡ŒåŠ¨
        content.append("## ğŸ“‹ ä¸‹ä¸€æ­¥è¡ŒåŠ¨")
        for i, step in enumerate(final_report["next_steps"], 1):
            content.append(f"{i}. {step}")
        content.append("")
        
        return "\n".join(content)

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆ /sc:load å‘½ä»¤")
    parser.add_argument("--project-root", default=".", help="é¡¹ç›®æ ¹ç›®å½•")
    parser.add_argument("--type", default="project", 
                       choices=["project", "config", "deps", "checkpoint"], 
                       help="åŠ è½½ç±»å‹")
    parser.add_argument("--refresh", action="store_true", help="åˆ·æ–°åˆ†æ")
    parser.add_argument("--analyze", action="store_true", help="æ‰§è¡Œæ·±åº¦åˆ†æ")
    parser.add_argument("--deep-analysis", action="store_true", help="æ‰§è¡Œæ·±åº¦åˆ†ææ‰«æ")
    parser.add_argument("--checkpoint", help="æŒ‡å®šæ£€æŸ¥ç‚¹ID")
    parser.add_argument("--no-interactive", action="store_true", help="éäº¤äº’æ¨¡å¼")
    parser.add_argument("--no-ai-awareness", action="store_true", help="ç¦ç”¨AIä¿¡æ¯ä¼ é€’")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¢å¼ºç‰ˆåŠ è½½å‘½ä»¤å®ä¾‹
    enhanced_load = EnhancedSCLoadCommand(args.project_root)
    
    # æ ¹æ®ç±»å‹æ‰§è¡Œä¸åŒçš„åŠ è½½æµç¨‹
    if args.type == "checkpoint":
        # æ£€æŸ¥ç‚¹æ¢å¤æ¨¡å¼
        results = await enhanced_load.execute_enhanced_load(
            load_type="checkpoint",
            checkpoint=args.checkpoint,
            interactive_mode=not args.no_interactive,
            force_ai_awareness=not args.no_ai_awareness
        )
    else:
        # æ ‡å‡†åŠ è½½æ¨¡å¼
        results = await enhanced_load.execute_enhanced_load(
            load_type=args.type,
            refresh=args.refresh,
            analyze=args.analyze,
            deep_analysis=args.deep_analysis,
            checkpoint=args.checkpoint,
            interactive_mode=not args.no_interactive,
            force_ai_awareness=not args.no_ai_awareness
        )
    
    print(f"\nğŸ‰ å¢å¼ºç‰ˆ /sc:load æ‰§è¡Œå®Œæˆ!")
    print(f"ğŸ“Š åŠ è½½çŠ¶æ€: {results['executive_summary']['overall_status']}")
    
    # æ˜¾ç¤ºå…³é”®ç»“æœ
    if results.get("project_context"):
        print(f"ğŸ“ é¡¹ç›®åç§°: {results['project_context']['project_name']}")
        print(f"ğŸ“ é¡¹ç›®çŠ¶æ€: {results['project_context']['project_status']}")
        print(f"ğŸ“ ä¸Šä¸‹æ–‡ç‰ˆæœ¬: {results['project_context']['context_version']}")
    
    # æ˜¾ç¤ºä¸‹ä¸€æ­¥è¡ŒåŠ¨
    if results.get("next_steps"):
        print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
        for i, step in enumerate(results["next_steps"][:3], 1):
            print(f"{i}. {step}")

if __name__ == "__main__":
    asyncio.run(main())
