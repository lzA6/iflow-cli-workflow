#!/usr/bin/env python3
"""
å†…ç½®æ·±åº¦åˆ†ææ‰«æå®¡æŸ¥åŠŸèƒ½
æä¾›å…¨é¢çš„ä»£ç è´¨é‡ã€å®‰å…¨æ€§ã€æ€§èƒ½å’Œæ¶æ„æ·±åº¦åˆ†æ
"""

import os
import re
import ast
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime
import asyncio
import subprocess
import sys

@dataclass
class ScanIssue:
    """æ‰«æé—®é¢˜"""
    file_path: str
    line_number: int
    issue_type: str
    severity: str
    category: str
    description: str
    evidence: str
    recommendation: str
    impact_score: float
    fix_complexity: str
    references: List[str]

@dataclass
class FileMetrics:
    """æ–‡ä»¶æŒ‡æ ‡"""
    file_path: str
    lines_of_code: int
    cyclomatic_complexity: int
    cognitive_complexity: int
    maintainability_index: float
    halstead_volume: float
    comment_ratio: float
    duplication_ratio: float
    test_coverage: float
    security_score: float
    performance_score: float
    architecture_score: float

@dataclass
class ArchitectureAnalysis:
    """æ¶æ„åˆ†æ"""
    module_dependencies: Dict[str, List[str]]
    circular_dependencies: List[Tuple[str, str]]
    coupling_metrics: Dict[str, float]
    cohesion_metrics: Dict[str, float]
    design_patterns: List[str]
    anti_patterns: List[str]
    layer_violations: List[str]
    interface_segregation: Dict[str, float]

class DeepAnalysisScanner:
    """æ·±åº¦åˆ†ææ‰«æå™¨"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.scan_results = {
            "issues": [],
            "metrics": {},
            "architecture": {},
            "summary": {}
        }
        
        # æ‰«æè§„åˆ™é…ç½®
        self.security_rules = self._load_security_rules()
        self.performance_rules = self._load_performance_rules()
        self.quality_rules = self._load_quality_rules()
        self.architecture_rules = self._load_architecture_rules()
        
    async def perform_comprehensive_scan(self) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨é¢æ‰«æ"""
        print("ğŸ”¬ å¯åŠ¨æ·±åº¦åˆ†ææ‰«æå®¡æŸ¥ç³»ç»Ÿ...")
        print("=" * 60)
        
        # 1. é¡¹ç›®æ–‡ä»¶å‘ç°
        print("ğŸ“ å‘ç°é¡¹ç›®æ–‡ä»¶...")
        python_files = await self._discover_python_files()
        
        # 2. å¹¶è¡Œæ–‡ä»¶åˆ†æ
        print("ğŸ” å¹¶è¡Œåˆ†ææ–‡ä»¶...")
        file_analyses = await self._analyze_files_parallel(python_files)
        
        # 3. å®‰å…¨æ€§æ·±åº¦æ‰«æ
        print("ğŸ›¡ï¸ æ‰§è¡Œå®‰å…¨æ€§æ·±åº¦æ‰«æ...")
        security_issues = await self._perform_security_scan(file_analyses)
        
        # 4. æ€§èƒ½æ·±åº¦æ‰«æ
        print("âš¡ æ‰§è¡Œæ€§èƒ½æ·±åº¦æ‰«æ...")
        performance_issues = await self._perform_performance_scan(file_analyses)
        
        # 5. ä»£ç è´¨é‡æ·±åº¦æ‰«æ
        print("ğŸ“‹ æ‰§è¡Œä»£ç è´¨é‡æ·±åº¦æ‰«æ...")
        quality_issues = await self._perform_quality_scan(file_analyses)
        
        # 6. æ¶æ„æ·±åº¦åˆ†æ
        print("ğŸ—ï¸ æ‰§è¡Œæ¶æ„æ·±åº¦åˆ†æ...")
        architecture_analysis = await self._perform_architecture_analysis(file_analyses)
        
        # 7. ä¾èµ–å…³ç³»åˆ†æ
        print("ğŸ”— æ‰§è¡Œä¾èµ–å…³ç³»åˆ†æ...")
        dependency_analysis = await self._analyze_dependencies(file_analyses)
        
        # 8. åæ¨¡å¼æ£€æµ‹
        print("ğŸš« æ‰§è¡Œåæ¨¡å¼æ£€æµ‹...")
        anti_patterns = await self._detect_anti_patterns(file_analyses)
        
        # 9. åˆå¹¶æ‰€æœ‰é—®é¢˜
        all_issues = security_issues + performance_issues + quality_issues + anti_patterns
        
        # 10. è®¡ç®—ç»¼åˆæŒ‡æ ‡
        comprehensive_metrics = await self._calculate_comprehensive_metrics(
            file_analyses, all_issues, architecture_analysis
        )
        
        # 11. ç”Ÿæˆæ‰«ææ‘˜è¦
        scan_summary = await self._generate_scan_summary(
            python_files, all_issues, comprehensive_metrics
        )
        
        # 12. æ„å»ºæœ€ç»ˆç»“æœ
        final_results = {
            "scan_metadata": {
                "timestamp": datetime.now().isoformat(),
                "project_root": str(self.project_root),
                "total_files_scanned": len(python_files),
                "scan_duration": "å¾…è®¡ç®—",
                "scan_version": "1.0.0"
            },
            "file_analyses": [asdict(analysis) for analysis in file_analyses],
            "security_issues": [asdict(issue) for issue in security_issues],
            "performance_issues": [asdict(issue) for issue in performance_issues],
            "quality_issues": [asdict(issue) for issue in quality_issues],
            "architecture_analysis": asdict(architecture_analysis),
            "dependency_analysis": dependency_analysis,
            "anti_patterns": [asdict(pattern) for pattern in anti_patterns],
            "comprehensive_metrics": comprehensive_metrics,
            "scan_summary": scan_summary,
            "recommendations": await self._generate_comprehensive_recommendations(all_issues)
        }
        
        print("âœ… æ·±åº¦åˆ†ææ‰«æå®¡æŸ¥å®Œæˆ")
        return final_results
    
    async def _discover_python_files(self) -> List[Path]:
        """å‘ç°Pythonæ–‡ä»¶"""
        python_files = []
        
        for root, dirs, files in os.walk(self.project_root):
            # è·³è¿‡ç‰¹å®šç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules', 'venv', 'env']]
            
            for file in files:
                if file.endswith('.py') and not file.startswith('.'):
                    file_path = Path(root) / file
                    python_files.append(file_path)
        
        return python_files
    
    async def _analyze_files_parallel(self, python_files: List[Path]) -> List[FileMetrics]:
        """å¹¶è¡Œåˆ†ææ–‡ä»¶"""
        # ç®€åŒ–çš„å¹¶è¡Œå¤„ç†ï¼ˆå®é™…å¯ä»¥ä½¿ç”¨asyncio.gatherï¼‰
        file_analyses = []
        
        for file_path in python_files:
            try:
                metrics = await self._analyze_file_metrics(file_path)
                file_analyses.append(metrics)
            except Exception as e:
                print(f"âš ï¸ åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return file_analyses
    
    async def _analyze_file_metrics(self, file_path: Path) -> FileMetrics:
        """åˆ†ææ–‡ä»¶æŒ‡æ ‡"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åŸºæœ¬æŒ‡æ ‡
            lines = content.split('\n')
            lines_of_code = len([line for line in lines if line.strip() and not line.strip().startswith('#')])
            
            # åœˆå¤æ‚åº¦
            cyclomatic_complexity = await self._calculate_cyclomatic_complexity(content)
            
            # è®¤çŸ¥å¤æ‚åº¦
            cognitive_complexity = await self._calculate_cognitive_complexity(content)
            
            # å¯ç»´æŠ¤æ€§æŒ‡æ•°
            maintainability_index = await self._calculate_maintainability_index(content, cyclomatic_complexity)
            
            # Halsteadä½“ç§¯
            halstead_volume = await self._calculate_halstead_volume(content)
            
            # æ³¨é‡Šæ¯”ç‡
            comment_lines = len([line for line in lines if line.strip().startswith('#')])
            comment_ratio = comment_lines / max(lines_of_code, 1)
            
            # é‡å¤ç‡
            duplication_ratio = await self._calculate_duplication_ratio(content)
            
            # æµ‹è¯•è¦†ç›–ç‡ï¼ˆä¼°ç®—ï¼‰
            test_coverage = await self._estimate_test_coverage(file_path)
            
            # å®‰å…¨è¯„åˆ†
            security_score = await self._calculate_security_score(content)
            
            # æ€§èƒ½è¯„åˆ†
            performance_score = await self._calculate_performance_score(content)
            
            # æ¶æ„è¯„åˆ†
            architecture_score = await self._calculate_architecture_score(content)
            
            return FileMetrics(
                file_path=str(file_path.relative_to(self.project_root)),
                lines_of_code=lines_of_code,
                cyclomatic_complexity=cyclomatic_complexity,
                cognitive_complexity=cognitive_complexity,
                maintainability_index=maintainability_index,
                halstead_volume=halstead_volume,
                comment_ratio=comment_ratio,
                duplication_ratio=duplication_ratio,
                test_coverage=test_coverage,
                security_score=security_score,
                performance_score=performance_score,
                architecture_score=architecture_score
            )
            
        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶æŒ‡æ ‡åˆ†æå¤±è´¥ {file_path}: {e}")
            return FileMetrics(
                file_path=str(file_path.relative_to(self.project_root)),
                lines_of_code=0,
                cyclomatic_complexity=0,
                cognitive_complexity=0,
                maintainability_index=0,
                halstead_volume=0,
                comment_ratio=0,
                duplication_ratio=0,
                test_coverage=0,
                security_score=0,
                performance_score=0,
                architecture_score=0
            )
    
    async def _calculate_cyclomatic_complexity(self, content: str) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦"""
        try:
            tree = ast.parse(content)
            complexity = 1  # åŸºç¡€å¤æ‚åº¦
            
            for node in ast.walk(tree):
                if isinstance(node, (ast.If, ast.While, ast.For, ast.With, ast.AsyncWith)):
                    complexity += 1
                elif isinstance(node, ast.ExceptHandler):
                    complexity += 1
                elif isinstance(node, ast.BoolOp):
                    complexity += len(node.values) - 1
                elif isinstance(node, (ast.ListComp, ast.DictComp, ast.SetComp, ast.GeneratorExp)):
                    complexity += 1
            
            return complexity
            
        except Exception:
            return 0
    
    async def _calculate_cognitive_complexity(self, content: str) -> int:
        """è®¡ç®—è®¤çŸ¥å¤æ‚åº¦"""
        complexity = 0
        nesting_level = 0
        
        lines = content.split('\n')
        for line in lines:
            stripped = line.strip()
            
            # å¢åŠ åµŒå¥—å±‚çº§
            if any(keyword in stripped for keyword in ['if', 'elif', 'else:', 'for', 'while', 'try:', 'except', 'with']):
                nesting_level += 1
                complexity += nesting_level
            
            # å‡å°‘åµŒå¥—å±‚çº§
            if stripped == 'pass' or stripped.startswith('return'):
                nesting_level = max(0, nesting_level - 1)
        
        return complexity
    
    async def _calculate_maintainability_index(self, content: str, cyclomatic_complexity: int) -> float:
        """è®¡ç®—å¯ç»´æŠ¤æ€§æŒ‡æ•°"""
        lines = len(content.split('\n'))
        
        # ç®€åŒ–çš„å¯ç»´æŠ¤æ€§æŒ‡æ•°è®¡ç®—
        base_score = 100.0
        
        # ä»£ç é‡å½±å“
        if lines > 1000:
            base_score -= 20
        elif lines > 500:
            base_score -= 10
        elif lines > 200:
            base_score -= 5
        
        # å¤æ‚åº¦å½±å“
        base_score -= cyclomatic_complexity * 2
        
        # æ³¨é‡Šå½±å“
        comment_lines = content.count('#')
        comment_ratio = comment_lines / max(lines, 1)
        base_score += comment_ratio * 10
        
        return max(0, min(100, base_score))
    
    async def _calculate_halstead_volume(self, content: str) -> float:
        """è®¡ç®—Halsteadä½“ç§¯"""
        # ç®€åŒ–çš„Halsteadä½“ç§¯è®¡ç®—
        operators = len(re.findall(r'[+\-*/%=<>!&|^~]', content))
        operands = len(re.findall(r'\b\w+\b', content))
        
        if operators == 0 or operands == 0:
            return 0.0
        
        vocabulary = operators + operands
        length = operators + operands
        
        try:
            volume = length * (vocabulary.bit_length() / 2)
            return volume
        except:
            return 0.0
    
    async def _calculate_duplication_ratio(self, content: str) -> float:
        """è®¡ç®—é‡å¤ç‡"""
        lines = [line.strip() for line in content.split('\n') if line.strip() and len(line.strip()) > 10]
        
        if len(lines) < 10:
            return 0.0
        
        unique_lines = set(lines)
        return 1.0 - (len(unique_lines) / len(lines))
    
    async def _estimate_test_coverage(self, file_path: Path) -> float:
        """ä¼°ç®—æµ‹è¯•è¦†ç›–ç‡"""
        # æ£€æŸ¥å¯¹åº”çš„æµ‹è¯•æ–‡ä»¶
        test_patterns = [
            f"test_{file_path.stem}.py",
            f"{file_path.stem}_test.py"
        ]
        
        for pattern in test_patterns:
            test_file = file_path.parent / pattern
            if test_file.exists():
                return 0.8
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ–‡ä»¶
        if 'test' in file_path.name.lower():
            return 0.9
        
        return 0.3
    
    async def _calculate_security_score(self, content: str) -> float:
        """è®¡ç®—å®‰å…¨è¯„åˆ†"""
        score = 1.0
        
        # å±é™©å‡½æ•°æ‰£åˆ†
        dangerous_functions = ['eval(', 'exec(', 'compile(']
        for func in dangerous_functions:
            if func in content:
                score -= 0.3
        
        # ç¡¬ç¼–ç å¯†ç æ‰£åˆ†
        if re.search(r'password\s*=\s*["\'][^"\']+["\']', content, re.IGNORECASE):
            score -= 0.4
        
        # SQLæ³¨å…¥é£é™©æ‰£åˆ†
        if 'execute(' in content and '%' in content:
            score -= 0.2
        
        # æ–‡ä»¶è·¯å¾„éå†æ‰£åˆ†
        if '../' in content:
            score -= 0.2
        
        return max(0, score)
    
    async def _calculate_performance_score(self, content: str) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        score = 1.0
        
        # å¾ªç¯ä¸­çš„æ•°æ®åº“æŸ¥è¯¢æ‰£åˆ†
        if re.search(r'for.*in.*:.*\.query\(', content):
            score -= 0.3
        
        # å¤§æ–‡ä»¶ä¸€æ¬¡æ€§è¯»å–æ‰£åˆ†
        if 'file.read()' in content and 'with open' in content:
            score -= 0.2
        
        # ä½æ•ˆå­—ç¬¦ä¸²æ“ä½œæ‰£åˆ†
        if content.count('+') > 50:
            score -= 0.1
        
        # æœªä½¿ç”¨ç¼“å­˜æ‰£åˆ†
        if 'database' in content.lower() and 'cache' not in content.lower():
            score -= 0.1
        
        return max(0, score)
    
    async def _calculate_architecture_score(self, content: str) -> float:
        """è®¡ç®—æ¶æ„è¯„åˆ†"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # é¢å‘å¯¹è±¡è®¾è®¡åŠ åˆ†
        if 'class ' in content:
            score += 0.2
        
        # æ¨¡å—åŒ–è®¾è®¡åŠ åˆ†
        if 'import' in content:
            score += 0.1
        
        # å¼‚æ­¥è®¾è®¡åŠ åˆ†
        if 'async def' in content:
            score += 0.1
        
        # é”™è¯¯å¤„ç†åŠ åˆ†
        if 'try:' in content and 'except' in content:
            score += 0.1
        
        return min(1.0, score)
    
    async def _perform_security_scan(self, file_analyses: List[FileMetrics]) -> List[ScanIssue]:
        """æ‰§è¡Œå®‰å…¨æ€§æ‰«æ"""
        issues = []
        
        for metrics in file_analyses:
            file_path = self.project_root / metrics.file_path
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                file_issues = await self._apply_security_rules(content, str(file_path))
                issues.extend(file_issues)
                
            except Exception as e:
                print(f"âš ï¸ å®‰å…¨æ‰«æå¤±è´¥ {file_path}: {e}")
        
        return issues
    
    async def _apply_security_rules(self, content: str, file_path: str) -> List[ScanIssue]:
        """åº”ç”¨å®‰å…¨è§„åˆ™"""
        issues = []
        lines = content.split('\n')
        
        for rule in self.security_rules:
            if rule["type"] == "pattern":
                matches = re.finditer(rule["pattern"], content)
                for match in matches:
                    line_num = content[:match.start()].count('\n') + 1
                    
                    issue = ScanIssue(
                        file_path=file_path,
                        line_number=line_num,
                        issue_type="security",
                        severity=rule["severity"],
                        category=rule["category"],
                        description=rule["description"],
                        evidence=match.group(0),
                        recommendation=rule["recommendation"],
                        impact_score=rule["impact_score"],
                        fix_complexity=rule["fix_complexity"],
                        references=rule.get("references", [])
                    )
                    issues.append(issue)
        
        return issues
    
    async def _perform_performance_scan(self, file_analyses: List[FileMetrics]) -> List[ScanIssue]:
        """æ‰§è¡Œæ€§èƒ½æ‰«æ"""
        issues = []
        
        for metrics in file_analyses:
            file_path = self.project_root / metrics.file_path
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                file_issues = await self._apply_performance_rules(content, str(file_path))
                issues.extend(file_issues)
                
            except Exception as e:
                print(f"âš ï¸ æ€§èƒ½æ‰«æå¤±è´¥ {file_path}: {e}")
        
        return issues
    
    async def _apply_performance_rules(self, content: str, file_path: str) -> List[ScanIssue]:
        """åº”ç”¨æ€§èƒ½è§„åˆ™"""
        issues = []
        lines = content.split('\n')
        
        for rule in self.performance_rules:
            if rule["type"] == "pattern":
                matches = re.finditer(rule["pattern"], content, re.MULTILINE)
                for match in matches:
                    line_num = content[:match.start()].count('\n') + 1
                    
                    issue = ScanIssue(
                        file_path=file_path,
                        line_number=line_num,
                        issue_type="performance",
                        severity=rule["severity"],
                        category=rule["category"],
                        description=rule["description"],
                        evidence=match.group(0),
                        recommendation=rule["recommendation"],
                        impact_score=rule["impact_score"],
                        fix_complexity=rule["fix_complexity"],
                        references=rule.get("references", [])
                    )
                    issues.append(issue)
        
        return issues
    
    async def _perform_quality_scan(self, file_analyses: List[FileMetrics]) -> List[ScanIssue]:
        """æ‰§è¡Œä»£ç è´¨é‡æ‰«æ"""
        issues = []
        
        for metrics in file_analyses:
            file_path = self.project_root / metrics.file_path
            
            # åŸºäºæŒ‡æ ‡çš„è´¨é‡é—®é¢˜
            if metrics.cyclomatic_complexity > 10:
                issue = ScanIssue(
                    file_path=str(file_path),
                    line_number=0,
                    issue_type="quality",
                    severity="medium",
                    category="complexity",
                    description=f"åœˆå¤æ‚åº¦è¿‡é«˜: {metrics.cyclomatic_complexity}",
                    evidence=f"åœˆå¤æ‚åº¦ = {metrics.cyclomatic_complexity}",
                    recommendation="é‡æ„å‡½æ•°ï¼Œé™ä½å¤æ‚åº¦",
                    impact_score=0.6,
                    fix_complexity="medium",
                    references=["Cyclomatic Complexity Best Practices"]
                )
                issues.append(issue)
            
            if metrics.maintainability_index < 50:
                issue = ScanIssue(
                    file_path=str(file_path),
                    line_number=0,
                    issue_type="quality",
                    severity="medium",
                    category="maintainability",
                    description=f"å¯ç»´æŠ¤æ€§æŒ‡æ•°è¿‡ä½: {metrics.maintainability_index:.1f}",
                    evidence=f"å¯ç»´æŠ¤æ€§æŒ‡æ•° = {metrics.maintainability_index:.1f}",
                    recommendation="æ”¹è¿›ä»£ç ç»“æ„ï¼Œæå‡å¯ç»´æŠ¤æ€§",
                    impact_score=0.5,
                    fix_complexity="medium",
                    references=["Maintainability Index Guidelines"]
                )
                issues.append(issue)
            
            if metrics.duplication_ratio > 0.3:
                issue = ScanIssue(
                    file_path=str(file_path),
                    line_number=0,
                    issue_type="quality",
                    severity="low",
                    category="duplication",
                    description=f"ä»£ç é‡å¤ç‡è¿‡é«˜: {metrics.duplication_ratio:.2f}",
                    evidence=f"é‡å¤ç‡ = {metrics.duplication_ratio:.2f}",
                    recommendation="æå–å…¬å…±å‡½æ•°ï¼Œå‡å°‘é‡å¤ä»£ç ",
                    impact_score=0.3,
                    fix_complexity="low",
                    references=["DRY Principle"]
                )
                issues.append(issue)
        
        return issues
    
    async def _perform_architecture_analysis(self, file_analyses: List[FileMetrics]) -> ArchitectureAnalysis:
        """æ‰§è¡Œæ¶æ„åˆ†æ"""
        # ç®€åŒ–çš„æ¶æ„åˆ†æ
        module_dependencies = {}
        circular_dependencies = []
        coupling_metrics = {}
        cohesion_metrics = {}
        design_patterns = []
        anti_patterns = []
        layer_violations = []
        interface_segregation = {}
        
        # åˆ†ææ¨¡å—ä¾èµ–
        for metrics in file_analyses:
            file_path = Path(metrics.file_path)
            module_name = file_path.stem
            
            # ç®€åŒ–çš„ä¾èµ–åˆ†æ
            dependencies = []  # å®é™…éœ€è¦è§£æimportè¯­å¥
            module_dependencies[module_name] = dependencies
            
            # è€¦åˆåº¦æŒ‡æ ‡ï¼ˆç®€åŒ–ï¼‰
            coupling_metrics[module_name] = len(dependencies) * 0.1
            
            # å†…èšåº¦æŒ‡æ ‡ï¼ˆç®€åŒ–ï¼‰
            cohesion_metrics[module_name] = 0.8  # é»˜è®¤å€¼
        
        return ArchitectureAnalysis(
            module_dependencies=module_dependencies,
            circular_dependencies=circular_dependencies,
            coupling_metrics=coupling_metrics,
            cohesion_metrics=cohesion_metrics,
            design_patterns=design_patterns,
            anti_patterns=anti_patterns,
            layer_violations=layer_violations,
            interface_segregation=interface_segregation
        )
    
    async def _analyze_dependencies(self, file_analyses: List[FileMetrics]) -> Dict[str, Any]:
        """åˆ†æä¾èµ–å…³ç³»"""
        dependency_graph = {}
        external_dependencies = set()
        internal_dependencies = {}
        
        for metrics in file_analyses:
            file_path = Path(metrics.file_path)
            module_name = file_path.stem
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ç®€åŒ–çš„ä¾èµ–æå–
                imports = re.findall(r'import\s+(\w+)', content)
                from_imports = re.findall(r'from\s+(\w+)', content)
                
                all_deps = imports + from_imports
                dependency_graph[module_name] = all_deps
                
                # åˆ†ç±»å¤–éƒ¨å’Œå†…éƒ¨ä¾èµ–
                for dep in all_deps:
                    if dep.startswith(('os', 'sys', 'json', 'datetime', 'asyncio')):
                        external_dependencies.add(dep)
                    else:
                        if module_name not in internal_dependencies:
                            internal_dependencies[module_name] = []
                        internal_dependencies[module_name].append(dep)
                        
            except Exception as e:
                print(f"âš ï¸ ä¾èµ–åˆ†æå¤±è´¥ {file_path}: {e}")
        
        return {
            "dependency_graph": dependency_graph,
            "external_dependencies": list(external_dependencies),
            "internal_dependencies": internal_dependencies,
            "dependency_metrics": {
                "total_modules": len(file_analyses),
                "total_dependencies": sum(len(deps) for deps in dependency_graph.values()),
                "average_dependencies_per_module": sum(len(deps) for deps in dependency_graph.values()) / max(len(dependency_graph), 1)
            }
        }
    
    async def _detect_anti_patterns(self, file_analyses: List[FileMetrics]) -> List[ScanIssue]:
        """æ£€æµ‹åæ¨¡å¼"""
        anti_patterns = []
        
        for metrics in file_analyses:
            file_path = self.project_root / metrics.file_path
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ£€æµ‹å„ç§åæ¨¡å¼
                file_anti_patterns = await self._detect_file_anti_patterns(content, str(file_path))
                anti_patterns.extend(file_anti_patterns)
                
            except Exception as e:
                print(f"âš ï¸ åæ¨¡å¼æ£€æµ‹å¤±è´¥ {file_path}: {e}")
        
        return anti_patterns
    
    async def _detect_file_anti_patterns(self, content: str, file_path: str) -> List[ScanIssue]:
        """æ£€æµ‹æ–‡ä»¶åæ¨¡å¼"""
        issues = []
        lines = content.split('\n')
        
        # God Classåæ¨¡å¼
        class_count = content.count('class ')
        method_count = content.count('def ')
        if class_count == 1 and method_count > 20:
            issue = ScanIssue(
                file_path=file_path,
                line_number=0,
                issue_type="anti_pattern",
                severity="medium",
                category="god_class",
                description="God Classåæ¨¡å¼ï¼šå•ä¸ªç±»åŒ…å«è¿‡å¤šæ–¹æ³•",
                evidence=f"1ä¸ªç±»åŒ…å«{method_count}ä¸ªæ–¹æ³•",
                recommendation="æ‹†åˆ†ä¸ºå¤šä¸ªèŒè´£å•ä¸€çš„ç±»",
                impact_score=0.7,
                fix_complexity="high",
                references=["Single Responsibility Principle"]
            )
            issues.append(issue)
        
        # Long Methodåæ¨¡å¼
        for i, line in enumerate(lines):
            if 'def ' in line:
                # ç®€åŒ–çš„é•¿æ–¹æ³•æ£€æµ‹
                method_lines = 0
                for j in range(i, len(lines)):
                    if lines[j].strip() and not lines[j].startswith(' '):
                        break
                    method_lines += 1
                
                if method_lines > 50:
                    issue = ScanIssue(
                        file_path=file_path,
                        line_number=i + 1,
                        issue_type="anti_pattern",
                        severity="medium",
                        category="long_method",
                        description="Long Methodåæ¨¡å¼ï¼šæ–¹æ³•è¿‡é•¿",
                        evidence=f"æ–¹æ³•é•¿åº¦: {method_lines}è¡Œ",
                        recommendation="æ‹†åˆ†ä¸ºå¤šä¸ªå°æ–¹æ³•",
                        impact_score=0.5,
                        fix_complexity="medium",
                        references=["Extract Method Refactoring"]
                    )
                    issues.append(issue)
        
        return issues
    
    async def _calculate_comprehensive_metrics(self, file_analyses: List[FileMetrics], 
                                             issues: List[ScanIssue], 
                                             architecture: ArchitectureAnalysis) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆæŒ‡æ ‡"""
        total_files = len(file_analyses)
        
        if total_files == 0:
            return {}
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_complexity = sum(m.cyclomatic_complexity for m in file_analyses) / total_files
        avg_maintainability = sum(m.maintainability_index for m in file_analyses) / total_files
        avg_security = sum(m.security_score for m in file_analyses) / total_files
        avg_performance = sum(m.performance_score for m in file_analyses) / total_files
        avg_architecture = sum(m.architecture_score for m in file_analyses) / total_files
        
        # é—®é¢˜ç»Ÿè®¡
        security_issues = [i for i in issues if i.issue_type == "security"]
        performance_issues = [i for i in issues if i.issue_type == "performance"]
        quality_issues = [i for i in issues if i.issue_type == "quality"]
        
        # ç»¼åˆè¯„åˆ†
        overall_score = (
            avg_security * 0.3 +
            avg_performance * 0.25 +
            avg_architecture * 0.2 +
            (avg_maintainability / 100) * 0.15 +
            (1 - min(avg_complexity / 20, 1)) * 0.1
        )
        
        return {
            "total_files": total_files,
            "average_complexity": avg_complexity,
            "average_maintainability": avg_maintainability,
            "average_security_score": avg_security,
            "average_performance_score": avg_performance,
            "average_architecture_score": avg_architecture,
            "total_issues": len(issues),
            "security_issues_count": len(security_issues),
            "performance_issues_count": len(performance_issues),
            "quality_issues_count": len(quality_issues),
            "overall_quality_score": overall_score,
            "quality_grade": self._calculate_quality_grade(overall_score)
        }
    
    def _calculate_quality_grade(self, score: float) -> str:
        """è®¡ç®—è´¨é‡ç­‰çº§"""
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"
    
    async def _generate_scan_summary(self, python_files: List[Path], 
                                   issues: List[ScanIssue], 
                                   metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰«ææ‘˜è¦"""
        return {
            "scan_overview": {
                "files_scanned": len(python_files),
                "total_issues": len(issues),
                "critical_issues": len([i for i in issues if i.severity == "critical"]),
                "high_issues": len([i for i in issues if i.severity == "high"]),
                "medium_issues": len([i for i in issues if i.severity == "medium"]),
                "low_issues": len([i for i in issues if i.severity == "low"])
            },
            "quality_metrics": metrics,
            "issue_distribution": {
                "by_type": {
                    "security": len([i for i in issues if i.issue_type == "security"]),
                    "performance": len([i for i in issues if i.issue_type == "performance"]),
                    "quality": len([i for i in issues if i.issue_type == "quality"]),
                    "anti_pattern": len([i for i in issues if i.issue_type == "anti_pattern"])
                },
                "by_severity": {
                    "critical": len([i for i in issues if i.severity == "critical"]),
                    "high": len([i for i in issues if i.severity == "high"]),
                    "medium": len([i for i in issues if i.severity == "medium"]),
                    "low": len([i for i in issues if i.severity == "low"])
                }
            },
            "recommendations_priority": {
                "immediate": [i.description for i in issues if i.severity in ["critical", "high"]][:5],
                "short_term": [i.description for i in issues if i.severity == "medium"][:5],
                "long_term": [i.description for i in issues if i.severity == "low"][:5]
            }
        }
    
    async def _generate_comprehensive_recommendations(self, issues: List[ScanIssue]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç»¼åˆæ¨èå»ºè®®"""
        recommendations = []
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        critical_issues = [i for i in issues if i.severity == "critical"]
        high_issues = [i for i in issues if i.severity == "high"]
        medium_issues = [i for i in issues if i.severity == "medium"]
        low_issues = [i for i in issues if i.severity == "low"]
        
        # ç«‹å³è¡ŒåŠ¨å»ºè®®
        if critical_issues:
            recommendations.append({
                "priority": "critical",
                "category": "ç«‹å³è¡ŒåŠ¨",
                "description": "ä¿®å¤æ‰€æœ‰å…³é”®å®‰å…¨é—®é¢˜",
                "items": [i.description for i in critical_issues],
                "estimated_effort": "é«˜",
                "impact": "æ¶ˆé™¤å®‰å…¨é£é™©ï¼Œç¡®ä¿ç³»ç»Ÿå®‰å…¨"
            })
        
        # é«˜ä¼˜å…ˆçº§å»ºè®®
        if high_issues:
            recommendations.append({
                "priority": "high",
                "category": "é«˜ä¼˜å…ˆçº§",
                "description": "å¤„ç†é«˜ä¼˜å…ˆçº§é—®é¢˜",
                "items": [i.description for i in high_issues],
                "estimated_effort": "ä¸­ç­‰",
                "impact": "æ˜¾è‘—æå‡ç³»ç»Ÿè´¨é‡"
            })
        
        # ä¸­æœŸæ”¹è¿›å»ºè®®
        if medium_issues:
            recommendations.append({
                "priority": "medium",
                "category": "ä¸­æœŸæ”¹è¿›",
                "description": "ä¼˜åŒ–æ€§èƒ½å’Œä»£ç è´¨é‡",
                "items": [i.description for i in medium_issues],
                "estimated_effort": "ä¸­ç­‰",
                "impact": "æå‡æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§"
            })
        
        # é•¿æœŸä¼˜åŒ–å»ºè®®
        if low_issues:
            recommendations.append({
                "priority": "low",
                "category": "é•¿æœŸä¼˜åŒ–",
                "description": "æŒç»­æ”¹è¿›å’Œé‡æ„",
                "items": [i.description for i in low_issues],
                "estimated_effort": "ä½",
                "impact": "ä¿æŒä»£ç è´¨é‡"
            })
        
        return recommendations
    
    def _load_security_rules(self) -> List[Dict[str, Any]]:
        """åŠ è½½å®‰å…¨è§„åˆ™"""
        return [
            {
                "type": "pattern",
                "pattern": r'eval\s*\(',
                "severity": "high",
                "category": "dangerous_function",
                "description": "ä½¿ç”¨äº†å±é™©çš„evalå‡½æ•°",
                "recommendation": "é¿å…ä½¿ç”¨evalï¼Œè€ƒè™‘ safer alternatives",
                "impact_score": 0.9,
                "fix_complexity": "medium",
                "references": ["CWE-94"]
            },
            {
                "type": "pattern",
                "pattern": r'exec\s*\(',
                "severity": "high",
                "category": "dangerous_function",
                "description": "ä½¿ç”¨äº†å±é™©çš„execå‡½æ•°",
                "recommendation": "é¿å…ä½¿ç”¨execï¼Œè€ƒè™‘ safer alternatives",
                "impact_score": 0.9,
                "fix_complexity": "medium",
                "references": ["CWE-94"]
            },
            {
                "type": "pattern",
                "pattern": r'password\s*=\s*["\'][^"\']+["\']',
                "severity": "critical",
                "category": "hardcoded_secret",
                "description": "ç¡¬ç¼–ç å¯†ç æˆ–å¯†é’¥",
                "recommendation": "ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶å­˜å‚¨æ•æ„Ÿä¿¡æ¯",
                "impact_score": 1.0,
                "fix_complexity": "low",
                "references": ["CWE-798"]
            },
            {
                "type": "pattern",
                "pattern": r'secret\s*=\s*["\'][^"\']+["\']',
                "severity": "critical",
                "category": "hardcoded_secret",
                "description": "ç¡¬ç¼–ç å¯†é’¥",
                "recommendation": "ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶å­˜å‚¨æ•æ„Ÿä¿¡æ¯",
                "impact_score": 1.0,
                "fix_complexity": "low",
                "references": ["CWE-798"]
            }
        ]
    
    def _load_performance_rules(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ€§èƒ½è§„åˆ™"""
        return [
            {
                "type": "pattern",
                "pattern": r'for\s+\w+\s+in\s+.*:\s*.*\.query\(',
                "severity": "medium",
                "category": "database_in_loop",
                "description": "å¾ªç¯ä¸­æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢",
                "recommendation": "å°†æŸ¥è¯¢ç§»å‡ºå¾ªç¯æˆ–ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢",
                "impact_score": 0.7,
                "fix_complexity": "medium",
                "references": ["Performance Best Practices"]
            },
            {
                "type": "pattern",
                "pattern": r'\.read\(\)\s*$',
                "severity": "medium",
                "category": "large_file_read",
                "description": "ä¸€æ¬¡æ€§è¯»å–å¤§æ–‡ä»¶",
                "recommendation": "ä½¿ç”¨æµå¼è¯»å–æˆ–åˆ†å—å¤„ç†",
                "impact_score": 0.6,
                "fix_complexity": "medium",
                "references": ["Memory Management"]
            }
        ]
    
    def _load_quality_rules(self) -> List[Dict[str, Any]]:
        """åŠ è½½è´¨é‡è§„åˆ™"""
        return [
            # è´¨é‡è§„åˆ™ä¸»è¦é€šè¿‡æŒ‡æ ‡åˆ†æå®ç°
        ]
    
    def _load_architecture_rules(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ¶æ„è§„åˆ™"""
        return [
            # æ¶æ„è§„åˆ™ä¸»è¦é€šè¿‡ä¾èµ–åˆ†æå®ç°
        ]

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°"""
    project_root = "."
    
    scanner = DeepAnalysisScanner(project_root)
    results = await scanner.perform_comprehensive_scan()
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = Path(project_root) / f"deep_scan_results_{timestamp}.json"
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ‰ æ·±åº¦æ‰«æå®Œæˆ!")
    print(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    print(f"ğŸ“Š æ‰«æäº† {results['scan_metadata']['total_files_scanned']} ä¸ªæ–‡ä»¶")
    print(f"ğŸš¨ å‘ç°äº† {len(results['security_issues']) + len(results['performance_issues']) + len(results['quality_issues'])} ä¸ªé—®é¢˜")

if __name__ == "__main__":
    asyncio.run(main())