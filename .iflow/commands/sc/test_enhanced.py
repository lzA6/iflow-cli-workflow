#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆ /sc:test æŒ‡ä»¤å®ç°
æä¾›å…¨é¢çš„é¡¹ç›®æµ‹è¯•ã€åˆ†æå’Œä¼˜åŒ–åŠŸèƒ½
"""

import os
import sys
import json
import time
import subprocess
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib
import difflib
import ast
import re

@dataclass
class TestConfiguration:
    """æµ‹è¯•é…ç½®ç±»"""
    project_root: str
    test_types: List[str]
    coverage_threshold: float = 25.0
    enable_security_scan: bool = True
    enable_performance_test: bool = True
    enable_deep_analysis: bool = True
    interactive_mode: bool = False
    force_ai_awareness: bool = True

@dataclass
class FileInfo:
    """æ–‡ä»¶ä¿¡æ¯ç±»"""
    path: str
    size: int
    modified_time: float
    file_type: str
    functions: List[str]
    classes: List[str]
    imports: List[str]
    complexity_score: float
    dependencies: List[str]
    functionality_description: str
    advantages: List[str]
    disadvantages: List[str]
    retention_reason: Optional[str] = None
    duplicate_check: Optional[str] = None

@dataclass
class ProjectStructure:
    """é¡¹ç›®ç»“æ„ç±»"""
    timestamp: str
    total_files: int
    total_dirs: int
    file_tree: Dict[str, Any]
    file_details: Dict[str, FileInfo]
    module_dependencies: Dict[str, List[str]]
    complexity_metrics: Dict[str, float]

class EnhancedTestEngine:
    """å¢å¼ºç‰ˆæµ‹è¯•å¼•æ“"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.project_root = Path(config.project_root)
        self.test_results = {}
        self.project_structure_before = None
        self.project_structure_after = None
        self.optimization_report = {}
        
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢æµ‹è¯•åˆ†æ"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆ /sc:test å…¨é¢åˆ†æç³»ç»Ÿ")
        print("=" * 60)
        
        # 1. å¼ºåˆ¶AIä¿¡æ¯ä¼ é€’
        if self.config.force_ai_awareness:
            await self._force_ai_awareness()
        
        # 2. è·å–é¡¹ç›®ç»“æ„ï¼ˆæµ‹è¯•å‰ï¼‰
        print("ğŸ“Š åˆ†æé¡¹ç›®ç»“æ„ï¼ˆæµ‹è¯•å‰ï¼‰...")
        self.project_structure_before = await self._analyze_project_structure()
        
        # 3. æ‰§è¡Œæ·±åº¦åˆ†ææ‰«æå®¡æŸ¥
        print("ğŸ” æ‰§è¡Œæ·±åº¦åˆ†ææ‰«æå®¡æŸ¥...")
        scan_results = await self._perform_deep_analysis_scan()
        
        # 4. è¿è¡Œæµ‹è¯•å¥—ä»¶
        print("ğŸ§ª æ‰§è¡Œæµ‹è¯•å¥—ä»¶...")
        test_results = await self._run_test_suite()
        
        # 5. å®‰å…¨æ‰«æ
        if self.config.enable_security_scan:
            print("ğŸ›¡ï¸ æ‰§è¡Œå®‰å…¨æ‰«æ...")
            security_results = await self._perform_security_scan()
        else:
            security_results = {"status": "skipped"}
        
        # 6. æ€§èƒ½æµ‹è¯•
        if self.config.enable_performance_test:
            print("âš¡ æ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
            performance_results = await self._perform_performance_test()
        else:
            performance_results = {"status": "skipped"}
        
        # 7. è·å–é¡¹ç›®ç»“æ„ï¼ˆæµ‹è¯•åï¼‰
        print("ğŸ“Š åˆ†æé¡¹ç›®ç»“æ„ï¼ˆæµ‹è¯•åï¼‰...")
        self.project_structure_after = await self._analyze_project_structure()
        
        # 8. ç”Ÿæˆç»“æ„å¯¹æ¯”åˆ†æ
        print("ğŸ”„ ç”Ÿæˆé¡¹ç›®ç»“æ„å¯¹æ¯”åˆ†æ...")
        structure_comparison = await self._compare_project_structures()
        
        # 9. è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        print("ğŸ“ˆ è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")
        self.optimization_report = await self._generate_optimization_report(
            test_results, security_results, performance_results, 
            scan_results, structure_comparison
        )
        
        # 10. äº¤äº’å¼å¤„ç†
        if self.config.interactive_mode:
            await self._interactive_analysis()
        
        # 11. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = await self._generate_final_report()
        
        print("âœ… å¢å¼ºç‰ˆ /sc:test åˆ†æå®Œæˆï¼")
        return final_report
    
    async def _force_ai_awareness(self):
        """å¼ºåˆ¶AIä¿¡æ¯ä¼ é€’"""
        print("ğŸ¤– å¼ºåˆ¶AIä¿¡æ¯ä¼ é€’ç³»ç»Ÿå¯åŠ¨...")
        
        ai_context = {
            "project_name": "iFlow CLI V16 Quantum Evolution",
            "project_root": str(self.project_root),
            "timestamp": datetime.now().isoformat(),
            "test_objectives": [
                "å…¨é¢æµ‹è¯•è¦†ç›–åˆ†æ",
                "æ·±åº¦ä»£ç è´¨é‡å®¡æŸ¥",
                "å®‰å…¨æ€§æ¼æ´æ‰«æ",
                "æ€§èƒ½åŸºå‡†æµ‹è¯•",
                "é¡¹ç›®ç»“æ„ä¼˜åŒ–åˆ†æ",
                "æ–‡ä»¶åŠŸèƒ½ç‰¹ç‚¹è¯„ä¼°",
                "ä¿ç•™/åˆ é™¤å†³ç­–ä¾æ®"
            ]
        }
    
    async def run_compatible_test(self, 
                                 target: Optional[str] = None,
                                 test_type: str = "all",
                                 enable_coverage: bool = True,
                                 watch_mode: bool = False,
                                 auto_fix: bool = False) -> Dict[str, Any]:
        """è¿è¡Œå…¼å®¹æ¨¡å¼æµ‹è¯•ï¼ˆåŸå§‹ç‰ˆæœ¬åŠŸèƒ½ï¼‰"""
        print("ğŸ”„ è¿è¡Œå…¼å®¹æ¨¡å¼æµ‹è¯•")
        print("=" * 60)
        
        # 1. å‘ç°å’Œé…ç½®æµ‹è¯•
        print("ğŸ” å‘ç°æµ‹è¯•é…ç½®...")
        test_config = await self._discover_test_configuration(target, test_type)
        
        # 2. æ‰§è¡Œæµ‹è¯•
        print("ğŸ§ª æ‰§è¡Œæµ‹è¯•...")
        test_results = await self._run_test_suite(target, test_type, enable_coverage)
        
        # 3. åˆ†ææµ‹è¯•ç»“æœ
        print("ğŸ“Š åˆ†ææµ‹è¯•ç»“æœ...")
        analysis_results = await self._analyze_test_results(test_results)
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        print("ğŸ“‹ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        report = await self._generate_test_report(test_results, analysis_results)
        
        # 5. å¤„ç†ç›‘è§†æ¨¡å¼
        if watch_mode:
            print("ğŸ‘ï¸ å¯åŠ¨ç›‘è§†æ¨¡å¼...")
            await self._start_watch_mode(target, test_type, enable_coverage, auto_fix)
        
        # 6. è‡ªåŠ¨ä¿®å¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if auto_fix and test_results.get("failed", 0) > 0:
            print("ğŸ”§ å°è¯•è‡ªåŠ¨ä¿®å¤...")
            fix_results = await self._attempt_auto_fix(test_results)
            report["auto_fix_results"] = fix_results
        
        return {
            "mode": "compatible",
            "test_results": test_results,
            "analysis_results": analysis_results,
            "report": report
        }
    
    async def _discover_test_configuration(self, target: Optional[str], test_type: str) -> Dict[str, Any]:
        """å‘ç°æµ‹è¯•é…ç½®ï¼ˆåŸå§‹ç‰ˆæœ¬åŠŸèƒ½ï¼‰"""
        config = {
            "test_framework": "pytest",
            "test_paths": [],
            "test_markers": [],
            "coverage_config": {}
        }
        
        # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
        if target:
            target_path = self.project_root / target
            if target_path.exists():
                config["test_paths"].append(str(target_path))
        else:
            # è‡ªåŠ¨å‘ç°æµ‹è¯•ç›®å½•
            test_dirs = ["tests", "test", "src/tests"]
            for test_dir in test_dirs:
                test_path = self.project_root / test_dir
                if test_path.exists():
                    config["test_paths"].append(str(test_path))
        
        # è®¾ç½®æµ‹è¯•æ ‡è®°
        if test_type == "unit":
            config["test_markers"] = ["unit"]
        elif test_type == "integration":
            config["test_markers"] = ["integration"]
        elif test_type == "e2e":
            config["test_markers"] = ["e2e"]
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_files = ["pyproject.toml", "setup.cfg", "pytest.ini"]
        for config_file in config_files:
            config_path = self.project_root / config_file
            if config_path.exists():
                config["config_file"] = str(config_path)
                break
        
        return config
    
    async def _analyze_test_results(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœï¼ˆåŸå§‹ç‰ˆæœ¬åŠŸèƒ½å¢å¼ºï¼‰"""
        analysis = {
            "success_rate": 0,
            "coverage_adequacy": "unknown",
            "performance_issues": [],
            "recommendations": []
        }
        
        total_tests = test_results.get("passed", 0) + test_results.get("failed", 0) + test_results.get("skipped", 0)
        if total_tests > 0:
            analysis["success_rate"] = (test_results.get("passed", 0) / total_tests) * 100
        
        # åˆ†æè¦†ç›–ç‡
        coverage = test_results.get("coverage", {})
        if coverage:
            coverage_pct = coverage.get("percent_covered", 0)
            if coverage_pct >= self.config.coverage_threshold:
                analysis["coverage_adequacy"] = "adequate"
            else:
                analysis["coverage_adequacy"] = "inadequate"
                analysis["recommendations"].append(
                    f"è¦†ç›–ç‡ {coverage_pct:.1f}% ä½äºé˜ˆå€¼ {self.config.coverage_threshold}%"
                )
        
        # ç”Ÿæˆå»ºè®®
        if test_results.get("failed", 0) > 0:
            analysis["recommendations"].append("æœ‰æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æµ‹è¯•ä»£ç ")
        
        if test_results.get("execution_time", 0) > 60:
            analysis["performance_issues"].append("æµ‹è¯•æ‰§è¡Œæ—¶é—´è¿‡é•¿")
        
        return analysis
    
    async def _generate_test_report(self, test_results: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šï¼ˆåŸå§‹ç‰ˆæœ¬åŠŸèƒ½ï¼‰"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": test_results.get("passed", 0) + test_results.get("failed", 0) + test_results.get("skipped", 0),
                "passed": test_results.get("passed", 0),
                "failed": test_results.get("failed", 0),
                "skipped": test_results.get("skipped", 0),
                "success_rate": analysis["success_rate"],
                "execution_time": test_results.get("execution_time", 0)
            },
            "coverage": test_results.get("coverage", {}),
            "recommendations": analysis["recommendations"],
            "failure_analysis": test_results.get("failure_analysis")
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.project_root / "reports" / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report
    
    async def _start_watch_mode(self, target: Optional[str], test_type: str, enable_coverage: bool, auto_fix: bool):
        """å¯åŠ¨ç›‘è§†æ¨¡å¼ï¼ˆåŸå§‹ç‰ˆæœ¬åŠŸèƒ½ï¼‰"""
        print("ğŸ‘ï¸ ç›‘è§†æ¨¡å¼å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢...")
        
        try:
            import time
            from watchdog.observers import Observer
            from watchdog.events import FileSystemEventHandler
            
            class TestFileHandler(FileSystemEventHandler):
                def __init__(self, callback):
                    self.callback = callback
                
                def on_modified(self, event):
                    if event.src_path.endswith('.py'):
                        print(f"ğŸ“ æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´: {event.src_path}")
                        self.callback()
            
            def run_tests():
                asyncio.create_task(
                    self._run_test_suite(target, test_type, enable_coverage)
                )
            
            observer = Observer()
            handler = TestFileHandler(run_tests)
            
            # ç›‘è§†æºä»£ç ç›®å½•
            watch_dirs = ["src", ".iflow"]
            for watch_dir in watch_dirs:
                watch_path = self.project_root / watch_dir
                if watch_path.exists():
                    observer.schedule(handler, str(watch_path), recursive=True)
            
            observer.start()
            
            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                observer.stop()
            
            observer.join()
            
        except ImportError:
            print("âš ï¸ éœ€è¦å®‰è£… watchdog åº“æ¥ä½¿ç”¨ç›‘è§†æ¨¡å¼: pip install watchdog")
    
    async def _attempt_auto_fix(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """å°è¯•è‡ªåŠ¨ä¿®å¤ï¼ˆåŸå§‹ç‰ˆæœ¬åŠŸèƒ½ï¼‰"""
        fix_results = {
            "attempted_fixes": 0,
            "successful_fixes": 0,
            "fix_details": []
        }
        
        # ç®€å•çš„è‡ªåŠ¨ä¿®å¤é€»è¾‘
        failure_analysis = test_results.get("failure_analysis", {})
        for pattern in failure_analysis.get("failure_patterns", []):
            if "ImportError" in pattern.get("error", ""):
                # å°è¯•ä¿®å¤å¯¼å…¥é”™è¯¯
                fix_results["attempted_fixes"] += 1
                # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„ä¿®å¤é€»è¾‘
                fix_results["fix_details"].append({
                    "type": "import_error",
                    "target": pattern.get("module"),
                    "status": "identified"
                })
        
        return fix_results
    
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢æµ‹è¯•åˆ†æ"""
        analysis_config = {
            "critical_requirements": [
                "æ¯ä¸€æ­¥éƒ½å¿…é¡»æä¾›å®Œæ•´ä¾æ®å’Œè§£é‡Š",
                "æ‰€æœ‰æ–‡ä»¶å†³ç­–éƒ½éœ€è¦è¯¦ç»†æ¨ç†è¿‡ç¨‹",
                "åŠŸèƒ½ç‰¹ç‚¹å’Œä¼˜ç¼ºç‚¹å¿…é¡»æ˜ç¡®åˆ—å‡º",
                "åˆ é™¤æ–‡ä»¶å¿…é¡»æœ‰å……åˆ†ç†ç”±",
                "ä¿ç•™æ–‡ä»¶éœ€è¦è¯´æ˜å…¶ç‹¬ç‰¹ä»·å€¼"
            ],
            "project_structure": await self._get_basic_structure(),
            "test_configuration": asdict(self.config)
        }
        
        # ä¿å­˜AIä¸Šä¸‹æ–‡åˆ°æ–‡ä»¶
        context_file = self.project_root / ".iflow" / "temp_docs" / "ai_context.json"
        context_file.parent.mkdir(exist_ok=True)
        with open(context_file, 'w', encoding='utf-8') as f:
            json.dump(ai_context, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… AIä¸Šä¸‹æ–‡å·²ä¿å­˜åˆ°: {context_file}")
        print("ğŸ¯ AIå·²å¼ºåˆ¶æ¥æ”¶é¡¹ç›®å®Œæ•´ä¿¡æ¯å’Œæµ‹è¯•è¦æ±‚")
    
    async def _analyze_project_structure(self) -> ProjectStructure:
        """åˆ†æé¡¹ç›®ç»“æ„"""
        structure = {
            "timestamp": datetime.now().isoformat(),
            "total_files": 0,
            "total_dirs": 0,
            "file_tree": {},
            "file_details": {},
            "module_dependencies": {},
            "complexity_metrics": {}
        }
        
        file_details = {}
        dependencies = {}
        complexity_metrics = {}
        
        # éå†é¡¹ç›®æ–‡ä»¶
        for root, dirs, files in os.walk(self.project_root):
            # è·³è¿‡ç‰¹å®šç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules']]
            
            rel_root = os.path.relpath(root, self.project_root)
            if rel_root == '.':
                rel_root = 'root'
            
            structure["file_tree"][rel_root] = {
                "dirs": dirs.copy(),
                "files": files.copy()
            }
            
            structure["total_dirs"] += len(dirs)
            structure["total_files"] += len(files)
            
            # åˆ†ææ¯ä¸ªæ–‡ä»¶
            for file in files:
                if file.endswith('.py'):
                    file_path = Path(root) / file
                    rel_path = str(file_path.relative_to(self.project_root))
                    
                    try:
                        file_info = await self._analyze_python_file(file_path)
                        file_details[rel_path] = file_info
                        
                        # åˆ†æä¾èµ–å…³ç³»
                        deps = await self._analyze_dependencies(file_path)
                        dependencies[rel_path] = deps
                        
                        # è®¡ç®—å¤æ‚åº¦
                        complexity = await self._calculate_complexity(file_path)
                        complexity_metrics[rel_path] = complexity
                        
                    except Exception as e:
                        print(f"âš ï¸ åˆ†ææ–‡ä»¶å¤±è´¥ {rel_path}: {e}")
        
        structure["file_details"] = file_details
        structure["module_dependencies"] = dependencies
        structure["complexity_metrics"] = complexity_metrics
        
        return ProjectStructure(**structure)
    
    async def _analyze_python_file(self, file_path: Path) -> FileInfo:
        """åˆ†æPythonæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            # æå–å‡½æ•°å’Œç±»
            functions = []
            classes = []
            imports = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    functions.append(node.name)
                elif isinstance(node, ast.ClassDef):
                    classes.append(node.name)
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    if isinstance(node, ast.Import):
                        imports.extend([alias.name for alias in node.names])
                    else:
                        module = node.module or ""
                        imports.extend([f"{module}.{alias.name}" for alias in node.names])
            
            # åˆ†æåŠŸèƒ½ç‰¹ç‚¹
            functionality = await self._analyze_functionality(content, file_path.name)
            
            # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
            complexity_score = len(functions) + len(classes) * 2 + len(imports) * 0.5
            
            return FileInfo(
                path=str(file_path.relative_to(self.project_root)),
                size=file_path.stat().st_size,
                modified_time=file_path.stat().st_mtime,
                file_type='python',
                functions=functions,
                classes=classes,
                imports=imports,
                complexity_score=complexity_score,
                dependencies=[],  # å°†åœ¨åç»­å¡«å……
                functionality_description=functionality["description"],
                advantages=functionality["advantages"],
                disadvantages=functionality["disadvantages"]
            )
            
        except Exception as e:
            return FileInfo(
                path=str(file_path.relative_to(self.project_root)),
                size=file_path.stat().st_size,
                modified_time=file_path.stat().st_mtime,
                file_type='python',
                functions=[],
                classes=[],
                imports=[],
                complexity_score=0.0,
                dependencies=[],
                functionality_description=f"åˆ†æå¤±è´¥: {e}",
                advantages=[],
                disadvantages=["æ— æ³•åˆ†ææ–‡ä»¶å†…å®¹"]
            )
    
    async def _analyze_functionality(self, content: str, filename: str) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶åŠŸèƒ½ç‰¹ç‚¹"""
        advantages = []
        disadvantages = []
        description = "é€šç”¨Pythonæ¨¡å—"
        
        # åŸºäºæ–‡ä»¶åå’Œå†…å®¹åˆ†æåŠŸèƒ½
        if 'test' in filename.lower():
            description = "æµ‹è¯•æ¨¡å—"
            advantages.append("ç¡®ä¿ä»£ç è´¨é‡")
            advantages.append("é˜²æ­¢å›å½’é”™è¯¯")
            disadvantages.append("éœ€è¦ç»´æŠ¤æˆæœ¬")
        
        elif 'engine' in filename.lower():
            description = "æ ¸å¿ƒå¼•æ“æ¨¡å—"
            advantages.append("ç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½")
            advantages.append("é«˜æ€§èƒ½å¤„ç†")
            disadvantages.append("å¤æ‚åº¦é«˜")
            disadvantages.append("ä¾èµ–æ€§å¼º")
        
        elif 'cache' in filename.lower():
            description = "ç¼“å­˜ç³»ç»Ÿæ¨¡å—"
            advantages.append("æå‡æ€§èƒ½")
            advantages.append("å‡å°‘é‡å¤è®¡ç®—")
            disadvantages.append("å†…å­˜å ç”¨")
            disadvantages.append("æ•°æ®ä¸€è‡´æ€§é—®é¢˜")
        
        elif 'security' in filename.lower():
            description = "å®‰å…¨ç›¸å…³æ¨¡å—"
            advantages.append("ç³»ç»Ÿå®‰å…¨æ€§")
            advantages.append("é˜²æŠ¤æœºåˆ¶")
            disadvantages.append("æ€§èƒ½å¼€é”€")
            disadvantages.append("é…ç½®å¤æ‚")
        
        # åŸºäºå†…å®¹åˆ†æ
        if 'class' in content and 'def' in content:
            advantages.append("é¢å‘å¯¹è±¡è®¾è®¡")
        if 'async def' in content:
            advantages.append("å¼‚æ­¥å¤„ç†èƒ½åŠ›")
            disadvantages.append("è°ƒè¯•å¤æ‚åº¦å¢åŠ ")
        if 'import' in content:
            advantages.append("æ¨¡å—åŒ–è®¾è®¡")
            disadvantages.append("å¤–éƒ¨ä¾èµ–é£é™©")
        
        return {
            "description": description,
            "advantages": advantages,
            "disadvantages": disadvantages
        }
    
    async def _analyze_dependencies(self, file_path: Path) -> List[str]:
        """åˆ†ææ–‡ä»¶ä¾èµ–å…³ç³»"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            dependencies = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        dependencies.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    module = node.module or ""
                    for alias in node.names:
                        dependencies.append(f"{module}.{alias.name}")
            
            return dependencies
            
        except Exception:
            return []
    
    async def _calculate_complexity(self, file_path: Path) -> float:
        """è®¡ç®—æ–‡ä»¶å¤æ‚åº¦"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            code_lines = len([line for line in lines if line.strip() and not line.strip().startswith('#')])
            
            tree = ast.parse(content)
            
            # è®¡ç®—åœˆå¤æ‚åº¦
            complexity = 1  # åŸºç¡€å¤æ‚åº¦
            
            for node in ast.walk(tree):
                if isinstance(node, (ast.If, ast.While, ast.For, ast.With)):
                    complexity += 1
                elif isinstance(node, ast.ExceptHandler):
                    complexity += 1
                elif isinstance(node, ast.BoolOp):
                    complexity += len(node.values) - 1
            
            return float(complexity + code_lines * 0.1)
            
        except Exception:
            return 0.0
    
    async def _perform_deep_analysis_scan(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ·±åº¦åˆ†ææ‰«æå®¡æŸ¥"""
        print("ğŸ”¬ æ‰§è¡Œæ·±åº¦ä»£ç åˆ†æ...")
        
        scan_results = {
            "timestamp": datetime.now().isoformat(),
            "total_files_scanned": 0,
            "issues_found": [],
            "recommendations": [],
            "code_quality_metrics": {},
            "duplicate_analysis": {},
            "unused_imports": {},
            "security_patterns": {},
            "performance_bottlenecks": []
        }
        
        # æ‰«ææ‰€æœ‰Pythonæ–‡ä»¶
        python_files = list(self.project_root.rglob("*.py"))
        scan_results["total_files_scanned"] = len(python_files)
        
        for file_path in python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ£€æŸ¥ä»£ç è´¨é‡é—®é¢˜
                issues = await self._check_code_quality(content, file_path)
                scan_results["issues_found"].extend(issues)
                
                # æ£€æŸ¥é‡å¤ä»£ç 
                duplicates = await self._check_duplicate_code(content, file_path)
                if duplicates:
                    scan_results["duplicate_analysis"][str(file_path.relative_to(self.project_root))] = duplicates
                
                # æ£€æŸ¥æœªä½¿ç”¨çš„å¯¼å…¥
                unused_imports = await self._check_unused_imports(content, file_path)
                if unused_imports:
                    scan_results["unused_imports"][str(file_path.relative_to(self.project_root))] = unused_imports
                
                # æ£€æŸ¥å®‰å…¨æ¨¡å¼
                security_issues = await self._check_security_patterns(content, file_path)
                if security_issues:
                    scan_results["security_patterns"][str(file_path.relative_to(self.project_root))] = security_issues
                
                # æ£€æŸ¥æ€§èƒ½ç“¶é¢ˆ
                perf_issues = await self._check_performance_bottlenecks(content, file_path)
                scan_results["performance_bottlenecks"].extend(perf_issues)
                
            except Exception as e:
                scan_results["issues_found"].append({
                    "file": str(file_path.relative_to(self.project_root)),
                    "type": "scan_error",
                    "message": f"æ‰«æé”™è¯¯: {e}",
                    "severity": "medium"
                })
        
        # ç”Ÿæˆæ¨èå»ºè®®
        scan_results["recommendations"] = await self._generate_recommendations(scan_results)
        
        return scan_results
    
    async def _check_code_quality(self, content: str, file_path: Path) -> List[Dict[str, Any]]:
        """æ£€æŸ¥ä»£ç è´¨é‡"""
        issues = []
        lines = content.split('\n')
        
        for i, line in enumerate(lines, 1):
            # æ£€æŸ¥è¡Œé•¿åº¦
            if len(line) > 120:
                issues.append({
                    "file": str(file_path.relative_to(self.project_root)),
                    "line": i,
                    "type": "line_too_long",
                    "message": f"è¡Œé•¿åº¦è¶…è¿‡120å­—ç¬¦ ({len(line)}å­—ç¬¦)",
                    "severity": "low"
                })
            
            # æ£€æŸ¥TODOæ³¨é‡Š
            if 'TODO' in line or 'FIXME' in line:
                issues.append({
                    "file": str(file_path.relative_to(self.project_root)),
                    "line": i,
                    "type": "todo_comment",
                    "message": "å­˜åœ¨å¾…åŠäº‹é¡¹æ³¨é‡Š",
                    "severity": "medium"
                })
            
            # æ£€æŸ¥è°ƒè¯•ä»£ç 
            if 'print(' in line and 'debug' not in line.lower():
                issues.append({
                    "file": str(file_path.relative_to(self.project_root)),
                    "line": i,
                    "type": "debug_print",
                    "message": "å¯èƒ½å­˜åœ¨è°ƒè¯•ä»£ç ",
                    "severity": "medium"
                })
        
        return issues
    
    async def _check_duplicate_code(self, content: str, file_path: Path) -> List[Dict[str, Any]]:
        """æ£€æŸ¥é‡å¤ä»£ç """
        duplicates = []
        
        # ç®€å•çš„é‡å¤ä»£ç æ£€æµ‹
        lines = content.split('\n')
        line_groups = {}
        
        for i, line in enumerate(lines):
            clean_line = line.strip()
            if len(clean_line) > 20:  # åªæ£€æŸ¥è¾ƒé•¿çš„è¡Œ
                if clean_line not in line_groups:
                    line_groups[clean_line] = []
                line_groups[clean_line].append(i + 1)
        
        for line, line_numbers in line_groups.items():
            if len(line_numbers) > 1:
                duplicates.append({
                    "content": line,
                    "lines": line_numbers,
                    "type": "exact_duplicate"
                })
        
        return duplicates
    
    async def _check_unused_imports(self, content: str, file_path: Path) -> List[str]:
        """æ£€æŸ¥æœªä½¿ç”¨çš„å¯¼å…¥"""
        try:
            tree = ast.parse(content)
            imports = []
            
            # è·å–æ‰€æœ‰å¯¼å…¥
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    module = node.module or ""
                    for alias in node.names:
                        imports.append(f"{module}.{alias.name}")
            
            # ç®€å•æ£€æŸ¥ï¼šå¦‚æœå¯¼å…¥åœ¨ä»£ç ä¸­æ²¡æœ‰å‡ºç°ï¼Œåˆ™è®¤ä¸ºæœªä½¿ç”¨
            unused = []
            for imp in imports:
                name = imp.split('.')[-1]
                if name not in content.replace(f"import {imp}", ""):
                    unused.append(imp)
            
            return unused
            
        except Exception:
            return []
    
    async def _check_security_patterns(self, content: str, file_path: Path) -> List[Dict[str, Any]]:
        """æ£€æŸ¥å®‰å…¨æ¨¡å¼"""
        security_issues = []
        
        # æ£€æŸ¥å±é™©å‡½æ•°
        dangerous_functions = ['eval', 'exec', 'compile', '__import__']
        for func in dangerous_functions:
            if f"{func}(" in content:
                security_issues.append({
                    "type": "dangerous_function",
                    "function": func,
                    "message": f"ä½¿ç”¨äº†å±é™©å‡½æ•°: {func}",
                    "severity": "high"
                })
        
        # æ£€æŸ¥ç¡¬ç¼–ç å¯†ç 
        password_patterns = [
            r'password\s*=\s*["\'][^"\']+["\']',
            r'pwd\s*=\s*["\'][^"\']+["\']',
            r'secret\s*=\s*["\'][^"\']+["\']'
        ]
        
        for pattern in password_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                security_issues.append({
                    "type": "hardcoded_secret",
                    "message": "å¯èƒ½å­˜åœ¨ç¡¬ç¼–ç å¯†ç æˆ–å¯†é’¥",
                    "severity": "high"
                })
        
        return security_issues
    
    async def _check_performance_bottlenecks(self, content: str, file_path: Path) -> List[Dict[str, Any]]:
        """æ£€æŸ¥æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # æ£€æŸ¥å¾ªç¯ä¸­çš„æ•°æ®åº“æŸ¥è¯¢
        if re.search(r'for.*in.*:.*query', content, re.IGNORECASE):
            bottlenecks.append({
                "file": str(file_path.relative_to(self.project_root)),
                "type": "query_in_loop",
                "message": "å¾ªç¯ä¸­å¯èƒ½å­˜åœ¨æ•°æ®åº“æŸ¥è¯¢",
                "severity": "medium"
            })
        
        # æ£€æŸ¥å¤§æ–‡ä»¶è¯»å–
        if 'file.read()' in content and 'with open' in content:
            bottlenecks.append({
                "file": str(file_path.relative_to(self.project_root)),
                "type": "large_file_read",
                "message": "å¯èƒ½å­˜åœ¨å¤§æ–‡ä»¶ä¸€æ¬¡æ€§è¯»å–",
                "severity": "medium"
            })
        
        return bottlenecks
    
    async def _generate_recommendations(self, scan_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ¨èå»ºè®®"""
        recommendations = []
        
        # åŸºäºå‘ç°çš„é—®é¢˜ç”Ÿæˆå»ºè®®
        if len(scan_results["issues_found"]) > 10:
            recommendations.append("å»ºè®®ä¼˜å…ˆä¿®å¤é«˜ä¸¥é‡æ€§é—®é¢˜")
        
        if len(scan_results["duplicate_analysis"]) > 5:
            recommendations.append("å‘ç°è¾ƒå¤šé‡å¤ä»£ç ï¼Œå»ºè®®é‡æ„å…¬å…±å‡½æ•°")
        
        if len(scan_results["unused_imports"]) > 3:
            recommendations.append("æ¸…ç†æœªä½¿ç”¨çš„å¯¼å…¥ä»¥æå‡ä»£ç è´¨é‡")
        
        if len(scan_results["security_patterns"]) > 0:
            recommendations.append("å­˜åœ¨å®‰å…¨é—®é¢˜ï¼Œéœ€è¦ç«‹å³å¤„ç†")
        
        if len(scan_results["performance_bottlenecks"]) > 0:
            recommendations.append("å‘ç°æ€§èƒ½ç“¶é¢ˆï¼Œå»ºè®®ä¼˜åŒ–")
        
        return recommendations
    
    async def _run_test_suite(self, target: Optional[str] = None, test_type: str = "all", enable_coverage: bool = True) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•å¥—ä»¶ï¼ˆå…¼å®¹åŸå§‹ç‰ˆæœ¬åŠŸèƒ½ï¼‰"""
        print(f"ğŸ§ª æ‰§è¡Œæµ‹è¯•å¥—ä»¶: {test_type if target else 'å…¨éƒ¨'}")
        
        test_results = {
            "timestamp": datetime.now().isoformat(),
            "target": target,
            "test_type": test_type,
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "coverage": {},
            "test_details": [],
            "execution_time": 0
        }
        
        try:
            # æ„å»ºpytestå‘½ä»¤
            start_time = time.time()
            
            cmd = [sys.executable, "-m", "pytest"]
            
            # æ·»åŠ ç›®æ ‡è·¯å¾„
            if target:
                cmd.append(target)
            
            # æ·»åŠ æµ‹è¯•ç±»å‹è¿‡æ»¤å™¨
            if test_type == "unit":
                cmd.extend(["-m", "unit"])
            elif test_type == "integration":
                cmd.extend(["-m", "integration"])
            elif test_type == "e2e":
                cmd.extend(["-m", "e2e"])
                # å¯¹äºE2Eæµ‹è¯•ï¼Œæ¿€æ´»Playwright MCP
                print("ğŸŒ æ¿€æ´»Playwright MCPè¿›è¡Œç«¯åˆ°ç«¯æµè§ˆå™¨æµ‹è¯•")
            
            # æ·»åŠ è¦†ç›–ç‡é€‰é¡¹
            if enable_coverage:
                cmd.extend([
                    "--cov=.iflow/core",
                    "--cov-report=json",
                    "--cov-report=term-missing",
                    "--cov-report=html",
                    f"--cov-fail-under={self.config.coverage_threshold}"
                ])
            
            # æ·»åŠ å…¶ä»–é€‰é¡¹
            cmd.extend([
                "--tb=short",
                "-v",
                "--maxfail=5"
            ])
            
            # æ‰§è¡Œæµ‹è¯•
            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            execution_time = time.time() - start_time
            test_results["execution_time"] = execution_time
            
            # è§£ææµ‹è¯•ç»“æœ
            output = result.stdout
            error_output = result.stderr
            
            # æå–æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
            import re
            passed_match = re.search(r'(\d+)\s+passed', output)
            failed_match = re.search(r'(\d+)\s+failed', output)
            skipped_match = re.search(r'(\d+)\s+skipped', output)
            
            if passed_match:
                test_results["passed"] = int(passed_match.group(1))
            if failed_match:
                test_results["failed"] = int(failed_match.group(1))
            if skipped_match:
                test_results["skipped"] = int(skipped_match.group(1))
            
            # è¯»å–è¦†ç›–ç‡æŠ¥å‘Š
            if enable_coverage:
                coverage_file = self.project_root / "coverage.json"
                if coverage_file.exists():
                    with open(coverage_file, 'r') as f:
                        coverage_data = json.load(f)
                        test_results["coverage"] = coverage_data.get("totals", {})
                
                # ç”ŸæˆHTMLè¦†ç›–ç‡æŠ¥å‘Š
                html_dir = self.project_root / "htmlcov"
                if html_dir.exists():
                    test_results["coverage_report"] = str(html_dir)
            
            test_results["test_details"] = output.split('\n')
            test_results["error_details"] = error_output.split('\n') if error_output else []
            
            # æ™ºèƒ½æµ‹è¯•å¤±è´¥åˆ†æ
            if test_results["failed"] > 0:
                test_results["failure_analysis"] = await self._analyze_test_failures(output)
            
        except subprocess.TimeoutExpired:
            test_results["error"] = "æµ‹è¯•æ‰§è¡Œè¶…æ—¶"
        except Exception as e:
            test_results["error"] = f"æµ‹è¯•æ‰§è¡Œé”™è¯¯: {e}"
        
        return test_results
    
    async def _analyze_test_failures(self, test_output: str) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•å¤±è´¥åŸå› ï¼ˆåŸå§‹ç‰ˆæœ¬åŠŸèƒ½å¢å¼ºï¼‰"""
        failure_analysis = {
            "total_failures": 0,
            "failure_patterns": [],
            "recommendations": [],
            "common_errors": []
        }
        
        # æå–å¤±è´¥æµ‹è¯•ä¿¡æ¯
        import re
        failure_pattern = r'FAILED\s+(.*?)::(.*?)\s*-\s*(.*)'
        failures = re.findall(failure_pattern, test_output)
        
        failure_analysis["total_failures"] = len(failures)
        
        # åˆ†æå¤±è´¥æ¨¡å¼
        for module, test, error in failures:
            failure_analysis["failure_patterns"].append({
                "module": module,
                "test": test,
                "error": error.strip()
            })
            
            # ç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
            if "ImportError" in error:
                failure_analysis["recommendations"].append(
                    f"æ£€æŸ¥ {module} çš„å¯¼å…¥ä¾èµ–"
                )
            elif "AssertionError" in error:
                failure_analysis["recommendations"].append(
                    f"æ£€æŸ¥ {test} çš„æ–­è¨€é€»è¾‘"
                )
            elif "Timeout" in error:
                failure_analysis["recommendations"].append(
                    f"ä¼˜åŒ– {test} çš„æ‰§è¡Œæ—¶é—´"
                )
        
        # è¯†åˆ«å¸¸è§é”™è¯¯
        common_errors = re.findall(r'(ImportError|AttributeError|TypeError|ValueError|AssertionError)', test_output)
        failure_analysis["common_errors"] = list(set(common_errors))
        
        return failure_analysis
    
    async def _perform_security_scan(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®‰å…¨æ‰«æ"""
        print("ğŸ›¡ï¸ æ‰§è¡Œå®‰å…¨æ‰«æ...")
        
        security_results = {
            "timestamp": datetime.now().isoformat(),
            "scan_tool": "bandit",
            "total_issues": 0,
            "high_severity": 0,
            "medium_severity": 0,
            "low_severity": 0,
            "issues": []
        }
        
        try:
            # è¿è¡Œbanditå®‰å…¨æ‰«æ
            cmd = [
                sys.executable, "-m", "bandit",
                "-r", ".iflow/core",
                "-f", "json",
                "-q"
            ]
            
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=120
            )
            
            if result.returncode == 0:
                bandit_data = json.loads(result.stdout)
                security_results["issues"] = bandit_data.get("results", [])
                security_results["total_issues"] = len(security_results["issues"])
                
                # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
                for issue in security_results["issues"]:
                    severity = issue.get("issue_severity", "LOW")
                    if severity == "HIGH":
                        security_results["high_severity"] += 1
                    elif severity == "MEDIUM":
                        security_results["medium_severity"] += 1
                    else:
                        security_results["low_severity"] += 1
            
        except Exception as e:
            security_results["error"] = f"å®‰å…¨æ‰«æé”™è¯¯: {e}"
        
        return security_results
    
    async def _perform_performance_test(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ€§èƒ½æµ‹è¯•"""
        print("âš¡ æ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
        
        performance_results = {
            "timestamp": datetime.now().isoformat(),
            "memory_usage": {},
            "execution_times": {},
            "bottlenecks": []
        }
        
        try:
            # æµ‹è¯•å†…å­˜ä½¿ç”¨
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            performance_results["memory_usage"] = {
                "rss": memory_info.rss,
                "vms": memory_info.vms,
                "percent": process.memory_percent()
            }
            
            # æµ‹è¯•å…³é”®æ¨¡å—æ‰§è¡Œæ—¶é—´
            key_modules = [
                ".iflow/core/arq_engine_v16_1.py",
                ".iflow/core/hrrk_kernel_v3_enterprise.py",
                ".iflow/core/refrag_system_v6.py"
            ]
            
            for module in key_modules:
                module_path = self.project_root / module
                if module_path.exists():
                    start_time = time.time()
                    try:
                        # ç®€å•çš„å¯¼å…¥æ—¶é—´æµ‹è¯•
                        spec = importlib.util.spec_from_file_location("test_module", module_path)
                        test_module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(test_module)
                        exec_time = time.time() - start_time
                        performance_results["execution_times"][module] = exec_time
                    except Exception:
                        performance_results["execution_times"][module] = None
            
        except Exception as e:
            performance_results["error"] = f"æ€§èƒ½æµ‹è¯•é”™è¯¯: {e}"
        
        return performance_results
    
    async def _compare_project_structures(self) -> Dict[str, Any]:
        """æ¯”è¾ƒé¡¹ç›®ç»“æ„å˜åŒ–"""
        if not self.project_structure_before or not self.project_structure_after:
            return {"status": "insufficient_data"}
        
        comparison = {
            "timestamp": datetime.now().isoformat(),
            "files_added": [],
            "files_removed": [],
            "files_modified": [],
            "structure_changes": {},
            "complexity_changes": {},
            "dependency_changes": {}
        }
        
        before_files = set(self.project_structure_before.file_details.keys())
        after_files = set(self.project_structure_after.file_details.keys())
        
        # æ‰¾å‡ºæ–°å¢æ–‡ä»¶
        comparison["files_added"] = list(after_files - before_files)
        
        # æ‰¾å‡ºåˆ é™¤æ–‡ä»¶
        comparison["files_removed"] = list(before_files - after_files)
        
        # æ‰¾å‡ºä¿®æ”¹æ–‡ä»¶
        common_files = before_files & after_files
        for file_path in common_files:
            before_info = self.project_structure_before.file_details[file_path]
            after_info = self.project_structure_after.file_details[file_path]
            
            if before_info.modified_time != after_info.modified_time:
                comparison["files_modified"].append(file_path)
        
        return comparison
    
    async def _generate_optimization_report(self, test_results: Dict, security_results: Dict, 
                                         performance_results: Dict, scan_results: Dict, 
                                         structure_comparison: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "executive_summary": {},
            "test_analysis": {},
            "security_analysis": {},
            "performance_analysis": {},
            "code_quality_analysis": {},
            "structure_analysis": {},
            "recommendations": [],
            "action_items": []
        }
        
        # æ‰§è¡Œæ‘˜è¦
        report["executive_summary"] = {
            "overall_health": "good" if test_results.get("passed", 0) > test_results.get("failed", 0) else "needs_attention",
            "critical_issues": len(security_results.get("high_severity", 0)),
            "test_coverage": test_results.get("coverage", {}).get("percent_covered", 0),
            "total_recommendations": len(scan_results.get("recommendations", []))
        }
        
        # æµ‹è¯•åˆ†æ
        report["test_analysis"] = {
            "total_tests": test_results.get("total_tests", 0),
            "pass_rate": test_results.get("passed", 0) / max(test_results.get("total_tests", 1), 1),
            "coverage_score": test_results.get("coverage", {}).get("percent_covered", 0),
            "execution_time": test_results.get("execution_time", 0)
        }
        
        # å®‰å…¨åˆ†æ
        report["security_analysis"] = {
            "total_security_issues": security_results.get("total_issues", 0),
            "high_risk_issues": security_results.get("high_severity", 0),
            "medium_risk_issues": security_results.get("medium_severity", 0),
            "low_risk_issues": security_results.get("low_severity", 0)
        }
        
        # æ€§èƒ½åˆ†æ
        report["performance_analysis"] = {
            "memory_usage_mb": performance_results.get("memory_usage", {}).get("rss", 0) / 1024 / 1024,
            "slow_modules": [k for k, v in performance_results.get("execution_times", {}).items() if v and v > 1.0]
        }
        
        # ä»£ç è´¨é‡åˆ†æ
        report["code_quality_analysis"] = {
            "total_issues": len(scan_results.get("issues_found", [])),
            "duplicate_code_blocks": len(scan_results.get("duplicate_analysis", {})),
            "unused_imports": len(scan_results.get("unused_imports", {}))
        }
        
        # ç»“æ„åˆ†æ
        report["structure_analysis"] = {
            "files_added": len(structure_comparison.get("files_added", [])),
            "files_removed": len(structure_comparison.get("files_removed", [])),
            "files_modified": len(structure_comparison.get("files_modified", []))
        }
        
        # ç”Ÿæˆæ¨èå»ºè®®
        report["recommendations"] = scan_results.get("recommendations", [])
        
        # ç”Ÿæˆè¡ŒåŠ¨é¡¹
        if security_results.get("high_severity", 0) > 0:
            report["action_items"].append("ç«‹å³å¤„ç†é«˜ä¸¥é‡æ€§å®‰å…¨é—®é¢˜")
        
        if test_results.get("coverage", {}).get("percent_covered", 0) < self.config.coverage_threshold:
            report["action_items"].append(f"æå‡æµ‹è¯•è¦†ç›–ç‡è‡³{self.config.coverage_threshold}%ä»¥ä¸Š")
        
        return report
    
    async def _interactive_analysis(self):
        """äº¤äº’å¼åˆ†æ"""
        print("\nğŸ¯ è¿›å…¥äº¤äº’å¼åˆ†ææ¨¡å¼")
        print("=" * 50)
        
        while True:
            print("\nå¯é€‰æ“ä½œ:")
            print("1. æŸ¥çœ‹è¯¦ç»†æµ‹è¯•ç»“æœ")
            print("2. æŸ¥çœ‹å®‰å…¨æ‰«ææŠ¥å‘Š")
            print("3. æŸ¥çœ‹æ€§èƒ½åˆ†ææŠ¥å‘Š")
            print("4. æŸ¥çœ‹ä»£ç è´¨é‡æŠ¥å‘Š")
            print("5. æŸ¥çœ‹é¡¹ç›®ç»“æ„å˜åŒ–")
            print("6. æŸ¥çœ‹ä¼˜åŒ–å»ºè®®")
            print("7. å¯¼å‡ºå®Œæ•´æŠ¥å‘Š")
            print("0. é€€å‡ºäº¤äº’æ¨¡å¼")
            
            choice = input("\nè¯·é€‰æ‹©æ“ä½œ (0-7): ").strip()
            
            if choice == "0":
                break
            elif choice == "1":
                await self._show_test_details()
            elif choice == "2":
                await self._show_security_details()
            elif choice == "3":
                await self._show_performance_details()
            elif choice == "4":
                await self._show_code_quality_details()
            elif choice == "5":
                await self._show_structure_details()
            elif choice == "6":
                await self._show_recommendations()
            elif choice == "7":
                await self._export_report()
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
    
    async def _show_test_details(self):
        """æ˜¾ç¤ºæµ‹è¯•è¯¦æƒ…"""
        print("\nğŸ“Š æµ‹è¯•ç»“æœè¯¦æƒ…")
        print("-" * 40)
        
        if self.test_results:
            for key, value in self.test_results.items():
                print(f"{key}: {value}")
        else:
            print("æš‚æ— æµ‹è¯•ç»“æœ")
    
    async def _show_security_details(self):
        """æ˜¾ç¤ºå®‰å…¨è¯¦æƒ…"""
        print("\nğŸ›¡ï¸ å®‰å…¨æ‰«æè¯¦æƒ…")
        print("-" * 40)
        # å®ç°å®‰å…¨è¯¦æƒ…æ˜¾ç¤ºé€»è¾‘
    
    async def _show_performance_details(self):
        """æ˜¾ç¤ºæ€§èƒ½è¯¦æƒ…"""
        print("\nâš¡ æ€§èƒ½åˆ†æè¯¦æƒ…")
        print("-" * 40)
        # å®ç°æ€§èƒ½è¯¦æƒ…æ˜¾ç¤ºé€»è¾‘
    
    async def _show_code_quality_details(self):
        """æ˜¾ç¤ºä»£ç è´¨é‡è¯¦æƒ…"""
        print("\nğŸ“‹ ä»£ç è´¨é‡è¯¦æƒ…")
        print("-" * 40)
        # å®ç°ä»£ç è´¨é‡è¯¦æƒ…æ˜¾ç¤ºé€»è¾‘
    
    async def _show_structure_details(self):
        """æ˜¾ç¤ºç»“æ„è¯¦æƒ…"""
        print("\nğŸ—ï¸ é¡¹ç›®ç»“æ„è¯¦æƒ…")
        print("-" * 40)
        # å®ç°ç»“æ„è¯¦æƒ…æ˜¾ç¤ºé€»è¾‘
    
    async def _show_recommendations(self):
        """æ˜¾ç¤ºæ¨èå»ºè®®"""
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®")
        print("-" * 40)
        
        if self.optimization_report.get("recommendations"):
            for i, rec in enumerate(self.optimization_report["recommendations"], 1):
                print(f"{i}. {rec}")
        else:
            print("æš‚æ— æ¨èå»ºè®®")
    
    async def _export_report(self):
        """å¯¼å‡ºæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.project_root / f"enhanced_test_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.optimization_report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {report_file}")
    
    async def _generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        final_report = {
            "timestamp": datetime.now().isoformat(),
            "test_configuration": asdict(self.config),
            "project_structure_before": asdict(self.project_structure_before) if self.project_structure_before else None,
            "project_structure_after": asdict(self.project_structure_after) if self.project_structure_after else None,
            "optimization_report": self.optimization_report,
            "ai_context_file": str(self.project_root / ".iflow" / "temp_docs" / "ai_context.json")
        }
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.project_root / f"enhanced_sc_test_final_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return final_report
    
    async def _get_basic_structure(self) -> Dict[str, Any]:
        """è·å–åŸºæœ¬é¡¹ç›®ç»“æ„"""
        structure = {
            "root_directories": [],
            "python_files": [],
            "config_files": [],
            "test_files": []
        }
        
        for item in self.project_root.iterdir():
            if item.is_dir() and not item.name.startswith('.'):
                structure["root_directories"].append(item.name)
            elif item.is_file():
                if item.suffix == '.py':
                    structure["python_files"].append(item.name)
                elif item.name in ['pyproject.toml', 'setup.py', 'requirements.txt']:
                    structure["config_files"].append(item.name)
                elif 'test' in item.name.lower():
                    structure["test_files"].append(item.name)
        
        return structure

# ä¸»å‡½æ•°
async def main():
    """ä¸»å‡½æ•°ï¼ˆå…¼å®¹åŸå§‹ç‰ˆæœ¬å‘½ä»¤è¡Œæ¥å£ï¼‰"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="å¢å¼ºç‰ˆ /sc:test æŒ‡ä»¤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s                                    # è¿è¡Œå®Œæ•´å¢å¼ºåˆ†æ
  %(prog)s src/core --type unit --coverage     # å•å…ƒæµ‹è¯•ä¸è¦†ç›–ç‡
  %(prog)s --type e2e                          # ç«¯åˆ°ç«¯æµè§ˆå™¨æµ‹è¯•
  %(prog)s --watch --fix                       # ç›‘è§†æ¨¡å¼ï¼ˆå¼€å‘ä¸­ï¼‰
  %(prog)s --no-interactive                    # éäº¤äº’æ¨¡å¼
        """
    )
    
    # åŸå§‹ç‰ˆæœ¬å‚æ•°
    parser.add_argument(
        "target", 
        nargs="?", 
        help="æµ‹è¯•ç›®æ ‡è·¯å¾„ï¼ˆå¦‚ src/componentsï¼‰"
    )
    parser.add_argument(
        "--type", 
        choices=["unit", "integration", "e2e", "all"],
        default="all",
        help="æµ‹è¯•ç±»å‹: unit(å•å…ƒ), integration(é›†æˆ), e2e(ç«¯åˆ°ç«¯), all(å…¨éƒ¨)"
    )
    parser.add_argument(
        "--coverage", 
        action="store_true",
        default=True,
        help="å¯ç”¨è¦†ç›–ç‡åˆ†æï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )
    parser.add_argument(
        "--no-coverage", 
        action="store_true",
        help="ç¦ç”¨è¦†ç›–ç‡åˆ†æ"
    )
    parser.add_argument(
        "--watch", 
        action="store_true",
        help="è¿ç»­ç›‘è§†æ¨¡å¼ï¼ˆå¼€å‘ä¸­ï¼‰"
    )
    parser.add_argument(
        "--fix", 
        action="store_true",
        help="è‡ªåŠ¨ç®€å•å¤±è´¥ä¿®å¤ï¼ˆå¼€å‘ä¸­ï¼‰"
    )
    
    # å¢å¼ºç‰ˆæœ¬å‚æ•°
    parser.add_argument(
        "--no-interactive",
        action="store_true",
        help="éäº¤äº’æ¨¡å¼"
    )
    parser.add_argument(
        "--project-root",
        type=str,
        default=".",
        help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--coverage-threshold",
        type=float,
        default=25.0,
        help="è¦†ç›–ç‡é˜ˆå€¼ï¼ˆé»˜è®¤25%%ï¼‰"
    )
    parser.add_argument(
        "--no-deep-analysis",
        action="store_true",
        help="ç¦ç”¨æ·±åº¦åˆ†ææ‰«æ"
    )
    parser.add_argument(
        "--no-optimization-report",
        action="store_true",
        help="ç¦ç”¨ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆ"
    )
    parser.add_argument(
        "--no-structure-comparison",
        action="store_true",
        help="ç¦ç”¨ç»“æ„å¯¹æ¯”åˆ†æ"
    )
    
    args = parser.parse_args()
    
    # å¤„ç†è¦†ç›–ç‡é€‰é¡¹
    enable_coverage = args.coverage and not args.no_coverage
    
    # åˆ›å»ºé…ç½®
    config = TestConfiguration(
        project_root=args.project_root,
        test_types=[args.type] if args.type != "all" else ["unit", "integration", "e2e"],
        coverage_threshold=args.coverage_threshold,
        interactive_mode=not args.no_interactive,
        enable_deep_analysis=not args.no_deep_analysis,
        force_ai_awareness=True
    )
    
    # è¿è¡Œå¢å¼ºç‰ˆæµ‹è¯•
    async def run_test():
        engine = EnhancedTestEngine(config)
        
        # å¦‚æœæ˜¯åŸå§‹ç‰ˆæœ¬æ¨¡å¼ï¼ˆæŒ‡å®šäº†targetæˆ–typeï¼‰ï¼Œè¿è¡Œå…¼å®¹æ¨¡å¼
        if args.target or args.type != "all" or args.watch or args.fix:
            print("ğŸ”„ è¿è¡Œå…¼å®¹æ¨¡å¼ - åŸå§‹ç‰ˆæœ¬åŠŸèƒ½")
            results = await engine.run_compatible_test(
                target=args.target,
                test_type=args.type,
                enable_coverage=enable_coverage,
                watch_mode=args.watch,
                auto_fix=args.fix
            )
        else:
            # è¿è¡Œå®Œæ•´å¢å¼ºç‰ˆåˆ†æ
            results = await engine.run_comprehensive_test()
        
        # è¾“å‡ºç»“æœæ‘˜è¦
        if "test_results" in results:
            tr = results["test_results"]
            print(f"\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
            print(f"é€šè¿‡: {tr.get('passed', 0)}")
            print(f"å¤±è´¥: {tr.get('failed', 0)}")
            print(f"è·³è¿‡: {tr.get('skipped', 0)}")
            print(f"æ‰§è¡Œæ—¶é—´: {tr.get('execution_time', 0):.2f}ç§’")
            
            if tr.get('coverage'):
                coverage_pct = tr['coverage'].get('percent_covered', 0)
                print(f"è¦†ç›–ç‡: {coverage_pct:.1f}%")
        
        return results
    
    # è¿è¡Œæµ‹è¯•
    results = await run_test()
    
    print("\nğŸ‰ å¢å¼ºç‰ˆ /sc:test æ‰§è¡Œå®Œæˆï¼")
    print(f"ğŸ“Š æµ‹è¯•é€šè¿‡ç‡: {results['optimization_report']['test_analysis']['pass_rate']:.2%}")
    print(f"ğŸ›¡ï¸ å®‰å…¨é—®é¢˜: {results['optimization_report']['security_analysis']['total_security_issues']}ä¸ª")
    print(f"ğŸ“ˆ ä»£ç è´¨é‡è¯„åˆ†: {results['optimization_report']['code_quality_analysis']['total_issues']}ä¸ªé—®é¢˜")

if __name__ == "__main__":
    asyncio.run(main())