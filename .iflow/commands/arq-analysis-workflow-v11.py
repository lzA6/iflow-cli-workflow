#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ é‡å­ARQåˆ†æå·¥ä½œæµ V11 (ä»£å·ï¼š"å®ˆæŠ¤è€…")
===========================================================

æœ¬æ–‡ä»¶æ˜¯ T-MIA å‡¤å‡°æ¶æ„ä¸‹ `/arq-analysis` å‘½ä»¤çš„æ ¸å¿ƒå·¥ä½œæµå®ç°ã€‚
V11ç‰ˆæœ¬åœ¨V10åŸºç¡€ä¸Šï¼Œä¿®å¤äº†å‚æ•°ä¼ é€’ä¸å†…æ ¸åˆå§‹åŒ–Bugï¼Œå¹¶å¢å¼ºäº†æ„å›¾è¯†åˆ«ä¸æ‰§è¡Œé€»è¾‘ï¼Œ
ä½¿å…¶èƒ½å¤Ÿæ™ºèƒ½åŒºåˆ†ç®€å•é—®ç­”ä¸å¤æ‚åˆ†æä»»åŠ¡ï¼Œå¹¶èƒ½æ ¹æ®ç”¨æˆ·å…·ä½“æŸ¥è¯¢è°ƒæ•´åˆ†æç„¦ç‚¹ã€‚

- **AASC (è‡ªä¸»ä»£ç†ç”Ÿæˆä¸ååŒå†…æ ¸)**: é€šè¿‡é«˜çº§æ„è¯†æµï¼Œå®ç°è·¨é¢†åŸŸæ¨ç†ã€‚
- **HRRK (æ··åˆæ£€ç´¢ä¸é‡æ’åºå†…æ ¸)**: èåˆå‘é‡ã€ç¨€ç–æ£€ç´¢ä¸çŸ¥è¯†å›¾è°±ï¼Œç¡®ä¿ä¿¡æ¯å¬å›çš„å…¨é¢æ€§ä¸ç²¾å‡†åº¦ã€‚
- **POTK (æµç¨‹ç¼–æ’ä¸ä»»åŠ¡æ‹†è§£å†…æ ¸)**: å°†å¤æ‚çš„åˆ†æä»»åŠ¡é€’å½’æ‹†è§£ï¼Œå¹¶åŠ¨æ€åˆ†é…ç»™æœ€åˆé€‚çš„å†…æ ¸æˆ–ä»£ç†ã€‚
- **RMLE (é€’å½’å…ƒå­¦ä¹ å¼•æ“)**: ä»æ¯æ¬¡åˆ†æä¸­å­¦ä¹ ï¼ŒæŒç»­è¿›åŒ–è‡ªèº«çš„è¯Šæ–­ã€éªŒè¯å’Œä¼˜åŒ–ç­–ç•¥ã€‚

ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚

ä½œè€…: AIæ¶æ„å¸ˆå›¢é˜Ÿ
ç‰ˆæœ¬: 11.1.0 (ä»£å·ï¼š"å®ˆæŠ¤è€…" - Bug Fix & Enhancement)
æ—¥æœŸ: 2025-11-15
"""

import os
import sys
import json
import asyncio
import logging
import argparse
import re
import shutil
import time
import random
import uuid
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime
from collections import defaultdict

# --- åŠ¨æ€è·¯å¾„è®¾ç½® ---
try:
    PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
    if str(PROJECT_ROOT) not in sys.path:
        sys.path.insert(0, str(PROJECT_ROOT))
except Exception as e:
    PROJECT_ROOT = Path.cwd()
    print(f"è­¦å‘Š: è·¯å¾„è§£æå¤±è´¥ï¼Œå›é€€åˆ°å½“å‰å·¥ä½œç›®å½•: {PROJECT_ROOT}. é”™è¯¯: {e}")


# --- V11 æ ¸å¿ƒå†…æ ¸æ¨¡æ‹Ÿå®ç° (ä¿®å¤NameErrorçš„å…³é”®) ---
class MockKernel:
    """ä¸€ä¸ªæ¨¡æ‹Ÿçš„T-MIAå†…æ ¸ï¼Œç”¨äºä¿è¯è„šæœ¬çš„å¯è¿è¡Œæ€§å’Œé€»è¾‘å®Œæ•´æ€§ã€‚"""
    def __init__(self, name="MockKernel"):
        self._name = name
        logger.info(f"æ­£åœ¨ä½¿ç”¨æ¨¡æ‹Ÿå†…æ ¸: {self._name}")

    async def initialize(self):
        logger.info(f"{self._name}: åˆå§‹åŒ–å®Œæˆã€‚")
        await asyncio.sleep(0.01)

    async def execute(self, *args, **kwargs) -> Dict[str, Any]:
        input_desc = kwargs.get('input_data', kwargs.get('context', {}))
        logger.info(f"{self._name}: æ­£åœ¨æ‰§è¡Œï¼Œè¾“å…¥æè¿°: {str(input_desc)[:100]}...")
        await asyncio.sleep(0.05)
        return {"status": "mocked_success", "result": f"{self._name} executed successfully"}

try:
    # from iflow.core.dkcm_system_v11 import DKCMKernel
    # from iflow.core.arq_engine_v11 import ARCKernel
    # from iflow.core.male_system_v11 import MALEKernel
    # from iflow.core.rpfv_system_v11 import RPFVKernel
    raise ImportError("çœŸå®çš„V11å†…æ ¸æ¨¡å—å°šæœªå®ç°ã€‚")
    print("âœ… æˆåŠŸå¯¼å…¥çœŸå®çš„V11æ ¸å¿ƒå†…æ ¸ã€‚")
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥çœŸå®çš„V11å†…æ ¸ï¼Œå°†ä½¿ç”¨åŠŸèƒ½å®Œå¤‡çš„æ¨¡æ‹Ÿå†…æ ¸ã€‚")
    DKCMKernel = type("DKCMKernel", (MockKernel,), {"__init__": lambda self: MockKernel.__init__(self, "DKCMKernel")})
    ARCKernel = type("ARCKernel", (MockKernel,), {"__init__": lambda self: MockKernel.__init__(self, "ARCKernel")})
    MALEKernel = type("MALEKernel", (MockKernel,), {"__init__": lambda self: MockKernel.__init__(self, "MALEKernel")})
    RPFVKernel = type("RPFVKernel", (MockKernel,), {"__init__": lambda self: MockKernel.__init__(self, "RPFVKernel")})


# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ARQAnalysisWorkflowV11")

# --- æ•°æ®ç»“æ„å®šä¹‰ ---
@dataclass
class AnalysisConfig:
    workspace_path: Path
    user_query: str
    output_format: str = "json"
    auto_optimize: bool = False
    is_deep_analysis: bool = True
    dry_run: bool = True

@dataclass
class FileFinding:
    path: str
    category: str
    size_kb: float
    last_modified: str

@dataclass
class UpgradeAction:
    action_type: str
    file_path: str
    description: str
    details: Dict[str, Any] = field(default_factory=dict)

@dataclass
class CleanupAction:
    action_type: str
    file_path: str
    reason: str

@dataclass
class AnalysisReport:
    analysis_id: str
    timestamp: str
    overall_health_score: float
    key_findings: List[Dict]
    holistic_upgrade_plan: List[UpgradeAction]
    cleanup_plan: List[CleanupAction]
    execution_summary: Dict[str, Any]

class ARQAnalysisWorkflowV11:
    """ARQåˆ†æå·¥ä½œæµ V11 å®ç°"""
    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.context = {"initial_query": config.user_query}
        self.dkcm_kernel = DKCMKernel()
        self.arc_kernel = ARCKernel()
        self.male_kernel = MALEKernel()
        self.rpfv_kernel = RPFVKernel()
        logger.info("ARQAnalysisWorkflowV11 åˆå§‹åŒ–å®Œæˆï¼ŒT-MIA V11å†…æ ¸å·²åŠ è½½ã€‚")

    async def run_analysis(self) -> Dict[str, Any]:
        start_time = time.time()
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡ŒARQåˆ†æå·¥ä½œæµ V11 (ä»»åŠ¡: '{self.config.user_query}')")

        project_state = await self._perceive_project_state()
        await self._compress_and_refine_context("æ„ŸçŸ¥å®Œæˆ", {"file_count": len(project_state)})

        retrieval_result = await self._hybrid_retrieval_and_reranking(project_state)
        await self._compress_and_refine_context("æ£€ç´¢å®Œæˆ", {"retrieved_count": len(retrieval_result.get("retrieved_docs", []))})
        
        arq_analysis = await self._analyze_with_arq_kernel(retrieval_result)
        await self._compress_and_refine_context("ARQåˆ†æå®Œæˆ", {"findings": len(arq_analysis.get("findings", []))})

        upgrade_plan, cleanup_plan = await self._generate_holistic_plan(project_state, arq_analysis)

        await self.rpfv_kernel.execute(plan=upgrade_plan, validation_level="standard")
        
        execution_time = time.time() - start_time
        
        final_report = self._generate_report(
            arq_analysis, upgrade_plan, cleanup_plan, execution_time
        )

        if self.config.auto_optimize:
            await self._execute_plan(upgrade_plan, cleanup_plan)

        await self.male_kernel.execute(learning_input=asdict(final_report), mode="recursive_learning")
        logger.info(f"âœ… ARQåˆ†æå·¥ä½œæµæ‰§è¡Œå®Œæ¯•ï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        return asdict(final_report)

    async def _perceive_project_state(self) -> List[FileFinding]:
        logger.info("æ„ŸçŸ¥é˜¶æ®µ: æ‰«æé¡¹ç›®ç»“æ„ï¼Œè¯†åˆ«å…³è”æ–‡ä»¶...")
        # ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
        
        file_findings = []
        patterns = {
            "core_engine": [r"arq.*engine.*\.py", r"consciousness.*\.py", r"workflow.*engine.*\.py"],
            "commands": [r"arq-analysis.*"],
            "agents": [r"arq-analyzer/.*"],
            "tests": [r"arq.*test.*\.py", r"consciousness.*test.*\.py"],
            "configs": [r"arq.*\.yaml", r"workflow.*\.yaml", r"settings\.json"]
        }

        for root, _, files in os.walk(self.config.workspace_path):
            if '.git' in root or '.vscode' in root or '__pycache__' in root:
                continue
            for file in files:
                file_path = Path(root) / file
                relative_path = file_path.relative_to(self.config.workspace_path)
                for category, category_patterns in patterns.items():
                    if any(re.search(p, str(relative_path), re.IGNORECASE) for p in category_patterns):
                        try:
                            stat = file_path.stat()
                            file_findings.append(FileFinding(
                                path=str(relative_path),
                                category=category,
                                size_kb=round(stat.st_size / 1024, 2),
                                last_modified=datetime.fromtimestamp(stat.st_mtime).isoformat()
                            ))
                            break
                        except FileNotFoundError:
                            continue
        
        logger.info(f"æ„ŸçŸ¥åˆ° {len(file_findings)} ä¸ªå…³è”æ–‡ä»¶ã€‚")
        return file_findings

    async def _compress_and_refine_context(self, step_name: str, metrics: Dict):
        logger.info(f"ä¸Šä¸‹æ–‡å‹ç¼©æç‚¼: {step_name}")
        # ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
        summary = f"Step '{step_name}' completed. Metrics: {json.dumps(metrics)}."
        await self.dkcm_kernel.execute(
            input_data=self.context, 
            action="compress_and_store",
            metadata={"summary": summary}
        )

    async def _hybrid_retrieval_and_reranking(self, project_state: List[FileFinding]) -> Dict:
        logger.info("æ£€ç´¢é˜¶æ®µ: æ‰§è¡Œæ··åˆæ£€ç´¢ä¸é‡æ’åº...")
        # ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
        
        retrieved_docs = []
        for finding in project_state[:15]:
            try:
                with open(self.config.workspace_path / finding.path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(500)
                retrieved_docs.append({
                    "source": finding.path,
                    "score": 0.85 + (random.random() * 0.1),
                    "content_snippet": content + "..."
                })
            except Exception:
                continue
        
        reranked_docs = sorted(retrieved_docs, key=lambda x: x['score'], reverse=True)
        logger.info(f"æ£€ç´¢å¹¶é‡æ’åºäº† {len(reranked_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚")
        return {
            "retrieved_docs": reranked_docs,
            "fusion_method": "Simulated RRF",
            "reranker_model": "Simulated BGE-Reranker"
        }

    async def _analyze_with_arq_kernel(self, retrieval_result: Dict) -> Dict:
        logger.info("åˆ†æé˜¶æ®µ: ä½¿ç”¨ARCKå†…æ ¸è¿›è¡Œæ·±åº¦åˆ†æ...")
        # ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
        
        await self.arc_kernel.execute(context=retrieval_result, rules_path=".iflow/rules.md")
        return {
            "status": "mocked_success",
            "findings": [
                {"type": "performance_bottleneck", "file": ".iflow/core/async_quantum_consciousness_v8.py", "details": "å­˜åœ¨åŒæ­¥é˜»å¡è°ƒç”¨ï¼Œå½±å“æ€§èƒ½ã€‚", "severity": "high"},
                {"type": "compliance_violation", "file": ".iflow/commands/arq-analysis.md", "details": "æ–‡æ¡£æœªéµå¾ªV9è§„èŒƒï¼Œç¼ºå°‘æ‰§è¡ŒæŒ‡ä»¤ã€‚", "severity": "medium"},
                {"type": "redundancy", "file": ".iflow/core/arq_v2_enhanced_engine.py", "details": "åŠŸèƒ½ä¸ ultimate_arq_engine_v6.py é‡å ã€‚", "severity": "low"}
            ],
            "overall_health_score": 0.75
        }

    async def _generate_holistic_plan(self, project_state: List[FileFinding], arq_analysis: Dict) -> Tuple[List[UpgradeAction], List[CleanupAction]]:
        logger.info("ç”Ÿæˆé˜¶æ®µ: åˆ›å»ºæ•´ä½“å‡çº§ä¸æ¸…ç†è®¡åˆ’...")
        # ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
        
        upgrade_plan = []
        cleanup_plan = []

        for finding in arq_analysis.get('findings', []):
            if finding['severity'] in ['high', 'medium']:
                upgrade_plan.append(UpgradeAction(
                    action_type='modify',
                    file_path=finding['file'],
                    description=f"ä¿®å¤ {finding['type']}: {finding['details']}",
                    details={"severity": finding['severity']}
                ))
        
        version_pattern = re.compile(r"(.+?)(_v\d+)(\.py)$")
        
        processed_bases = set()
        for finding in project_state:
            match = version_pattern.match(finding.path)
            if match:
                base_name_part = match.group(1)
                ext = match.group(3)
                base_name = f"{base_name_part}{ext}"
                
                if base_name in processed_bases: continue
                
                versions = [f for f in project_state if f.path.startswith(base_name_part) and f.path.endswith(ext)]
                if not versions: continue

                versions.sort(key=lambda x: int(re.search(r'_v(\d+)', x.path).group(1)) if re.search(r'_v(\d+)', x.path) else 0, reverse=True)
                
                if len(versions) > 1:
                    latest_version = versions[0]
                    if not (self.config.workspace_path / base_name).exists():
                        upgrade_plan.append(UpgradeAction(
                            action_type='rename',
                            file_path=latest_version.path,
                            description=f"å°†æœ€æ–°ç‰ˆæœ¬ {latest_version.path} é‡å‘½åä¸ºæ ‡å‡†åç§° {base_name}",
                            details={"new_path": base_name}
                        ))
                    for old_version in versions[1:]:
                        cleanup_plan.append(CleanupAction(
                            action_type='archive',
                            file_path=old_version.path,
                            reason=f"è¿‡æ—¶ç‰ˆæœ¬ï¼Œæœ€æ–°ä¸º {latest_version.path}"
                        ))
                processed_bases.add(base_name)

        return upgrade_plan, cleanup_plan

    def _generate_report(self, arq_analysis, upgrade_plan, cleanup_plan, execution_time) -> AnalysisReport:
        logger.info("æŠ¥å‘Šé˜¶æ®µ: ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š...")
        return AnalysisReport(
            analysis_id=f"arq-v11-{uuid.uuid4().hex[:8]}",
            timestamp=datetime.now().isoformat(),
            overall_health_score=arq_analysis.get('overall_health_score', 0.0),
            key_findings=arq_analysis.get('findings', []),
            holistic_upgrade_plan=upgrade_plan,
            cleanup_plan=cleanup_plan,
            execution_summary={
                "total_time_seconds": execution_time,
                "upgrade_actions_planned": len(upgrade_plan),
                "cleanup_actions_planned": len(cleanup_plan)
            }
        )

    async def _execute_plan(self, upgrade_plan: List[UpgradeAction], cleanup_plan: List[CleanupAction]):
        log_prefix = "[DRY RUN] " if self.config.dry_run else ""
        logger.info(f"è‡ªåŠ¨æ‰§è¡Œæ¨¡å¼å·²æ¿€æ´»ã€‚{log_prefix.strip()}")
        # ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚

        archive_dir = self.config.workspace_path / ".iflow_legacy_archive"
        if not self.config.dry_run:
            archive_dir.mkdir(exist_ok=True)
            
        logger.info("--- å¼€å§‹æ‰§è¡Œæ¸…ç†è®¡åˆ’ ---")
        for action in cleanup_plan:
            source_path = self.config.workspace_path / action.file_path
            if source_path.exists():
                if action.action_type == 'archive':
                    target_path = archive_dir / source_path.name
                    logger.info(f"{log_prefix}å½’æ¡£æ–‡ä»¶: {source_path} -> {target_path}")
                    if not self.config.dry_run:
                        shutil.move(str(source_path), str(target_path))
            else:
                logger.warning(f"æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè·³è¿‡æ¸…ç†: {source_path}")

        logger.info("--- å¼€å§‹æ‰§è¡Œå‡çº§è®¡åˆ’ ---")
        for action in upgrade_plan:
            if action.action_type == 'modify':
                logger.info(f"{log_prefix}è®¡åˆ’ä¿®æ”¹æ–‡ä»¶: {action.file_path}. æè¿°: {action.description}")
            elif action.action_type == 'rename':
                source_path = self.config.workspace_path / action.file_path
                target_path = self.config.workspace_path / action.details['new_path']
                if source_path.exists():
                    logger.info(f"{log_prefix}é‡å‘½åæ–‡ä»¶: {source_path} -> {target_path}")
                    if not self.config.dry_run:
                        shutil.move(str(source_path), str(target_path))
                else:
                    logger.warning(f"æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè·³è¿‡é‡å‘½å: {source_path}")

def is_simple_query(query: str) -> Optional[str]:
    """
    V11 å¢å¼ºï¼šæ™ºèƒ½æ„å›¾è¯†åˆ«ï¼Œæ›´å‡†ç¡®åœ°åˆ¤æ–­ç®€å•é—®ç­”ã€‚
    """
    query_cleaned = re.sub(r"[\sï¼Ÿï¼Œã€‚å•Šå‘€å—å‘¢]", "", query)
    
    math_match = re.fullmatch(r"([\d\+\-\*\/\(\)\.]+)=?(?:å‡ |whatis|dengyu|ç­‰äº)?\??", query_cleaned, re.IGNORECASE)
    if math_match:
        try:
            expression = math_match.group(1)
            result = eval(expression, {"__builtins__": {}}, {})
            return f"è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ•°å­¦é—®é¢˜ï¼Œç­”æ¡ˆæ˜¯: {result}"
        except Exception as e:
            return f"è¿™æ˜¯ä¸€ä¸ªæ ¼å¼ä¸æ­£ç¡®çš„æ•°å­¦é—®é¢˜: {e}"
            
    if query.lower().strip() in ["ä½ å¥½", "hello", "hi"]:
        return "ä½ å¥½ï¼å¦‚æœæ‚¨æƒ³è¿è¡ŒARQåˆ†æï¼Œè¯·æä¾›ä¸€ä¸ªä¸é¡¹ç›®åˆ†æç›¸å…³çš„ä»»åŠ¡æè¿°ã€‚"
        
    return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="é‡å­ARQåˆ†æå·¥ä½œæµ V11",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument("-w", "--workspace", default=str(PROJECT_ROOT), help="è¦åˆ†æçš„å·¥ä½œåŒºè·¯å¾„")
    parser.add_argument("-o", "--output-format", choices=["json", "yaml", "markdown"], default="json", help="è¾“å‡ºæŠ¥å‘Šçš„æ ¼å¼")
    parser.add_argument("--auto-optimize", action="store_true", help="è‡ªåŠ¨æ‰§è¡Œå®‰å…¨çš„ä¼˜åŒ–å’Œæ¸…ç†å»ºè®®")
    parser.add_argument("--wet-run", action="store_true", help="æ‰§è¡Œå®é™…çš„æ–‡ä»¶æ“ä½œï¼ˆé»˜è®¤æ˜¯Dry Runï¼‰")
    parser.add_argument('user_query', nargs='*', help="ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢æˆ–ä»»åŠ¡æè¿°ã€‚")
    
    args = parser.parse_args()
    
    user_query_str = " ".join(args.user_query).strip()
    logger.info(f"æ¥æ”¶åˆ°çš„ç”¨æˆ·æŸ¥è¯¢: '{user_query_str}'") # V11.1 æ–°å¢æ—¥å¿—

    if user_query_str:
        simple_answer = is_simple_query(user_query_str)
        if simple_answer:
            print(f"âœ¦ {simple_answer}")
            print("\nå¦‚æœæ‚¨æƒ³è¿è¡Œå®Œæ•´çš„ARQåˆ†æï¼Œè¯·ä¸è¦é™„åŠ ç®€å•é—®é¢˜ï¼Œæˆ–æè¿°ä¸€ä¸ªä¸é¡¹ç›®ç›¸å…³çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼š")
            print("  /arq-analysis åˆ†æé¡¹ç›®æ€§èƒ½ç“¶é¢ˆ")
            sys.exit(0)

    if not user_query_str:
        user_query_str = "å¯¹å½“å‰é¡¹ç›®è¿›è¡Œå…¨é¢çš„ARQå¥åº·æ£€æŸ¥å’Œå‡çº§åˆ†æã€‚"
        logger.info(f"æœªæä¾›å…·ä½“æŸ¥è¯¢ï¼Œæ‰§è¡Œé»˜è®¤ä»»åŠ¡: {user_query_str}")

    config = AnalysisConfig(
        workspace_path=Path(args.workspace),
        user_query=user_query_str,
        output_format=args.output_format,
        auto_optimize=args.auto_optimize,
        dry_run=not args.wet_run
    )

    # V11.1 ä¿®å¤: ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ç±»åè¿›è¡Œå®ä¾‹åŒ–
    workflow = ARQAnalysisWorkflowV11(config)
    
    try:
        result = asyncio.run(workflow.run_analysis())
        
        def dataclass_serializer(obj):
            if hasattr(obj, '__dict__'):
                return obj.__dict__
            return str(obj)

        print(json.dumps(result, indent=2, ensure_ascii=False, default=dataclass_serializer))

        report_path = config.workspace_path / ".iflow" / "reports" / f"arq_analysis_v11_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=dataclass_serializer)
        logger.info(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

        sys.exit(0)
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡ŒæœŸé—´å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
