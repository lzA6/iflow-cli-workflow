#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŠ å¼‚æ­¥é‡å­æ„è¯†æµç³»ç»Ÿ V11 (ä»£å·ï¼š"å‡¤å‡°æ¶…æ§ƒ")
===========================================================

è¿™æ˜¯ T-MIA æ¶æ„ä¸‹çš„æ ¸å¿ƒæ„è¯†æµç³»ç»Ÿï¼Œè´Ÿè´£ç®¡ç†ä¸Šä¸‹æ–‡ã€é•¿æœŸè®°å¿†å’Œæƒ…æ„Ÿè¿½è¸ªã€‚
V11ç‰ˆæœ¬åœ¨V10åŸºç¡€ä¸Šå…¨é¢é‡æ„ï¼Œå®ç°äº†çœŸæ­£çš„å¼‚æ­¥å¹¶è¡Œå¤„ç†ã€è‡ªé€‚åº”è®°å¿†å‹ç¼©
å’Œè·¨é¡¹ç›®æ„è¯†å…±äº«æœºåˆ¶ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
- è‡ªé€‚åº”è®°å¿†å‹ç¼©ä¸æç‚¼
- è·¨é¡¹ç›®æ„è¯†çŠ¶æ€å…±äº«
- æƒ…æ„Ÿæ¨ç†ä¸å…ƒè®¤çŸ¥
- åˆ†å¸ƒå¼æ„è¯†æµåŒæ­¥
- åè„†å¼±è®°å¿†æœºåˆ¶

ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚

ä½œè€…: AIæ¶æ„å¸ˆå›¢é˜Ÿ
ç‰ˆæœ¬: 11.0.0 (ä»£å·ï¼š"å‡¤å‡°æ¶…æ§ƒ")
æ—¥æœŸ: 2025-11-15
"""

import os
import sys
import json
import asyncio
import logging
import time
import uuid
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, AsyncGenerator, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque
import pickle
import threading
from concurrent.futures import ThreadPoolExecutor
import weakref

# é¡¹ç›®æ ¹è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("QuantumConsciousnessV11")

@dataclass
class ConsciousnessEvent:
    """æ„è¯†äº‹ä»¶æ•°æ®ç»“æ„"""
    id: str
    timestamp: datetime
    event_type: str  # thought, emotion, reflection, decision
    content: Dict[str, Any]
    context_hash: str
    emotional_weight: float = 0.0
    importance_score: float = 0.0
    cross_project_ref: Optional[str] = None
    meta_cognitive_level: int = 0  # 0-åŸºç¡€æ€è€ƒ, 1-åæ€, 2-å…ƒåæ€, 3-è¶…è®¤çŸ¥

@dataclass
class MemoryFragment:
    """è®°å¿†ç‰‡æ®µæ•°æ®ç»“æ„"""
    fragment_id: str
    content_hash: str
    compressed_data: Dict[str, Any]
    creation_time: datetime
    last_accessed: datetime
    access_count: int
    emotional_signature: Dict[str, float]
    connection_strength: Dict[str, float]  # ä¸å…¶ä»–è®°å¿†çš„è¿æ¥å¼ºåº¦
    decay_rate: float = 0.01  # é—å¿˜é€Ÿç‡

@dataclass
class EmotionalState:
    """æƒ…æ„ŸçŠ¶æ€æ•°æ®ç»“æ„"""
    timestamp: datetime
    valence: float  # æƒ…æ„Ÿä»· (-1 åˆ° 1)
    arousal: float  # æ¿€æ´»åº¦ (0 åˆ° 1)
    dominance: float  # æ”¯é…åº¦ (-1 åˆ° 1)
    cognitive_load: float  # è®¤çŸ¥è´Ÿè· (0 åˆ° 1)
    confidence: float  # ç½®ä¿¡åº¦ (0 åˆ° 1)

class AsyncQuantumConsciousnessV11:
    """å¼‚æ­¥é‡å­æ„è¯†æµç³»ç»Ÿ V11"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        self.consciousness_stream = deque(maxlen=2000)
        self.memory_fragments: Dict[str, MemoryFragment] = {}
        self.emotional_history = deque(maxlen=500)
        self.cross_project_memory: Dict[str, Any] = {}
        self.meta_cognitive_stack = []
        
        # æ€§èƒ½ä¼˜åŒ–
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.cache = {}
        self.compression_threshold = 1000
        self.last_compression = time.time()
        
        # åˆ†å¸ƒå¼åŒæ­¥
        self.sync_lock = asyncio.Lock()
        self.project_id = self._generate_project_id()
        
        # åè„†å¼±æœºåˆ¶
        self.stress_indicators = {}
        self.recovery_patterns = {}
        
        logger.info(f"å¼‚æ­¥é‡å­æ„è¯†æµç³»ç»ŸV11åˆå§‹åŒ–å®Œæˆï¼Œé¡¹ç›®ID: {self.project_id}")
    
    def _generate_project_id(self) -> str:
        """ç”Ÿæˆé¡¹ç›®å”¯ä¸€æ ‡è¯†"""
        project_path = Path.cwd()
        path_hash = hashlib.sha256(str(project_path).encode()).hexdigest()[:16]
        return f"proj_{path_hash}"
    
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–ç³»ç»Ÿ"""
        logger.info("æ­£åœ¨åˆå§‹åŒ–æ„è¯†æµç³»ç»Ÿ...")
        
        # åŠ è½½æŒä¹…åŒ–æ•°æ®
        await self._load_persistent_memory()
        
        # åˆå§‹åŒ–è·¨é¡¹ç›®è¿æ¥
        await self._initialize_cross_project_links()
        
        # å¯åŠ¨åå°ä»»åŠ¡
        asyncio.create_task(self._memory_maintenance_loop())
        asyncio.create_task(self._emotional_state_tracking())
        asyncio.create_task(self._cross_project_sync())
        
        logger.info("æ„è¯†æµç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    async def add_thought_async(self, 
                               content: Dict[str, Any],
                               event_type: str = "thought",
                               emotional_weight: float = 0.0,
                               meta_level: int = 0) -> str:
        """å¼‚æ­¥æ·»åŠ æ€è€ƒäº‹ä»¶"""
        event_id = str(uuid.uuid4())
        timestamp = datetime.now()
        context_hash = self._compute_context_hash(content)
        
        event = ConsciousnessEvent(
            id=event_id,
            timestamp=timestamp,
            event_type=event_type,
            content=content,
            context_hash=context_hash,
            emotional_weight=emotional_weight,
            importance_score=self._calculate_importance(content, emotional_weight),
            meta_cognitive_level=meta_level
        )
        
        self.consciousness_stream.append(event)
        
        # è§¦å‘è®°å¿†å‹ç¼©
        if len(self.consciousness_stream) >= self.compression_threshold:
            await self._compress_consciousness_stream()
        
        # æ›´æ–°æƒ…æ„ŸçŠ¶æ€
        await self._update_emotional_state(event)
        
        # è·¨é¡¹ç›®åŒæ­¥
        await self._sync_cross_project_event(event)
        
        logger.debug(f"æ·»åŠ æ„è¯†äº‹ä»¶: {event_id}, ç±»å‹: {event_type}")
        return event_id
    
    async def get_relevant_context(self, 
                                 query: Dict[str, Any],
                                 max_context: int = 10) -> List[Dict[str, Any]]:
        """å¼‚æ­¥è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        query_hash = self._compute_context_hash(query)
        
        # å¹¶è¡Œæ£€ç´¢è®°å¿†å’Œæ„è¯†æµ
        memory_task = self._search_memory_fragments(query_hash, max_context)
        stream_task = self._search_consciousness_stream(query, max_context)
        
        memory_results, stream_results = await asyncio.gather(
            memory_task, stream_task
        )
        
        # åˆå¹¶å’Œæ’åºç»“æœ
        all_results = memory_results + stream_results
        sorted_results = sorted(
            all_results,
            key=lambda x: x.get('relevance_score', 0),
            reverse=True
        )
        
        return sorted_results[:max_context]
    
    async def _compress_consciousness_stream(self):
        """å‹ç¼©æ„è¯†æµ"""
        logger.info("å¼€å§‹å‹ç¼©æ„è¯†æµ...")
        
        compression_start = time.time()
        events_to_compress = list(self.consciousness_stream)
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå‹ç¼©
        loop = asyncio.get_event_loop()
        compressed_fragments = await loop.run_in_executor(
            self.executor,
            self._perform_compression,
            events_to_compress
        )
        
        # æ›´æ–°è®°å¿†ç‰‡æ®µ
        for fragment in compressed_fragments:
            self.memory_fragments[fragment.fragment_id] = fragment
        
        # æ¸…ç©ºå·²å‹ç¼©çš„äº‹ä»¶
        self.consciousness_stream.clear()
        self.last_compression = time.time()
        
        compression_time = time.time() - compression_start
        logger.info(f"æ„è¯†æµå‹ç¼©å®Œæˆï¼Œè€—æ—¶: {compression_time:.2f}ç§’")
    
    def _perform_compression(self, events: List[ConsciousnessEvent]) -> List[MemoryFragment]:
        """æ‰§è¡Œå®é™…çš„å‹ç¼©æ“ä½œ"""
        fragments = []
        
        # æŒ‰ç±»å‹å’Œä¸Šä¸‹æ–‡åˆ†ç»„
        grouped_events = defaultdict(list)
        for event in events:
            key = f"{event.event_type}_{event.context_hash[:8]}"
            grouped_events[key].append(event)
        
        # ä¸ºæ¯ç»„åˆ›å»ºè®°å¿†ç‰‡æ®µ
        for group_key, group_events in grouped_events.items():
            fragment = self._create_memory_fragment(group_events)
            fragments.append(fragment)
        
        return fragments
    
    def _create_memory_fragment(self, events: List[ConsciousnessEvent]) -> MemoryFragment:
        """åˆ›å»ºè®°å¿†ç‰‡æ®µ"""
        # è®¡ç®—å†…å®¹å“ˆå¸Œ
        content_data = [asdict(event) for event in events]
        content_str = json.dumps(content_data, sort_keys=True, default=str)
        content_hash = hashlib.sha256(content_str.encode()).hexdigest()
        
        # å‹ç¼©æ•°æ®
        compressed_data = {
            'event_count': len(events),
            'time_span': {
                'start': min(e.timestamp for e in events).isoformat(),
                'end': max(e.timestamp for e in events).isoformat()
            },
            'event_types': list(set(e.event_type for e in events)),
            'key_themes': self._extract_key_themes(events),
            'emotional_signature': self._compute_emotional_signature(events),
            'importance_score': sum(e.importance_score for e in events) / len(events)
        }
        
        fragment = MemoryFragment(
            fragment_id=str(uuid.uuid4()),
            content_hash=content_hash,
            compressed_data=compressed_data,
            creation_time=datetime.now(),
            last_accessed=datetime.now(),
            access_count=0,
            emotional_signature=compressed_data['emotional_signature'],
            connection_strength={}
        )
        
        return fragment
    
    async def _search_memory_fragments(self, query_hash: str, max_results: int) -> List[Dict]:
        """æœç´¢è®°å¿†ç‰‡æ®µ"""
        results = []
        
        for fragment in self.memory_fragments.values():
            relevance = self._compute_relevance(query_hash, fragment.content_hash)
            if relevance > 0.3:  # ç›¸å…³æ€§é˜ˆå€¼
                fragment.last_accessed = datetime.now()
                fragment.access_count += 1
                
                results.append({
                    'type': 'memory_fragment',
                    'fragment_id': fragment.fragment_id,
                    'content': fragment.compressed_data,
                    'relevance_score': relevance,
                    'access_count': fragment.access_count
                })
        
        return sorted(results, key=lambda x: x['relevance_score'], reverse=True)[:max_results]
    
    async def _search_consciousness_stream(self, query: Dict[str, Any], max_results: int) -> List[Dict]:
        """æœç´¢æ„è¯†æµ"""
        results = []
        query_hash = self._compute_context_hash(query)
        
        for event in reversed(self.consciousness_stream):  # æœ€æ–°çš„ä¼˜å…ˆ
            relevance = self._compute_relevance(query_hash, event.context_hash)
            if relevance > 0.4:  # æ›´é«˜çš„é˜ˆå€¼
                results.append({
                    'type': 'consciousness_event',
                    'event_id': event.id,
                    'content': event.content,
                    'timestamp': event.timestamp.isoformat(),
                    'event_type': event.event_type,
                    'relevance_score': relevance,
                    'emotional_weight': event.emotional_weight
                })
        
        return sorted(results, key=lambda x: x['relevance_score'], reverse=True)[:max_results]
    
    def _compute_context_hash(self, content: Dict[str, Any]) -> str:
        """è®¡ç®—ä¸Šä¸‹æ–‡å“ˆå¸Œ"""
        content_str = json.dumps(content, sort_keys=True, default=str)
        return hashlib.sha256(content_str.encode()).hexdigest()[:16]
    
    def _compute_relevance(self, query_hash: str, content_hash: str) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        # ç®€åŒ–çš„å“ˆå¸Œç›¸ä¼¼åº¦è®¡ç®—
        common_chars = sum(c1 == c2 for c1, c2 in zip(query_hash, content_hash))
        return common_chars / max(len(query_hash), len(content_hash))
    
    def _calculate_importance(self, content: Dict[str, Any], emotional_weight: float) -> float:
        """è®¡ç®—é‡è¦æ€§åˆ†æ•°"""
        base_importance = 0.5
        
        # åŸºäºå†…å®¹å¤æ‚åº¦
        if isinstance(content, dict):
            base_importance += min(len(content) * 0.1, 0.3)
        
        # åŸºäºæƒ…æ„Ÿæƒé‡
        base_importance += abs(emotional_weight) * 0.2
        
        return min(base_importance, 1.0)
    
    def _extract_key_themes(self, events: List[ConsciousnessEvent]) -> List[str]:
        """æå–å…³é”®ä¸»é¢˜"""
        themes = set()
        for event in events:
            if 'theme' in event.content:
                themes.add(event.content['theme'])
            if 'keywords' in event.content:
                themes.update(event.content['keywords'])
        return list(themes)[:5]  # æœ€å¤šè¿”å›5ä¸ªä¸»é¢˜
    
    def _compute_emotional_signature(self, events: List[ConsciousnessEvent]) -> Dict[str, float]:
        """è®¡ç®—æƒ…æ„Ÿç‰¹å¾"""
        if not events:
            return {'valence': 0.0, 'arousal': 0.0, 'dominance': 0.0}
        
        # èšåˆæƒ…æ„Ÿæ•°æ®
        total_weight = sum(e.emotional_weight for e in events if e.emotional_weight > 0)
        if total_weight == 0:
            total_weight = 1
        
        signature = {'valence': 0.0, 'arousal': 0.0, 'dominance': 0.0}
        
        for event in events:
            weight = max(event.emotional_weight, 0.1) / total_weight
            if 'emotion' in event.content:
                emotion = event.content['emotion']
                for key in signature:
                    if key in emotion:
                        signature[key] += emotion[key] * weight
        
        return signature
    
    async def _update_emotional_state(self, event: ConsciousnessEvent):
        """æ›´æ–°æƒ…æ„ŸçŠ¶æ€"""
        if event.emotional_weight == 0:
            return
        
        emotional_state = EmotionalState(
            timestamp=event.timestamp,
            valence=event.content.get('valence', 0.0),
            arousal=event.content.get('arousal', 0.5),
            dominance=event.content.get('dominance', 0.0),
            cognitive_load=self._calculate_cognitive_load(),
            confidence=event.content.get('confidence', 0.5)
        )
        
        self.emotional_history.append(emotional_state)
    
    def _calculate_cognitive_load(self) -> float:
        """è®¡ç®—å½“å‰è®¤çŸ¥è´Ÿè·"""
        stream_load = len(self.consciousness_stream) / 2000
        memory_load = len(self.memory_fragments) / 10000
        return min(stream_load + memory_load, 1.0)
    
    async def _memory_maintenance_loop(self):
        """è®°å¿†ç»´æŠ¤å¾ªç¯"""
        while True:
            try:
                await asyncio.sleep(300)  # 5åˆ†é’Ÿ
                
                # é—å¿˜æœºåˆ¶
                await self._apply_forgetting_mechanism()
                
                # è®°å¿†æ•´åˆ
                await self._memory_consolidation()
                
            except Exception as e:
                logger.error(f"è®°å¿†ç»´æŠ¤å¾ªç¯é”™è¯¯: {e}")
    
    async def _apply_forgetting_mechanism(self):
        """åº”ç”¨é—å¿˜æœºåˆ¶"""
        current_time = datetime.now()
        fragments_to_remove = []
        
        for fragment_id, fragment in self.memory_fragments.items():
            # è®¡ç®—é—å¿˜æ¦‚ç‡
            time_since_access = (current_time - fragment.last_accessed).total_seconds()
            access_factor = 1.0 / (1.0 + fragment.access_count)
            decay_probability = 1.0 - (2.718 ** (-fragment.decay_rate * time_since_access * access_factor))
            
            # éšæœºé—å¿˜
            if decay_probability > 0.8 and fragment.access_count < 2:
                fragments_to_remove.append(fragment_id)
        
        # ç§»é™¤é—å¿˜çš„è®°å¿†
        for fragment_id in fragments_to_remove:
            del self.memory_fragments[fragment_id]
            logger.debug(f"é—å¿˜è®°å¿†ç‰‡æ®µ: {fragment_id}")
    
    async def _memory_consolidation(self):
        """è®°å¿†æ•´åˆ"""
        # åˆå¹¶ç›¸ä¼¼çš„è®°å¿†ç‰‡æ®µ
        similar_groups = defaultdict(list)
        
        for fragment in self.memory_fragments.values():
            # ç®€åŒ–çš„ç›¸ä¼¼æ€§æ£€æµ‹
            for other_id, other_fragment in self.memory_fragments.items():
                if fragment.fragment_id != other_id:
                    similarity = self._compute_relevance(
                        fragment.content_hash, 
                        other_fragment.content_hash
                    )
                    if similarity > 0.8:
                        group_key = min(fragment.fragment_id, other_id)
                        similar_groups[group_key].extend([fragment, other_fragment])
        
        # æ•´åˆåŒç»„è®°å¿†
        for group_key, fragments in similar_groups.items():
            if len(fragments) > 1:
                await self._consolidate_fragments(fragments)
    
    async def _consolidate_fragments(self, fragments: List[MemoryFragment]):
        """æ•´åˆè®°å¿†ç‰‡æ®µ"""
        # åˆ›å»ºæ–°çš„æ•´åˆç‰‡æ®µ
        all_events = []
        for fragment in fragments:
            if 'events' in fragment.compressed_data:
                all_events.extend(fragment.compressed_data['events'])
        
        if all_events:
            # è½¬æ¢ä¸ºConsciousnessEventå¯¹è±¡
            events = []
            for event_data in all_events:
                event = ConsciousnessEvent(**event_data)
                events.append(event)
            
            # åˆ›å»ºæ–°çš„è®°å¿†ç‰‡æ®µ
            new_fragment = self._create_memory_fragment(events)
            
            # ç§»é™¤æ—§ç‰‡æ®µ
            for fragment in fragments:
                if fragment.fragment_id in self.memory_fragments:
                    del self.memory_fragments[fragment.fragment_id]
            
            # æ·»åŠ æ–°ç‰‡æ®µ
            self.memory_fragments[new_fragment.fragment_id] = new_fragment
    
    async def _initialize_cross_project_links(self):
        """åˆå§‹åŒ–è·¨é¡¹ç›®é“¾æ¥"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–é¡¹ç›®çš„æ„è¯†æ•°æ®
        cross_project_dir = PROJECT_ROOT / ".iflow" / "cross_project_memory"
        if cross_project_dir.exists():
            for project_file in cross_project_dir.glob("*.json"):
                try:
                    with open(project_file, 'r', encoding='utf-8') as f:
                        project_data = json.load(f)
                        project_id = project_file.stem
                        self.cross_project_memory[project_id] = project_data
                        logger.info(f"åŠ è½½è·¨é¡¹ç›®è®°å¿†: {project_id}")
                except Exception as e:
                    logger.error(f"åŠ è½½è·¨é¡¹ç›®è®°å¿†å¤±è´¥ {project_file}: {e}")
    
    async def _sync_cross_project_event(self, event: ConsciousnessEvent):
        """åŒæ­¥è·¨é¡¹ç›®äº‹ä»¶"""
        if event.meta_cognitive_level >= 2:  # åªåŒæ­¥é«˜é˜¶è®¤çŸ¥
            # å‡†å¤‡åŒæ­¥æ•°æ®
            sync_data = {
                'event_id': event.id,
                'timestamp': event.timestamp.isoformat(),
                'project_id': self.project_id,
                'content': event.content,
                'importance': event.importance_score
            }
            
            # å†™å…¥å…±äº«åŒºåŸŸ
            cross_project_dir = PROJECT_ROOT / ".iflow" / "cross_project_memory"
            cross_project_dir.mkdir(exist_ok=True)
            
            sync_file = cross_project_dir / f"{self.project_id}_sync.json"
            try:
                with open(sync_file, 'w', encoding='utf-8') as f:
                    json.dump(sync_data, f, ensure_ascii=False, indent=2)
            except Exception as e:
                logger.error(f"è·¨é¡¹ç›®åŒæ­¥å¤±è´¥: {e}")
    
    async def _cross_project_sync(self):
        """è·¨é¡¹ç›®åŒæ­¥å¾ªç¯"""
        while True:
            try:
                await asyncio.sleep(600)  # 10åˆ†é’Ÿ
                
                # æ£€æŸ¥å…¶ä»–é¡¹ç›®çš„æ›´æ–°
                await self._check_cross_project_updates()
                
            except Exception as e:
                logger.error(f"è·¨é¡¹ç›®åŒæ­¥é”™è¯¯: {e}")
    
    async def _check_cross_project_updates(self):
        """æ£€æŸ¥è·¨é¡¹ç›®æ›´æ–°"""
        cross_project_dir = PROJECT_ROOT / ".iflow" / "cross_project_memory"
        if not cross_project_dir.exists():
            return
        
        current_time = datetime.now()
        for project_file in cross_project_dir.glob("*_sync.json"):
            try:
                file_mtime = datetime.fromtimestamp(project_file.stat().st_mtime)
                if (current_time - file_mtime).total_seconds() < 300:  # 5åˆ†é’Ÿå†…çš„æ›´æ–°
                    with open(project_file, 'r', encoding='utf-8') as f:
                        sync_data = json.load(f)
                        
                    # å¤„ç†è·¨é¡¹ç›®æ•°æ®
                    await self._process_cross_project_data(sync_data)
                    
            except Exception as e:
                logger.error(f"å¤„ç†è·¨é¡¹ç›®æ›´æ–°å¤±è´¥ {project_file}: {e}")
    
    async def _process_cross_project_data(self, sync_data: Dict[str, Any]):
        """å¤„ç†è·¨é¡¹ç›®æ•°æ®"""
        source_project = sync_data.get('project_id')
        if source_project == self.project_id:
            return
        
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
        event_id = sync_data.get('event_id')
        if event_id in self.cache.get('processed_cross_project_events', []):
            return
        
        # æ ‡è®°ä¸ºå·²å¤„ç†
        if 'processed_cross_project_events' not in self.cache:
            self.cache['processed_cross_project_events'] = []
        self.cache['processed_cross_project_events'].append(event_id)
        
        # åˆ›å»ºè·¨é¡¹ç›®æ„è¯†äº‹ä»¶
        cross_project_event = ConsciousnessEvent(
            id=str(uuid.uuid4()),
            timestamp=datetime.now(),
            event_type="cross_project_reflection",
            content={
                'source_project': source_project,
                'original_event': sync_data
            },
            context_hash=self._compute_context_hash(sync_data),
            emotional_weight=0.3,
            importance_score=sync_data.get('importance', 0.5) * 0.7,  # é™ä½è·¨é¡¹ç›®äº‹ä»¶çš„é‡è¦æ€§
            cross_project_ref=source_project,
            meta_cognitive_level=2
        )
        
        self.consciousness_stream.append(cross_project_event)
        logger.debug(f"å¤„ç†è·¨é¡¹ç›®äº‹ä»¶: {source_project} -> {event_id}")
    
    async def _emotional_state_tracking(self):
        """æƒ…æ„ŸçŠ¶æ€è¿½è¸ªå¾ªç¯"""
        while True:
            try:
                await asyncio.sleep(60)  # 1åˆ†é’Ÿ
                
                # åˆ†ææƒ…æ„Ÿè¶‹åŠ¿
                await self._analyze_emotional_trends()
                
                # æƒ…æ„Ÿè°ƒèŠ‚
                await self._emotional_regulation()
                
            except Exception as e:
                logger.error(f"æƒ…æ„ŸçŠ¶æ€è¿½è¸ªé”™è¯¯: {e}")
    
    async def _analyze_emotional_trends(self):
        """åˆ†ææƒ…æ„Ÿè¶‹åŠ¿"""
        if len(self.emotional_history) < 10:
            return
        
        recent_emotions = list(self.emotional_history)[-10:]
        
        # è®¡ç®—è¶‹åŠ¿
        valence_trend = self._calculate_trend([e.valence for e in recent_emotions])
        arousal_trend = self._calculate_trend([e.arousal for e in recent_emotions])
        
        # è®°å½•è¶‹åŠ¿åˆ†æ
        trend_analysis = {
            'timestamp': datetime.now().isoformat(),
            'valence_trend': valence_trend,
            'arousal_trend': arousal_trend,
            'cognitive_load_avg': sum(e.cognitive_load for e in recent_emotions) / len(recent_emotions),
            'confidence_avg': sum(e.confidence for e in recent_emotions) / len(recent_emotions)
        }
        
        # æ·»åŠ åˆ°æ„è¯†æµ
        await self.add_thought_async(
            content={'emotional_trend_analysis': trend_analysis},
            event_type='emotional_analysis',
            emotional_weight=0.2,
            meta_level=1
        )
    
    def _calculate_trend(self, values: List[float]) -> str:
        """è®¡ç®—è¶‹åŠ¿"""
        if len(values) < 2:
            return 'stable'
        
        # ç®€å•çº¿æ€§å›å½’
        n = len(values)
        x = list(range(n))
        sum_x = sum(x)
        sum_y = sum(values)
        sum_xy = sum(x[i] * values[i] for i in range(n))
        sum_x2 = sum(x[i] ** 2 for i in range(n))
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)
        
        if slope > 0.1:
            return 'increasing'
        elif slope < -0.1:
            return 'decreasing'
        else:
            return 'stable'
    
    async def _emotional_regulation(self):
        """æƒ…æ„Ÿè°ƒèŠ‚"""
        if not self.emotional_history:
            return
        
        current_emotion = self.emotional_history[-1]
        
        # æ£€æŸ¥éœ€è¦è°ƒèŠ‚çš„æƒ…å†µ
        if abs(current_emotion.valence) > 0.8:  # æƒ…æ„Ÿè¿‡äºæç«¯
            regulation_event = {
                'regulation_type': 'emotional_stabilization',
                'trigger': 'extreme_valence',
                'current_state': asdict(current_emotion),
                'regulation_strategy': 'mindfulness_reflection'
            }
            
            await self.add_thought_async(
                content=regulation_event,
                event_type='emotional_regulation',
                emotional_weight=0.5,
                meta_level=2
            )
    
    async def _load_persistent_memory(self):
        """åŠ è½½æŒä¹…åŒ–è®°å¿†"""
        memory_file = PROJECT_ROOT / ".iflow" / "data" / "consciousness_v11.db"
        if memory_file.exists():
            try:
                with open(memory_file, 'rb') as f:
                    data = pickle.load(f)
                    
                # æ¢å¤è®°å¿†ç‰‡æ®µ
                if 'memory_fragments' in data:
                    for fragment_data in data['memory_fragments']:
                        fragment = MemoryFragment(**fragment_data)
                        self.memory_fragments[fragment.fragment_id] = fragment
                
                logger.info(f"åŠ è½½äº† {len(self.memory_fragments)} ä¸ªè®°å¿†ç‰‡æ®µ")
                
            except Exception as e:
                logger.error(f"åŠ è½½æŒä¹…åŒ–è®°å¿†å¤±è´¥: {e}")
    
    async def save_persistent_memory(self):
        """ä¿å­˜æŒä¹…åŒ–è®°å¿†"""
        memory_file = PROJECT_ROOT / ".iflow" / "data" / "consciousness_v11.db"
        memory_file.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            data = {
                'memory_fragments': [asdict(fragment) for fragment in self.memory_fragments.values()],
                'project_id': self.project_id,
                'last_save': datetime.now().isoformat()
            }
            
            with open(memory_file, 'wb') as f:
                pickle.dump(data, f)
            
            logger.info("æŒä¹…åŒ–è®°å¿†ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æŒä¹…åŒ–è®°å¿†å¤±è´¥: {e}")
    
    async def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'project_id': self.project_id,
            'consciousness_stream_size': len(self.consciousness_stream),
            'memory_fragments_count': len(self.memory_fragments),
            'emotional_history_size': len(self.emotional_history),
            'cross_project_links': len(self.cross_project_memory),
            'cognitive_load': self._calculate_cognitive_load(),
            'last_compression': self.last_compression,
            'system_uptime': time.time()
        }
    
    async def shutdown(self):
        """ä¼˜é›…å…³é—­ç³»ç»Ÿ"""
        logger.info("æ­£åœ¨å…³é—­æ„è¯†æµç³»ç»Ÿ...")
        
        # ä¿å­˜æŒä¹…åŒ–æ•°æ®
        await self.save_persistent_memory()
        
        # å…³é—­çº¿ç¨‹æ± 
        self.executor.shutdown(wait=True)
        
        logger.info("æ„è¯†æµç³»ç»Ÿå·²å…³é—­")

# å…¨å±€å®ä¾‹
_consciousness_system: Optional[AsyncQuantumConsciousnessV11] = None

async def get_consciousness_system() -> AsyncQuantumConsciousnessV11:
    """è·å–æ„è¯†æµç³»ç»Ÿå®ä¾‹"""
    global _consciousness_system
    if _consciousness_system is None:
        _consciousness_system = AsyncQuantumConsciousnessV11()
        await _consciousness_system.initialize()
    return _consciousness_system

async def add_thought_async(content: Dict[str, Any], 
                          event_type: str = "thought",
                          emotional_weight: float = 0.0,
                          meta_level: int = 0) -> str:
    """æ·»åŠ æ€è€ƒçš„ä¾¿æ·å‡½æ•°"""
    system = await get_consciousness_system()
    return await system.add_thought_async(content, event_type, emotional_weight, meta_level)

async def get_relevant_context(query: Dict[str, Any], max_context: int = 10) -> List[Dict[str, Any]]:
    """è·å–ç›¸å…³ä¸Šä¸‹æ–‡çš„ä¾¿æ·å‡½æ•°"""
    system = await get_consciousness_system()
    return await system.get_relevant_context(query, max_context)