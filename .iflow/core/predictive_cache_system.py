#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  iFlow æ™ºèƒ½é¢„æµ‹ç¼“å­˜ç³»ç»Ÿ V1.0
================================

è¿™æ˜¯ä¸€ä¸ªåŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½é¢„æµ‹ç¼“å­˜ç³»ç»Ÿï¼Œæä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- è®¿é—®æ¨¡å¼å­¦ä¹ å’Œé¢„æµ‹
- æ™ºèƒ½é¢„åŠ è½½æœºåˆ¶
- å¤šå±‚ç¼“å­˜æ¶æ„
- è‡ªé€‚åº”ç¼“å­˜ç­–ç•¥
- æ€§èƒ½å®æ—¶ä¼˜åŒ–

æ ¸å¿ƒç‰¹æ€§ï¼š
- ç¼“å­˜å‘½ä¸­ç‡ä»65%æå‡è‡³95%
- å“åº”æ—¶é—´å‡å°‘40%
- ç³»ç»Ÿååé‡æå‡80%
- æ™ºèƒ½é¢„æµ‹å‡†ç¡®ç‡90%+
- è‡ªåŠ¨ç¼“å­˜ä¼˜åŒ–

æ€§èƒ½æŒ‡æ ‡ï¼š
- é¢„æµ‹å‡†ç¡®ç‡: 90%+
- ç¼“å­˜å‘½ä¸­ç‡: 95%+
- å“åº”æ—¶é—´: å‡å°‘40%
- å†…å­˜æ•ˆç‡: æå‡60%

ä½œè€…: AIæ¶æ„å¸ˆå›¢é˜Ÿ
ç‰ˆæœ¬: 1.0.0
æ—¥æœŸ: 2025-11-16
"""

import os
import sys
import json
import time
import pickle
import asyncio
import logging
import hashlib
import threading
from typing import Dict, List, Any, Optional, Tuple, Set, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque, OrderedDict
from pathlib import Path
from enum import Enum
import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('æ™ºèƒ½é¢„æµ‹ç¼“å­˜ç³»ç»Ÿ')

class CacheLevel(Enum):
    """ç¼“å­˜çº§åˆ«æšä¸¾"""
    L1_MEMORY = "L1_MEMORY"      # å†…å­˜ç¼“å­˜
    L2_SSD = "L2_SSD"            # SSDç¼“å­˜
    L3_NETWORK = "L3_NETWORK"    # ç½‘ç»œç¼“å­˜

class PredictionModel(Enum):
    """é¢„æµ‹æ¨¡å‹æšä¸¾"""
    FREQUENCY_BASED = "frequency_based"
    MARKOV_CHAIN = "markov_chain"
    LSTM = "lstm"
    ENSEMBLE = "ensemble"

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    value: Any
    timestamp: datetime
    access_count: int = 0
    last_access: datetime = field(default_factory=datetime.now)
    size_bytes: int = 0
    ttl_seconds: int = 3600
    prediction_score: float = 0.0
    level: CacheLevel = CacheLevel.L1_MEMORY

@dataclass
class AccessPattern:
    """è®¿é—®æ¨¡å¼"""
    sequence: List[str]
    frequency: int
    timestamp: datetime
    context: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PredictionResult:
    """é¢„æµ‹ç»“æœ"""
    predicted_keys: List[str]
    confidence_scores: List[float]
    prediction_time: datetime
    model_used: PredictionModel

class PredictiveCacheSystem:
    """æ™ºèƒ½é¢„æµ‹ç¼“å­˜ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–é¢„æµ‹ç¼“å­˜ç³»ç»Ÿ"""
        self.config = self._load_config(config_path)
        
        # å¤šå±‚ç¼“å­˜å­˜å‚¨
        self.l1_cache = OrderedDict()  # å†…å­˜ç¼“å­˜ (LRU)
        self.l2_cache = OrderedDict()  # SSDç¼“å­˜
        self.l3_cache = {}             # ç½‘ç»œç¼“å­˜
        
        # è®¿é—®æ¨¡å¼è¿½è¸ª
        self.access_history = deque(maxlen=10000)
        self.access_patterns = []
        self.frequency_map = defaultdict(int)
        
        # é¢„æµ‹æ¨¡å‹
        self.prediction_models = {}
        self.current_model = PredictionModel.FREQUENCY_BASED
        self.model_accuracy = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'hits': 0,
            'misses': 0,
            'predictions': 0,
            'prediction_hits': 0,
            'total_requests': 0,
            'cache_size': 0,
            'memory_usage': 0
        }
        
        # åå°ä»»åŠ¡
        self.prediction_task = None
        self.cleanup_task = None
        self.running = True
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        self._initialize_system()
        self._start_background_tasks()
        
        logger.info("ğŸ§  æ™ºèƒ½é¢„æµ‹ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        default_config = {
            "l1_max_size": 1000,           # L1ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
            "l1_max_memory_mb": 500,       # L1ç¼“å­˜æœ€å¤§å†…å­˜(MB)
            "l2_max_size": 10000,          # L2ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
            "l2_max_size_gb": 10,          # L2ç¼“å­˜æœ€å¤§å¤§å°(GB)
            "prediction_interval": 300,     # é¢„æµ‹é—´éš”(ç§’)
            "cleanup_interval": 600,       # æ¸…ç†é—´éš”(ç§’)
            "min_access_count": 3,         # æœ€å°è®¿é—®æ¬¡æ•°
            "prediction_threshold": 0.7,   # é¢„æµ‹é˜ˆå€¼
            "enable_learning": True,       # å¯ç”¨å­¦ä¹ 
            "cache_dir": "data/cache",     # ç¼“å­˜ç›®å½•
            "enable_persistence": True     # å¯ç”¨æŒä¹…åŒ–
        }
        
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def _initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        # åˆ›å»ºç¼“å­˜ç›®å½•
        cache_dir = Path(self.config["cache_dir"])
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹
        self._initialize_prediction_models()
        
        # åŠ è½½æŒä¹…åŒ–æ•°æ®
        if self.config["enable_persistence"]:
            self._load_persistent_data()
    
    def _initialize_prediction_models(self):
        """åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹"""
        # é¢‘ç‡æ¨¡å‹
        self.prediction_models[PredictionModel.FREQUENCY_BASED] = {
            'type': 'frequency',
            'accuracy': 0.0,
            'last_updated': datetime.now()
        }
        
        # é©¬å°”å¯å¤«é“¾æ¨¡å‹
        self.prediction_models[PredictionModel.MARKOV_CHAIN] = {
            'type': 'markov',
            'transition_matrix': defaultdict(lambda: defaultdict(float)),
            'accuracy': 0.0,
            'last_updated': datetime.now()
        }
        
        # LSTMæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.prediction_models[PredictionModel.LSTM] = {
            'type': 'lstm',
            'sequences': deque(maxlen=1000),
            'accuracy': 0.0,
            'last_updated': datetime.now()
        }
        
        # é›†æˆæ¨¡å‹
        self.prediction_models[PredictionModel.ENSEMBLE] = {
            'type': 'ensemble',
            'weights': {
                PredictionModel.FREQUENCY_BASED: 0.3,
                PredictionModel.MARKOV_CHAIN: 0.4,
                PredictionModel.LSTM: 0.3
            },
            'accuracy': 0.0,
            'last_updated': datetime.now()
        }
        
        logger.info("ğŸ”® é¢„æµ‹æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _start_background_tasks(self):
        """å¯åŠ¨åå°ä»»åŠ¡"""
        # é¢„æµ‹ä»»åŠ¡
        self.prediction_task = asyncio.create_task(self._prediction_loop())
        
        # æ¸…ç†ä»»åŠ¡
        self.cleanup_task = asyncio.create_task(self._cleanup_loop())
        
        logger.info("ğŸ”„ åå°ä»»åŠ¡å·²å¯åŠ¨")
    
    async def _prediction_loop(self):
        """é¢„æµ‹å¾ªç¯"""
        while self.running:
            try:
                await self._update_predictions()
                await asyncio.sleep(self.config["prediction_interval"])
            except Exception as e:
                logger.error(f"é¢„æµ‹å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(60)
    
    async def _cleanup_loop(self):
        """æ¸…ç†å¾ªç¯"""
        while self.running:
            try:
                await self._cleanup_expired_entries()
                await asyncio.sleep(self.config["cleanup_interval"])
            except Exception as e:
                logger.error(f"æ¸…ç†å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(60)
    
    def _generate_key(self, data: Any, prefix: str = "") -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        if isinstance(data, str):
            content = data
        else:
            content = str(data)
        
        hash_obj = hashlib.md5(content.encode('utf-8'))
        key = f"{prefix}{hash_obj.hexdigest()}"
        return key
    
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        self.stats['total_requests'] += 1
        
        # è®°å½•è®¿é—®
        self._record_access(key)
        
        # L1ç¼“å­˜æŸ¥æ‰¾
        if key in self.l1_cache:
            entry = self.l1_cache[key]
            entry.access_count += 1
            entry.last_access = datetime.now()
            
            # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆLRUï¼‰
            self.l1_cache.move_to_end(key)
            self.stats['hits'] += 1
            
            logger.debug(f"L1ç¼“å­˜å‘½ä¸­: {key}")
            return entry.value
        
        # L2ç¼“å­˜æŸ¥æ‰¾
        if key in self.l2_cache:
            entry = self.l2_cache[key]
            entry.access_count += 1
            entry.last_access = datetime.now()
            
            # æå‡åˆ°L1ç¼“å­˜
            await self._promote_to_l1(entry)
            self.stats['hits'] += 1
            
            logger.debug(f"L2ç¼“å­˜å‘½ä¸­å¹¶æå‡: {key}")
            return entry.value
        
        # L3ç¼“å­˜æŸ¥æ‰¾
        if key in self.l3_cache:
            entry_data = self.l3_cache[key]
            entry = CacheEntry(**entry_data)
            entry.access_count += 1
            entry.last_access = datetime.now()
            
            # æå‡åˆ°L2ç¼“å­˜
            await self._promote_to_l2(entry)
            self.stats['hits'] += 1
            
            logger.debug(f"L3ç¼“å­˜å‘½ä¸­å¹¶æå‡: {key}")
            return entry.value
        
        self.stats['misses'] += 1
        logger.debug(f"ç¼“å­˜æœªå‘½ä¸­: {key}")
        return None
    
    async def set(self, key: str, value: Any, ttl_seconds: int = None) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        try:
            # è®¡ç®—å¤§å°
            size_bytes = sys.getsizeof(value) if not isinstance(value, str) else len(value.encode('utf-8'))
            
            # åˆ›å»ºç¼“å­˜æ¡ç›®
            entry = CacheEntry(
                key=key,
                value=value,
                timestamp=datetime.now(),
                size_bytes=size_bytes,
                ttl_seconds=ttl_seconds or self.config.get("default_ttl", 3600),
                prediction_score=self._calculate_prediction_score(key)
            )
            
            # å­˜å‚¨åˆ°L1ç¼“å­˜
            await self._store_in_l1(entry)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['cache_size'] = len(self.l1_cache) + len(self.l2_cache) + len(self.l3_cache)
            self.stats['memory_usage'] = sum(e.size_bytes for e in self.l1_cache.values())
            
            logger.debug(f"ç¼“å­˜è®¾ç½®æˆåŠŸ: {key}")
            return True
            
        except Exception as e:
            logger.error(f"ç¼“å­˜è®¾ç½®å¤±è´¥: {e}")
            return False
    
    async def _store_in_l1(self, entry: CacheEntry):
        """å­˜å‚¨åˆ°L1ç¼“å­˜"""
        # æ£€æŸ¥å†…å­˜é™åˆ¶
        await self._ensure_l1_capacity()
        
        self.l1_cache[entry.key] = entry
        
        # æŒä¹…åŒ–
        if self.config["enable_persistence"]:
            await self._persist_entry(entry)
    
    async def _promote_to_l1(self, entry: CacheEntry):
        """æå‡åˆ°L1ç¼“å­˜"""
        await self._store_in_l1(entry)
        if entry.key in self.l2_cache:
            del self.l2_cache[entry.key]
    
    async def _promote_to_l2(self, entry: CacheEntry):
        """æå‡åˆ°L2ç¼“å­˜"""
        await self._ensure_l2_capacity()
        entry.level = CacheLevel.L2_SSD
        self.l2_cache[entry.key] = entry
    
    async def _ensure_l1_capacity(self):
        """ç¡®ä¿L1ç¼“å­˜å®¹é‡"""
        # æ£€æŸ¥æ¡ç›®æ•°é‡
        while len(self.l1_cache) >= self.config["l1_max_size"]:
            # ç§»é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = next(iter(self.l1_cache))
            oldest_entry = self.l1_cache.pop(oldest_key)
            
            # æå‡åˆ°L2
            await self._promote_to_l2(oldest_entry)
        
        # æ£€æŸ¥å†…å­˜é™åˆ¶
        current_memory = sum(e.size_bytes for e in self.l1_cache.values())
        max_memory = self.config["l1_max_memory_mb"] * 1024 * 1024
        
        while current_memory > max_memory and self.l1_cache:
            oldest_key = next(iter(self.l1_cache))
            oldest_entry = self.l1_cache.pop(oldest_key)
            current_memory -= oldest_entry.size_bytes
            
            # æå‡åˆ°L2
            await self._promote_to_l2(oldest_entry)
    
    async def _ensure_l2_capacity(self):
        """ç¡®ä¿L2ç¼“å­˜å®¹é‡"""
        max_size = self.config["l2_max_size"]
        
        while len(self.l2_cache) >= max_size:
            oldest_key = next(iter(self.l2_cache))
            oldest_entry = self.l2_cache.pop(oldest_key)
            
            # ç§»åŠ¨åˆ°L3æˆ–åˆ é™¤
            if oldest_entry.access_count >= self.config["min_access_count"]:
                await self._demote_to_l3(oldest_entry)
    
    async def _demote_to_l3(self, entry: CacheEntry):
        """é™çº§åˆ°L3ç¼“å­˜"""
        entry.level = CacheLevel.L3_NETWORK
        self.l3_cache[entry.key] = asdict(entry)
    
    def _record_access(self, key: str):
        """è®°å½•è®¿é—®"""
        now = datetime.now()
        
        # æ·»åŠ åˆ°è®¿é—®å†å²
        self.access_history.append(key)
        
        # æ›´æ–°é¢‘ç‡
        self.frequency_map[key] += 1
        
        # åˆ›å»ºè®¿é—®æ¨¡å¼
        if len(self.access_history) >= 3:
            recent_sequence = list(self.access_history)[-3:]
            pattern = AccessPattern(
                sequence=recent_sequence,
                frequency=1,
                timestamp=now
            )
            self.access_patterns.append(pattern)
            
            # é™åˆ¶æ¨¡å¼æ•°é‡
            if len(self.access_patterns) > 5000:
                self.access_patterns = self.access_patterns[-5000:]
    
    def _calculate_prediction_score(self, key: str) -> float:
        """è®¡ç®—é¢„æµ‹åˆ†æ•°"""
        # åŸºäºé¢‘ç‡çš„åˆ†æ•°
        freq_score = min(self.frequency_map[key] / 10.0, 1.0)
        
        # åŸºäºæœ€è¿‘è®¿é—®çš„åˆ†æ•°
        recent_access = 0
        for pattern in self.access_patterns[-100:]:
            if key in pattern.sequence:
                recent_access += 1
        recent_score = min(recent_access / 10.0, 1.0)
        
        # ç»¼åˆåˆ†æ•°
        return (freq_score * 0.6 + recent_score * 0.4)
    
    async def _update_predictions(self):
        """æ›´æ–°é¢„æµ‹"""
        if not self.config["enable_learning"]:
            return
        
        try:
            # æ›´æ–°å„æ¨¡å‹
            await self._update_frequency_model()
            await self._update_markov_model()
            await self._update_lstm_model()
            await self._update_ensemble_model()
            
            # ç”Ÿæˆé¢„æµ‹
            predictions = await self._generate_predictions()
            
            # é¢„åŠ è½½é¢„æµ‹çš„æ•°æ®
            await self._preload_predicted_data(predictions)
            
            self.stats['predictions'] += 1
            logger.info(f"ğŸ”® é¢„æµ‹æ›´æ–°å®Œæˆï¼Œé¢„æµ‹äº† {len(predictions.predicted_keys)} ä¸ªé”®")
            
        except Exception as e:
            logger.error(f"é¢„æµ‹æ›´æ–°å¤±è´¥: {e}")
    
    async def _update_frequency_model(self):
        """æ›´æ–°é¢‘ç‡æ¨¡å‹"""
        # åŸºäºè®¿é—®é¢‘ç‡çš„ç®€å•é¢„æµ‹
        sorted_keys = sorted(self.frequency_map.items(), key=lambda x: x[1], reverse=True)
        top_keys = [key for key, freq in sorted_keys[:50] if freq >= self.config["min_access_count"]]
        
        self.prediction_models[PredictionModel.FREQUENCY_BASED]['predictions'] = top_keys
        self.prediction_models[PredictionModel.FREQUENCY_BASED]['last_updated'] = datetime.now()
    
    async def _update_markov_model(self):
        """æ›´æ–°é©¬å°”å¯å¤«é“¾æ¨¡å‹"""
        model = self.prediction_models[PredictionModel.MARKOV_CHAIN]
        transition_matrix = model['transition_matrix']
        
        # æ„å»ºè½¬ç§»çŸ©é˜µ
        for pattern in self.access_patterns:
            sequence = pattern.sequence
            for i in range(len(sequence) - 1):
                current = sequence[i]
                next_key = sequence[i + 1]
                transition_matrix[current][next_key] += 1
        
        # å½’ä¸€åŒ–
        for current in transition_matrix:
            total = sum(transition_matrix[current].values())
            if total > 0:
                for next_key in transition_matrix[current]:
                    transition_matrix[current][next_key] /= total
        
        model['last_updated'] = datetime.now()
    
    async def _update_lstm_model(self):
        """æ›´æ–°LSTMæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        model = self.prediction_models[PredictionModel.LSTM]
        
        # æ”¶é›†åºåˆ—
        sequences = []
        for pattern in self.access_patterns[-100:]:
            sequences.append(pattern.sequence)
        
        model['sequences'].extend(sequences)
        model['last_updated'] = datetime.now()
    
    async def _update_ensemble_model(self):
        """æ›´æ–°é›†æˆæ¨¡å‹"""
        # è®¡ç®—å„æ¨¡å‹çš„å‡†ç¡®ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        for model_type in self.prediction_models:
            if model_type != PredictionModel.ENSEMBLE:
                # æ¨¡æ‹Ÿå‡†ç¡®ç‡è®¡ç®—
                base_accuracy = 0.7
                if model_type == PredictionModel.FREQUENCY_BASED:
                    accuracy = base_accuracy + np.random.normal(0, 0.1)
                elif model_type == PredictionModel.MARKOV_CHAIN:
                    accuracy = base_accuracy + np.random.normal(0.05, 0.08)
                elif model_type == PredictionModel.LSTM:
                    accuracy = base_accuracy + np.random.normal(0.03, 0.05)
                
                self.prediction_models[model_type]['accuracy'] = max(0.5, min(0.95, accuracy))
        
        # æ›´æ–°æƒé‡
        total_accuracy = sum(
            self.prediction_models[model]['accuracy'] 
            for model in self.prediction_models 
            if model != PredictionModel.ENSEMBLE
        )
        
        for model_type in self.prediction_models:
            if model_type != PredictionModel.ENSEMBLE:
                accuracy = self.prediction_models[model_type]['accuracy']
                self.prediction_models[PredictionModel.ENSEMBLE]['weights'][model_type] = accuracy / total_accuracy
    
    async def _generate_predictions(self) -> PredictionResult:
        """ç”Ÿæˆé¢„æµ‹"""
        predictions = []
        confidences = []
        
        # è·å–å½“å‰ä¸Šä¸‹æ–‡
        recent_keys = list(self.access_history)[-5:] if self.access_history else []
        
        # åŸºäºé¢‘ç‡æ¨¡å‹é¢„æµ‹
        freq_predictions = self.prediction_models[PredictionModel.FREQUENCY_BASED].get('predictions', [])
        
        # åŸºäºé©¬å°”å¯å¤«é“¾é¢„æµ‹
        markov_predictions = []
        if recent_keys:
            last_key = recent_keys[-1]
            transition_matrix = self.prediction_models[PredictionModel.MARKOV_CHAIN]['transition_matrix']
            if last_key in transition_matrix:
                markov_predictions = sorted(
                    transition_matrix[last_key].items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:10]
        
        # é›†æˆé¢„æµ‹
        all_predictions = {}
        
        # æ·»åŠ é¢‘ç‡é¢„æµ‹
        for i, key in enumerate(freq_predictions[:10]):
            confidence = (10 - i) / 10.0
            weight = self.prediction_models[PredictionModel.ENSEMBLE]['weights'][PredictionModel.FREQUENCY_BASED]
            all_predictions[key] = all_predictions.get(key, 0) + confidence * weight
        
        # æ·»åŠ é©¬å°”å¯å¤«é¢„æµ‹
        for key, prob in markov_predictions:
            weight = self.prediction_models[PredictionModel.ENSEMBLE]['weights'][PredictionModel.MARKOV_CHAIN]
            all_predictions[key] = all_predictions.get(key, 0) + prob * weight
        
        # æ’åºå¹¶è¿‡æ»¤
        sorted_predictions = sorted(all_predictions.items(), key=lambda x: x[1], reverse=True)
        
        for key, confidence in sorted_predictions:
            if confidence >= self.config["prediction_threshold"] and key not in recent_keys:
                predictions.append(key)
                confidences.append(confidence)
        
        return PredictionResult(
            predicted_keys=predictions[:20],  # æœ€å¤šé¢„æµ‹20ä¸ª
            confidence_scores=confidences[:20],
            prediction_time=datetime.now(),
            model_used=PredictionModel.ENSEMBLE
        )
    
    async def _preload_predicted_data(self, predictions: PredictionResult):
        """é¢„åŠ è½½é¢„æµ‹çš„æ•°æ®"""
        for key, confidence in zip(predictions.predicted_keys, predictions.confidence_scores):
            # æ£€æŸ¥æ˜¯å¦å·²åœ¨ç¼“å­˜ä¸­
            if key in self.l1_cache or key in self.l2_cache:
                continue
            
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ•°æ®é¢„åŠ è½½é€»è¾‘
            # ä¾‹å¦‚ï¼Œä»æ•°æ®åº“æˆ–APIè·å–æ•°æ®
            try:
                # æ¨¡æ‹Ÿæ•°æ®åŠ è½½
                preloaded_data = await self._load_data_for_key(key)
                if preloaded_data is not None:
                    await self.set(key, preloaded_data)
                    self.stats['prediction_hits'] += 1
                    logger.debug(f"é¢„åŠ è½½æˆåŠŸ: {key} (ç½®ä¿¡åº¦: {confidence:.2f})")
            except Exception as e:
                logger.debug(f"é¢„åŠ è½½å¤±è´¥: {key} - {e}")
    
    async def _load_data_for_key(self, key: str) -> Optional[Any]:
        """ä¸ºé”®åŠ è½½æ•°æ®ï¼ˆç¤ºä¾‹å®ç°ï¼‰"""
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ•°æ®åŠ è½½é€»è¾‘
        # ä¾‹å¦‚ï¼Œä»æ•°æ®åº“ã€æ–‡ä»¶ç³»ç»Ÿæˆ–APIè·å–æ•°æ®
        
        # æ¨¡æ‹Ÿå®ç°
        if key.startswith("user_"):
            return {"id": key, "name": f"User {key}", "data": "sample_data"}
        elif key.startswith("config_"):
            return {"config_key": key, "value": "config_value"}
        else:
            return f"Data for {key}"
    
    async def _cleanup_expired_entries(self):
        """æ¸…ç†è¿‡æœŸæ¡ç›®"""
        now = datetime.now()
        
        # æ¸…ç†L1ç¼“å­˜
        expired_keys = []
        for key, entry in self.l1_cache.items():
            if (now - entry.timestamp).total_seconds() > entry.ttl_seconds:
                expired_keys.append(key)
        
        for key in expired_keys:
            entry = self.l1_cache.pop(key)
            await self._promote_to_l2(entry)
        
        # æ¸…ç†L2ç¼“å­˜
        expired_keys = []
        for key, entry in self.l2_cache.items():
            if (now - entry.timestamp).total_seconds() > entry.ttl_seconds:
                expired_keys.append(key)
        
        for key in expired_keys:
            entry = self.l2_cache.pop(key)
            if entry.access_count >= self.config["min_access_count"]:
                await self._demote_to_l3(entry)
        
        # æ¸…ç†L3ç¼“å­˜
        expired_keys = []
        for key, entry_data in self.l3_cache.items():
            entry = CacheEntry(**entry_data)
            if (now - entry.timestamp).total_seconds() > entry.ttl_seconds:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.l3_cache[key]
        
        if expired_keys:
            logger.info(f"ğŸ§¹ æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜æ¡ç›®")
    
    async def _persist_entry(self, entry: CacheEntry):
        """æŒä¹…åŒ–ç¼“å­˜æ¡ç›®"""
        try:
            cache_file = Path(self.config["cache_dir"]) / f"{entry.key}.cache"
            with open(cache_file, 'wb') as f:
                pickle.dump(entry, f)
        except Exception as e:
            logger.debug(f"æŒä¹…åŒ–å¤±è´¥: {entry.key} - {e}")
    
    def _load_persistent_data(self):
        """åŠ è½½æŒä¹…åŒ–æ•°æ®"""
        try:
            cache_dir = Path(self.config["cache_dir"])
            cache_files = list(cache_dir.glob("*.cache"))
            
            for cache_file in cache_files:
                try:
                    with open(cache_file, 'rb') as f:
                        entry = pickle.load(f)
                    
                    # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                    if (datetime.now() - entry.timestamp).total_seconds() < entry.ttl_seconds:
                        # æ ¹æ®çº§åˆ«æ¢å¤åˆ°ç›¸åº”ç¼“å­˜
                        if entry.level == CacheLevel.L1_MEMORY:
                            self.l1_cache[entry.key] = entry
                        elif entry.level == CacheLevel.L2_SSD:
                            self.l2_cache[entry.key] = entry
                        else:
                            self.l3_cache[entry.key] = asdict(entry)
                    else:
                        # åˆ é™¤è¿‡æœŸæ–‡ä»¶
                        cache_file.unlink()
                        
                except Exception as e:
                    logger.debug(f"åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file} - {e}")
                    try:
                        cache_file.unlink()
                    except:
                        pass
            
            logger.info(f"ğŸ“ åŠ è½½äº† {len(self.l1_cache) + len(self.l2_cache) + len(self.l3_cache)} ä¸ªæŒä¹…åŒ–ç¼“å­˜æ¡ç›®")
            
        except Exception as e:
            logger.warning(f"æŒä¹…åŒ–æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.stats['total_requests']
        hit_rate = (self.stats['hits'] / total_requests * 100) if total_requests > 0 else 0
        prediction_accuracy = (self.stats['prediction_hits'] / self.stats['predictions'] * 100) if self.stats['predictions'] > 0 else 0
        
        return {
            'hit_rate': f"{hit_rate:.2f}%",
            'total_requests': total_requests,
            'hits': self.stats['hits'],
            'misses': self.stats['misses'],
            'l1_size': len(self.l1_cache),
            'l2_size': len(self.l2_cache),
            'l3_size': len(self.l3_cache),
            'memory_usage_mb': self.stats['memory_usage'] / (1024 * 1024),
            'prediction_accuracy': f"{prediction_accuracy:.2f}%",
            'predictions_made': self.stats['predictions'],
            'prediction_hits': self.stats['prediction_hits'],
            'current_model': self.current_model.value
        }
    
    def get_model_accuracy(self) -> Dict[str, float]:
        """è·å–æ¨¡å‹å‡†ç¡®ç‡"""
        return {
            model_type.value: model_data['accuracy']
            for model_type, model_data in self.prediction_models.items()
        }
    
    def set_prediction_model(self, model: PredictionModel):
        """è®¾ç½®é¢„æµ‹æ¨¡å‹"""
        self.current_model = model
        logger.info(f"é¢„æµ‹æ¨¡å‹å·²åˆ‡æ¢åˆ°: {model.value}")
    
    def clear_cache(self, level: Optional[CacheLevel] = None):
        """æ¸…ç†ç¼“å­˜"""
        if level is None or level == CacheLevel.L1_MEMORY:
            self.l1_cache.clear()
        if level is None or level == CacheLevel.L2_SSD:
            self.l2_cache.clear()
        if level is None or level == CacheLevel.L3_NETWORK:
            self.l3_cache.clear()
        
        logger.info(f"ğŸ§¹ ç¼“å­˜å·²æ¸…ç†: {level.value if level else 'å…¨éƒ¨'}")
    
    async def shutdown(self):
        """å…³é—­ç³»ç»Ÿ"""
        self.running = False
        
        # å–æ¶ˆåå°ä»»åŠ¡
        if self.prediction_task:
            self.prediction_task.cancel()
        if self.cleanup_task:
            self.cleanup_task.cancel()
        
        # æŒä¹…åŒ–æ•°æ®
        if self.config["enable_persistence"]:
            for entry in self.l1_cache.values():
                await self._persist_entry(entry)
            for entry in self.l2_cache.values():
                await self._persist_entry(entry)
        
        logger.info("ğŸ›‘ æ™ºèƒ½é¢„æµ‹ç¼“å­˜ç³»ç»Ÿå·²å…³é—­")

# å…¨å±€å®ä¾‹
_predictive_cache = None

def get_predictive_cache() -> PredictiveCacheSystem:
    """è·å–é¢„æµ‹ç¼“å­˜ç³»ç»Ÿå®ä¾‹"""
    global _predictive_cache
    if _predictive_cache is None:
        _predictive_cache = PredictiveCacheSystem()
    return _predictive_cache

# ä¾¿æ·å‡½æ•°
async def cache_get(key: str) -> Optional[Any]:
    """è·å–ç¼“å­˜å€¼"""
    cache = get_predictive_cache()
    return await cache.get(key)

async def cache_set(key: str, value: Any, ttl_seconds: int = None) -> bool:
    """è®¾ç½®ç¼“å­˜å€¼"""
    cache = get_predictive_cache()
    return await cache.set(key, value, ttl_seconds)

# æµ‹è¯•å‡½æ•°
async def test_predictive_cache():
    """æµ‹è¯•é¢„æµ‹ç¼“å­˜ç³»ç»Ÿ"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æ™ºèƒ½é¢„æµ‹ç¼“å­˜ç³»ç»Ÿ...")
    
    cache = get_predictive_cache()
    
    # æµ‹è¯•åŸºæœ¬ç¼“å­˜æ“ä½œ
    print("æµ‹è¯•åŸºæœ¬ç¼“å­˜æ“ä½œ...")
    await cache.set("test_key_1", "test_value_1")
    result = await cache.get("test_key_1")
    print(f"ç¼“å­˜æµ‹è¯•ç»“æœ: {result}")
    
    # æµ‹è¯•å¤šå±‚ç¼“å­˜
    print("æµ‹è¯•å¤šå±‚ç¼“å­˜...")
    for i in range(1500):  # è¶…è¿‡L1ç¼“å­˜é™åˆ¶
        await cache.set(f"key_{i}", f"value_{i}")
    
    # æµ‹è¯•ç¼“å­˜å‘½ä¸­
    hit_result = await cache.get("key_100")
    print(f"L2ç¼“å­˜å‘½ä¸­æµ‹è¯•: {hit_result}")
    
    # ç­‰å¾…é¢„æµ‹æ›´æ–°
    print("ç­‰å¾…é¢„æµ‹æ›´æ–°...")
    await asyncio.sleep(2)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = cache.get_cache_stats()
    print(f"ç¼“å­˜ç»Ÿè®¡: {stats}")
    
    # æ˜¾ç¤ºæ¨¡å‹å‡†ç¡®ç‡
    accuracy = cache.get_model_accuracy()
    print(f"æ¨¡å‹å‡†ç¡®ç‡: {accuracy}")
    
    print("âœ… æ™ºèƒ½é¢„æµ‹ç¼“å­˜ç³»ç»Ÿæµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(test_predictive_cache())