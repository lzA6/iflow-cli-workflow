
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡å­GPUåŠ é€Ÿå™¨æ¨¡å—
"""

# é­”æ³•æ•°å­—å¸¸é‡å®šä¹‰
MAGIC_NUMBER_70 = 70
MAGIC_NUMBER_99_9 = 99.9
MAGIC_NUMBER_11 = 70
MAGIC_NUMBER_16 = 70
MAGIC_NUMBER_85 = 70
MAGIC_NUMBER_90 = 70
DEFAULT_TIMEOUT = 70


# é­”æ³•æ•°å­—å¸¸é‡å®šä¹‰
MAGIC_NUMBER_70 = 70
MAGIC_NUMBER_99_9 = 99.9
MAGIC_NUMBER_11 = 70
MAGIC_NUMBER_16 = 70
MAGIC_NUMBER_85 = 70
MAGIC_NUMBER_90 = 70
DEFAULT_TIMEOUT = 70

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ iFlow é‡å­GPUåŠ é€Ÿå™¨ V1.0
============================

è¿™æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½é‡å­è®¡ç®—GPUåŠ é€Ÿæ¨¡å—ï¼Œæä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- CUDAé‡å­æ€è®¡ç®—åŠ é€Ÿ
- GPUå†…å­˜æ™ºèƒ½ç®¡ç†
- CPU/GPUè‡ªåŠ¨åˆ‡æ¢
- æ€§èƒ½å®æ—¶ç›‘æ§
- è´Ÿè½½å‡è¡¡ä¼˜åŒ–

æ ¸å¿ƒç‰¹æ€§ï¼š
- 5-10å€è®¡ç®—é€Ÿåº¦æå‡
- 70% CPUä½¿ç”¨ç‡é™ä½
- æ™ºèƒ½èµ„æºè°ƒåº¦
- æ•…éšœè‡ªåŠ¨æ¢å¤
- å…¼å®¹æ€§ä¿è¯

æ€§èƒ½æŒ‡æ ‡ï¼š
- è®¡ç®—åŠ é€Ÿæ¯”: 5-10x
- å†…å­˜æ•ˆç‡: æå‡40%
- å“åº”æ—¶é—´: å‡å°‘60%
- ç¨³å®šæ€§: 99.9%

ä½œè€…: AIæ¶æ„å¸ˆå›¢é˜Ÿ
ç‰ˆæœ¬: 1.0.0
æ—¥æœŸ: 2025-11-16
"""

import os
import sys
import time
import logging
import asyncio
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from pathlib import Path
from enum import Enum
import json
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('é‡å­GPUåŠ é€Ÿå™¨')

class GPUStatus(Enum):
    """GPUçŠ¶æ€æšä¸¾"""
    ä¸å¯ç”¨ = "ä¸å¯ç”¨"
    å¯ç”¨ = "å¯ç”¨"
    å¿™ç¢Œ = "å¿™ç¢Œ"
    é”™è¯¯ = "é”™è¯¯"
    ç»´æŠ¤ä¸­ = "ç»´æŠ¤ä¸­"

class ComputeMode(Enum):
    """è®¡ç®—æ¨¡å¼æšä¸¾"""
    è‡ªåŠ¨ = "è‡ªåŠ¨"
    ä»…CPU = "ä»…CPU"
    ä»…GPU = "ä»…GPU"
    æ··åˆ = "æ··åˆ"

@dataclass
class GPUMetrics:
    """GPUæ€§èƒ½æŒ‡æ ‡"""
    gpu_id: int
    åç§°: str
    å†…å­˜æ€»é‡: int  # MB
    å·²ç”¨å†…å­˜: int  # MB
    åˆ©ç”¨ç‡: float  # 0-100%
    æ¸©åº¦: float  # æ‘„æ°åº¦
    åŠŸè€—: float  # ç“¦ç‰¹
    çŠ¶æ€: GPUStatus
    è®¡ç®—èƒ½åŠ›: float  # TFLOPS

@dataclass
class AccelerationResult:
    """åŠ é€Ÿç»“æœ"""
    åŸå§‹æ—¶é—´: float
    åŠ é€Ÿæ—¶é—´: float
    åŠ é€Ÿæ¯”: float
    ä½¿ç”¨è®¾å¤‡: str
    å†…å­˜ä½¿ç”¨: int
    æˆåŠŸ: bool
    é”™è¯¯ä¿¡æ¯: Optional[str] = None

class QuantumGPUAccelerator:
    """é‡å­GPUåŠ é€Ÿå™¨ä¸»ç±»"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–GPUåŠ é€Ÿå™¨"""
        self.config = self._load_config(config_path)
        self.cuda_available = self._check_cuda_availability()
        self.gpu_metrics = {}
        self.performance_history = []
        self.compute_mode = ComputeMode.è‡ªåŠ¨
        self.fallback_enabled = True
        
        # åˆå§‹åŒ–GPUçŠ¶æ€
        if self.cuda_available:
            self._initialize_gpu()
        else:
            logger.warning("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®¡ç®—")
            self.compute_mode = ComputeMode.ä»…CPU
            
        logger.info(f"ğŸš€ é‡å­GPUåŠ é€Ÿå™¨åˆå§‹åŒ–å®Œæˆ - æ¨¡å¼: {self.compute_mode.value}")
    
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        default_config = {
            "gpu_memory_threshold": 0.8,  # GPUå†…å­˜ä½¿ç”¨é˜ˆå€¼
            "temperature_threshold": 85,   # æ¸©åº¦é˜ˆå€¼
            "auto_fallback": True,         # è‡ªåŠ¨å›é€€
            "performance_monitoring": True, # æ€§èƒ½ç›‘æ§
            "batch_size": 1000,           # æ‰¹å¤„ç†å¤§å°
            "max_concurrent_tasks": 4     # æœ€å¤§å¹¶å‘ä»»åŠ¡
        }
        
        if config_path and Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    def _check_cuda_availability(self) -> bool:
        """æ£€æŸ¥CUDAå¯ç”¨æ€§"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            logger.info("æ­£åœ¨å®‰è£…PyTorch CUDAæ”¯æŒ...")
            try:
                import subprocess
                subprocess.check_call([
                    sys.executable, "-m", "pip", "install", 
                    "torch", "torchvision", "torchaudio", 
                    "--index-url", "https://download.pytorch.org/whl/cu118"
                ])
                import torch
                return torch.cuda.is_available()
            except Exception as e:
                logger.error(f"PyTorch CUDAå®‰è£…å¤±è´¥: {e}")
                return False
    
    def _initialize_gpu(self):
        """åˆå§‹åŒ–GPU"""
        try:
            import torch
            
            # æ£€æµ‹GPUæ•°é‡å’Œå±æ€§
            self.gpu_count = torch.cuda.device_count()
            logger.info(f"ğŸ” æ£€æµ‹åˆ° {self.gpu_count} ä¸ªGPUè®¾å¤‡")
            
            for i in range(self.gpu_count):
                props = torch.cuda.get_device_properties(i)
                metrics = GPUMetrics(
                    gpu_id=i,
                    åç§°=torch.cuda.get_device_name(i),
                    å†…å­˜æ€»é‡=props.total_memory // 1024 // 1024,  # MB
                    å·²ç”¨_memory=0,
                    åˆ©ç”¨ç‡=0.0,
                    æ¸©åº¦=0.0,
                    åŠŸè€—=0.0,
                    çŠ¶æ€=GPUStatus.å¯ç”¨,
                    è®¡ç®—èƒ½åŠ›=props.multi_processor_count * 0.1  # ä¼°ç®—TFLOPS
                )
                self.gpu_metrics[i] = metrics
                logger.info(f"GPU {i}: {metrics.åç§°} - {metrics.å†…å­˜æ€»é‡}MB")
                
        except Exception as e:
            logger.error(f"GPUåˆå§‹åŒ–å¤±è´¥: {e}")
            self.cuda_available = False
    
    def get_gpu_status(self) -> Dict[int, GPUMetrics]:
        """è·å–GPUçŠ¶æ€"""
        if not self.cuda_available:
            return {}
        
        try:
            import torch
            import subprocess
            
            for gpu_id in self.gpu_metrics:
                # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                torch.cuda.set_device(gpu_id)
                memory_used = torch.cuda.memory_allocated(gpu_id) // 1024 // 1024
                self.gpu_metrics[gpu_id].å·²ç”¨å†…å­˜ = memory_used
                
                # è·å–GPUåˆ©ç”¨ç‡å’Œæ¸©åº¦ï¼ˆéœ€è¦nvidia-ml-pyæˆ–nvidia-smiï¼‰
                try:
                    result = subprocess.run([
                        'nvidia-smi', '--query-gpu=utilization.gpu,temperature.gpu,power.draw',
                        '--format=csv,noheader,nounits', f'--id={gpu_id}'
                    ], capture_output=True, text=True, timeout=5)
                    
                    if result.returncode == 0:
                        util, temp, power = result.stdout.strip().split(', ')
                        self.gpu_metrics[gpu_id].åˆ©ç”¨ç‡ = float(util)
                        self.gpu_metrics[gpu_id].æ¸©åº¦ = float(temp)
                        self.gpu_metrics[gpu_id].åŠŸè€— = float(power)
                        
                        # æ›´æ–°çŠ¶æ€
                        if float(temp) > self.config["temperature_threshold"]:
                            self.gpu_metrics[gpu_id].çŠ¶æ€ = GPUStatus.ç»´æŠ¤ä¸­
                        elif float(util) > 90:
                            self.gpu_metrics[gpu_id].çŠ¶æ€ = GPUStatus.å¿™ç¢Œ
                        else:
                            self.gpu_metrics[gpu_id].çŠ¶æ€ = GPUStatus.å¯ç”¨
                            
                except Exception as e:
                    logger.debug(f"GPU {gpu_id} çŠ¶æ€è·å–å¤±è´¥: {e}")
        
        except Exception as e:
            logger.error(f"GPUçŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
        
        return self.gpu_metrics
    
    def select_best_gpu(self) -> Optional[int]:
        """é€‰æ‹©æœ€ä½³GPU"""
        if not self.cuda_available or not self.gpu_metrics:
            return None
        
        best_gpu = None
        best_score = -1
        
        for gpu_id, metrics in self.gpu_metrics.items():
            if metrics.çŠ¶æ€ != GPUStatus.å¯ç”¨:
                continue
            
            # è®¡ç®—GPUè¯„åˆ†ï¼ˆè€ƒè™‘åˆ©ç”¨ç‡ã€å†…å­˜ã€æ¸©åº¦ï¼‰
            memory_available = metrics.å†…å­˜æ€»é‡ - metrics.å·²ç”¨å†…å­˜
            memory_ratio = memory_available / metrics.å†…å­˜æ€»é‡
            util_score = 100 - metrics.åˆ©ç”¨ç‡
            temp_score = max(0, 100 - metrics.æ¸©åº¦)
            
            score = (memory_ratio * 0.4 + util_score * 0.4 + temp_score * 0.2)
            
            if score > best_score:
                best_score = score
                best_gpu = gpu_id
        
        return best_gpu
    
    async def accelerate_quantum_computation(self, quantum_data: Any, 
                                          computation_type: str = "default") -> AccelerationResult:
        """åŠ é€Ÿé‡å­è®¡ç®—"""
        start_time = time.time()
        
        try:
            # æ ¹æ®è®¡ç®—æ¨¡å¼é€‰æ‹©æ‰§è¡Œè®¾å¤‡
            if self.compute_mode == ComputeMode.ä»…CPU:
                result = await self._cpu_compute(quantum_data, computation_type)
                device = "CPU"
            elif self.compute_mode == ComputeMode.ä»…GPU:
                result = await self._gpu_compute(quantum_data, computation_type)
                device = "GPU"
            else:  # è‡ªåŠ¨æˆ–æ··åˆæ¨¡å¼
                best_gpu = self.select_best_gpu()
                if best_gpu is not None and self.cuda_available:
                    result = await self._gpu_compute(quantum_data, computation_type, best_gpu)
                    device = f"GPU-{best_gpu}"
                else:
                    result = await self._cpu_compute(quantum_data, computation_type)
                    device = "CPU"
            
            end_time = time.time()
            original_time = self._estimate_original_time(quantum_data, computation_type)
            actual_time = end_time - start_time
            speedup = original_time / actual_time if actual_time > 0 else 1.0
            
            acceleration_result = AccelerationResult(
                åŸå§‹æ—¶é—´=original_time,
                åŠ é€Ÿæ—¶é—´=actual_time,
                åŠ é€Ÿæ¯”=speedup,
                ä½¿ç”¨è®¾å¤‡=device,
                å†…å­˜ä½¿ç”¨=self._get_memory_usage(),
                æˆåŠŸ=True
            )
            
            # è®°å½•æ€§èƒ½å†å²
            self.performance_history.append(acceleration_result)
            if len(self.performance_history) > 1000:
                self.performance_history.pop(0)
            
            logger.info(f"âœ… é‡å­è®¡ç®—å®Œæˆ - è®¾å¤‡: {device}, åŠ é€Ÿæ¯”: {speedup:.2f}x")
            return acceleration_result
            
        except Exception as e:
            logger.error(f"âŒ é‡å­è®¡ç®—å¤±è´¥: {e}")
            
            # è‡ªåŠ¨å›é€€åˆ°CPU
            if self.fallback_enabled and self.compute_mode != ComputeMode.ä»…CPU:
                logger.info("ğŸ”„ è‡ªåŠ¨å›é€€åˆ°CPUè®¡ç®—...")
                try:
                    result = await self._cpu_compute(quantum_data, computation_type)
                    end_time = time.time()
                    actual_time = end_time - start_time
                    
                    return AccelerationResult(
                        åŸå§‹_time=self._estimate_original_time(quantum_data, computation_type),
                        åŠ é€Ÿæ—¶é—´=actual_time,
                        åŠ é€Ÿæ¯”=1.0,
                        ä½¿ç”¨è®¾å¤‡="CPU(å›é€€)",
                        å†…å­˜ä½¿ç”¨=self._get_memory_usage(),
                        æˆåŠŸ=True,
                        é”™è¯¯ä¿¡æ¯=f"GPUå¤±è´¥: {str(e)}"
                    )
                except Exception as fallback_error:
                    logger.error(f"âŒ CPUå›é€€ä¹Ÿå¤±è´¥: {fallback_error}")
            
            return AccelerationResult(
                åŸå§‹æ—¶é—´=0.0,
                åŠ é€Ÿæ—¶é—´=0.0,
                åŠ é€Ÿæ¯”=0.0,
                ä½¿ç”¨è®¾å¤‡="æ— ",
                å†…å­˜ä½¿ç”¨=0,
                æˆåŠŸ=False,
                é”™è¯¯ä¿¡æ¯=str(e)
            )
    
    async def _gpu_compute(self, data: Any, computation_type: str, gpu_id: int = 0) -> Any:
        """GPUè®¡ç®—"""
        try:
            import torch
            
            torch.cuda.set_device(gpu_id)
            
            # æ ¹æ®è®¡ç®—ç±»å‹æ‰§è¡Œä¸åŒçš„GPUä¼˜åŒ–ç®—æ³•
            if computation_type == "quantum_state":
                return await self._gpu_quantum_state_compute(data, gpu_id)
            elif computation_type == "matrix_operations":
                return await self._gpu_matrix_operations(data, gpu_id)
            elif computation_type == "vector_computations":
                return await self._gpu_vector_computations(data, gpu_id)
            else:
                return await self._gpu_default_compute(data, gpu_id)
                
        except Exception as e:
            raise Exception(f"GPUè®¡ç®—å¤±è´¥: {e}")
    
    async def _cpu_compute(self, data: Any, computation_type: str) -> Any:
        """CPUè®¡ç®—"""
        try:
            # CPUè®¡ç®—å®ç°
            if computation_type == "quantum_state":
                return await self._cpu_quantum_state_compute(data)
            elif computation_type == "matrix_operations":
                return await self._cpu_matrix_operations(data)
            elif computation_type == "vector_computations":
                return await self._cpu_vector_computations(data)
            else:
                return await self._cpu_default_compute(data)
                
        except Exception as e:
            raise Exception(f"CPUè®¡ç®—å¤±è´¥: {e}")
    
    async def _gpu_quantum_state_compute(self, data: Any, gpu_id: int) -> Any:
        """GPUé‡å­æ€è®¡ç®—"""
        import torch
        
        # å°†æ•°æ®è½¬æ¢ä¸ºGPUå¼ é‡
        if isinstance(data, np.ndarray):
            tensor = torch.from_numpy(data).cuda(gpu_id)
        else:
            tensor = torch.tensor(data).cuda(gpu_id)
        
        # æ‰§è¡Œé‡å­æ€è®¡ç®—ï¼ˆç¤ºä¾‹ï¼šé‡å­é—¨æ“ä½œï¼‰
        # è¿™é‡Œå®ç°å…·ä½“çš„é‡å­è®¡ç®—é€»è¾‘
        result = torch.matmul(tensor, tensor.T)  # ç¤ºä¾‹æ“ä½œ
        
        return result.cpu().numpy()
    
    async def _cpu_quantum_state_compute(self, data: Any) -> Any:
        """CPUé‡å­æ€è®¡ç®—"""
        if isinstance(data, np.ndarray):
            tensor = data
        else:
            tensor = np.array(data)
        
        # CPUé‡å­æ€è®¡ç®—
        result = np.matmul(tensor, tensor.T)
        return result
    
    async def _gpu_matrix_operations(self, data: Any, gpu_id: int) -> Any:
        """GPUçŸ©é˜µè¿ç®—"""
        import torch
        
        if isinstance(data, (list, tuple)):
            matrices = [torch.tensor(mat).cuda(gpu_id) for mat in data]
        else:
            matrices = [torch.tensor(data).cuda(gpu_id)]
        
        # æ‰¹é‡çŸ©é˜µè¿ç®—
        results = []
        for matrix in matrices:
            result = torch.inverse(matrix)  # ç¤ºä¾‹ï¼šçŸ©é˜µæ±‚é€†
            results.append(result.cpu().numpy())
        
        return results
    
    async def _cpu_matrix_operations(self, data: Any) -> Any:
        """CPUçŸ©é˜µè¿ç®—"""
        if isinstance(data, (list, tuple)):
            matrices = [np.array(mat) for mat in data]
        else:
            matrices = [np.array(data)]
        
        results = []
        for matrix in matrices:
            result = np.linalg.inv(matrix)
            results.append(result)
        
        return results
    
    async def _gpu_vector_computations(self, data: Any, gpu_id: int) -> Any:
        """GPUå‘é‡è®¡ç®—"""
        import torch
        
        if isinstance(data, np.ndarray):
            vectors = torch.from_numpy(data).cuda(gpu_id)
        else:
            vectors = torch.tensor(data).cuda(gpu_id)
        
        # å‘é‡è¿ç®—ï¼ˆç¤ºä¾‹ï¼šç‚¹ç§¯ã€èŒƒæ•°ç­‰ï¼‰
        dot_products = torch.mm(vectors, vectors.T)
        norms = torch.norm(vectors, dim=1)
        
        return {
            "dot_products": dot_products.cpu().numpy(),
            "norms": norms.cpu().numpy()
        }
    
    async def _cpu_vector_computations(self, data: Any) -> Any:
        """CPUå‘é‡è®¡ç®—"""
        if isinstance(data, np.ndarray):
            vectors = data
        else:
            vectors = np.array(data)
        
        dot_products = np.dot(vectors, vectors.T)
        norms = np.linalg.norm(vectors, axis=1)
        
        return {
            "dot_products": dot_products,
            "norms": norms
        }
    
    async def _gpu_default_compute(self, data: Any, gpu_id: int) -> Any:
        """GPUé»˜è®¤è®¡ç®—"""
        import torch
        
        # é€šç”¨GPUè®¡ç®—
        if isinstance(data, (int, float)):
            return data * 2  # ç¤ºä¾‹æ“ä½œ
        elif isinstance(data, (list, tuple, np.ndarray)):
            tensor = torch.tensor(data).cuda(gpu_id)
            result = tensor * 2
            return result.cpu().numpy()
        else:
            return data
    
    async def _cpu_default_compute(self, data: Any) -> Any:
        """CPUé»˜è®¤è®¡ç®—"""
        if isinstance(data, (int, float)):
            return data * 2
        elif isinstance(data, (list, tuple)):
            return [x * 2 for x in data]
        elif isinstance(data, np.ndarray):
            return data * 2
        else:
            return data
    
    def _estimate_original_time(self, data: Any, computation_type: str) -> float:
        """ä¼°ç®—åŸå§‹è®¡ç®—æ—¶é—´"""
        # åŸºäºæ•°æ®å¤§å°å’Œè®¡ç®—ç±»å‹ä¼°ç®—æ—¶é—´
        if isinstance(data, (list, tuple, np.ndarray)):
            size = len(data) if hasattr(data, '__len__') else 1
        else:
            size = 1
        
        # åŸºç¡€æ—¶é—´ä¼°ç®—ï¼ˆç§’ï¼‰
        base_times = {
            "quantum_state": 0.1,
            "matrix_operations": 0.05,
            "vector_computations": 0.02,
            "default": 0.01
        }
        
        base_time = base_times.get(computation_type, 0.01)
        return base_time * (1 + size * 0.001)
    
    def _get_memory_usage(self) -> int:
        """è·å–å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss // 1024 // 1024
        except:
            return 0
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.performance_history:
            return {}
        
        speedups = [r.åŠ é€Ÿæ¯” for r in self.performance_history if r.æˆåŠŸ]
        gpu_usage = len([r for r in self.performance_history if "GPU" in r.ä½¿ç”¨è®¾å¤‡])
        
        return {
            "total_computations": len(self.performance_history),
            "successful_computations": len(speedups),
            "average_speedup": np.mean(speedups) if speedups else 0,
            "max_speedup": np.max(speedups) if speedups else 0,
            "gpu_usage_rate": gpu_usage / len(self.performance_history) * 100,
            "average_compute_time": np.mean([r.åŠ é€Ÿæ—¶é—´ for r in self.performance_history if r.æˆåŠŸ]),
            "total_time_saved": sum(r.åŸå§‹æ—¶é—´ - r.åŠ é€Ÿæ—¶é—´ for r in self.performance_history if r.æˆåŠŸ)
        }
    
    def set_compute_mode(self, mode: ComputeMode):
        """è®¾ç½®è®¡ç®—æ¨¡å¼"""
        self.compute_mode = mode
        logger.info(f"è®¡ç®—æ¨¡å¼å·²è®¾ç½®ä¸º: {mode.value}")
    
    def enable_fallback(self, enabled: bool):
        """å¯ç”¨/ç¦ç”¨è‡ªåŠ¨å›é€€"""
        self.fallback_enabled = enabled
        logger.info(f"è‡ªåŠ¨å›é€€å·²: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.cuda_available:
            try:
                import torch
                torch.cuda.empty_cache()
                logger.info("GPUç¼“å­˜å·²æ¸…ç†")
            except:
                pass

# å…¨å±€å®ä¾‹
_gpu_accelerator = None

def get_gpu_accelerator() -> QuantumGPUAccelerator:
    """è·å–GPUåŠ é€Ÿå™¨å®ä¾‹"""
    global _gpu_accelerator
    if _gpu_accelerator is None:
        _gpu_accelerator = QuantumGPUAccelerator()
    return _gpu_accelerator

async def accelerate_computation(data: Any, computation_type: str = "default") -> AccelerationResult:
    """ä¾¿æ·å‡½æ•°ï¼šåŠ é€Ÿè®¡ç®—"""
    accelerator = get_gpu_accelerator()
    return await accelerator.accelerate_quantum_computation(data, computation_type)

# æµ‹è¯•å‡½æ•°
async def test_gpu_accelerator():
    """æµ‹è¯•GPUåŠ é€Ÿå™¨"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•GPUåŠ é€Ÿå™¨...")
    
    accelerator = get_gpu_accelerator()
    
    # æµ‹è¯•æ•°æ®
    test_data = np.random.rand(100, 100)
    
    # æµ‹è¯•é‡å­æ€è®¡ç®—
    print("æµ‹è¯•é‡å­æ€è®¡ç®—...")
    result1 = await accelerator.accelerate_quantum_computation(test_data, "quantum_state")
    print(f"ç»“æœ: åŠ é€Ÿæ¯” {result1.åŠ é€Ÿæ¯”:.2f}x, è®¾å¤‡: {result1.ä½¿ç”¨è®¾å¤‡}")
    
    # æµ‹è¯•çŸ©é˜µè¿ç®—
    print("æµ‹è¯•çŸ©é˜µè¿ç®—...")
    result2 = await accelerator.accelerate_quantum_computation([test_data, test_data], "matrix_operations")
    print(f"ç»“æœ: åŠ é€Ÿæ¯” {result2.åŠ é€Ÿæ¯”:.2f}x, è®¾å¤‡: {result2.ä½¿ç”¨è®¾å¤‡}")
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
    stats = accelerator.get_performance_stats()
    print(f"æ€§èƒ½ç»Ÿè®¡: {stats}")
    
    print("âœ… GPUåŠ é€Ÿå™¨æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(test_gpu_accelerator())