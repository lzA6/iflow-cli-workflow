#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŸ ARQæ•°æ®ç®¡ç†å™¨ V17 Hyperdimensional Singularity
=================================================

è¿™æ˜¯ARQç³»ç»Ÿçš„ç»¼åˆæ•°æ®ç®¡ç†å™¨ï¼Œå®ç°è‡ªåŠ¨åŒ–çš„æ•°æ®è¯»å–ã€è°ƒç”¨ã€è®°å½•ã€æ€»ç»“å’ŒæŸ¥çœ‹åŠŸèƒ½ï¼š
- ğŸ”„ è‡ªåŠ¨æ•°æ®è¯»å–å’Œè°ƒç”¨æœºåˆ¶
- ğŸ“Š æ™ºèƒ½æ•°æ®è®°å½•å’Œåˆ†æ
- ğŸ§  ä¼šè¯å†å²ç®¡ç†
- ğŸ’¾ çŸ¥è¯†åº“è‡ªåŠ¨åŒæ­¥
- ğŸ¯ åå¥½æ•°æ®å­¦ä¹ 
- ğŸ“ˆ æ•°æ®è¶‹åŠ¿åˆ†æ
- ğŸ” æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ
- ğŸ›¡ï¸ æ•°æ®å®‰å…¨ä¿éšœ

æ ¸å¿ƒç‰¹æ€§ï¼š
- å…¨è‡ªåŠ¨æ•°æ®å¤„ç†æµç¨‹
- æ™ºèƒ½ç¼“å­˜æœºåˆ¶
- å®æ—¶æ•°æ®åŒæ­¥
- æ·±åº¦å­¦ä¹ ç”¨æˆ·åå¥½
- é«˜æ•ˆæ£€ç´¢ç®—æ³•
- æ•°æ®æŒä¹…åŒ–
- è·¨ä¼šè¯æ•°æ®è¿ç»­æ€§

ä½œè€…: AIæ¶æ„å¸ˆå›¢é˜Ÿ
ç‰ˆæœ¬: 17.0.0 Hyperdimensional Singularity
æ—¥æœŸ: 2025-11-17
"""

import os
import sys
import json
import sqlite3
import asyncio
import logging
import time
import uuid
import hashlib
import threading
import pickle
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set, Union, Callable
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor
import warnings

# é¡¹ç›®æ ¹è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# å¯¼å…¥ç°æœ‰ç»„ä»¶
try:
    from session_cache_manager import get_ç¼“å­˜ç®¡ç†å™¨
    from memory_optimizer import get_memory_optimizer
    from arq_reasoning_engine_v17_hyperdimensional_singularity import ARQReasoningEngineV17HyperdimensionalSingularity
    LEGACY_COMPONENTS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ éƒ¨åˆ†ä¼ ç»Ÿç»„ä»¶ä¸å¯ç”¨: {e}")
    LEGACY_COMPONENTS_AVAILABLE = False

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ•°æ®ç±»å‹æšä¸¾
class DataType(Enum):
    """æ•°æ®ç±»å‹æšä¸¾"""
    SESSION_DATA = "session_data"
    KNOWLEDGE_BASE = "knowledge_base"
    USER_PREFERENCES = "user_preferences"
    QUERY_HISTORY = "query_history"
    ARQ_HISTORY = "arq_history"
    SYSTEM_METRICS = "system_metrics"
    CACHE_DATA = "cache_data"
    MEMORY_SNAPSHOT = "memory_snapshot"

# æ•°æ®ä¼˜å…ˆçº§
class DataPriority(Enum):
    """æ•°æ®ä¼˜å…ˆçº§"""
    CRITICAL = 1    # å…³é”®æ•°æ®ï¼Œæ°¸ä¸åˆ é™¤
    HIGH = 2        # é«˜ä¼˜å…ˆçº§ï¼Œä¼˜å…ˆä¿ç•™
    NORMAL = 3      # æ™®é€šä¼˜å…ˆçº§
    LOW = 4         # ä½ä¼˜å…ˆçº§ï¼Œä¼˜å…ˆæ¸…ç†

# æ•°æ®é¡¹ç»“æ„
@dataclass
class DataItem:
    """æ•°æ®é¡¹ç»“æ„"""
    id: str
    data_type: DataType
    content: Any
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    last_accessed: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    priority: DataPriority = DataPriority.NORMAL
    tags: Set[str] = field(default_factory=set)
    size_bytes: int = 0
    checksum: str = ""

# ä¼šè¯ä¸Šä¸‹æ–‡
@dataclass
class SessionContext:
    """ä¼šè¯ä¸Šä¸‹æ–‡"""
    session_id: str
    project_id: str
    user_id: Optional[str] = None
    start_time: datetime = field(default_factory=datetime.now)
    goals: List[str] = field(default_factory=list)
    achievements: List[str] = field(default_factory=list)
    blockers: List[str] = field(default_factory=list)
    preferences: Dict[str, Any] = field(default_factory=dict)
    context_data: Dict[str, Any] = field(default_factory=dict)
    active: bool = True

# ç”¨æˆ·åå¥½
@dataclass
class UserPreferences:
    """ç”¨æˆ·åå¥½è®¾ç½®"""
    user_id: str
    preferred_thinking_mode: str = "hyperdimensional_singularity"
    language_preference: str = "zh-CN"
    response_style: str = "professional"
    auto_save_frequency: int = 300  # ç§’
    cache_retention_days: int = 30
    privacy_level: str = "standard"
    notification_settings: Dict[str, bool] = field(default_factory=dict)
    custom_settings: Dict[str, Any] = field(default_factory=dict)
    last_updated: datetime = field(default_factory=datetime.now)

class ARQDataManagerV17:
    """ARQæ•°æ®ç®¡ç†å™¨V17ä¸»ç±»"""
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨"""
        self.config = config or {}
        
        # æ•°æ®ç›®å½•
        self.data_root = PROJECT_ROOT / "data"
        self.data_root.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®åº“è·¯å¾„
        self.main_db_path = self.data_root / "arq_data_manager.db"
        self.cache_db_path = self.data_root / "cache" / "cache.db"
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_databases()
        
        # å†…å­˜ç¼“å­˜
        self.memory_cache = {}
        self.cache_lock = threading.RLock()
        
        # ä¼šè¯ç®¡ç†
        self.active_sessions = {}
        self.session_lock = threading.RLock()
        
        # ç”¨æˆ·åå¥½
        self.user_preferences = {}
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            "data_reads": 0,
            "data_writes": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "query_count": 0,
            "session_count": 0
        }
        
        # è‡ªåŠ¨åŒæ­¥çº¿ç¨‹
        self.sync_thread = None
        self.sync_interval = 60  # ç§’
        self.running = False
        
        # ä¼ ç»Ÿç»„ä»¶é›†æˆ
        self.legacy_cache_manager = None
        self.memory_optimizer = None
        
        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # åˆå§‹åŒ–ä¼ ç»Ÿç»„ä»¶
        self._init_legacy_components()
        
        logger.info("ğŸŒŸ ARQæ•°æ®ç®¡ç†å™¨V17åˆå§‹åŒ–å®Œæˆ")
    
    def _init_databases(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        # ä¸»æ•°æ®åº“
        with sqlite3.connect(self.main_db_path) as conn:
            # æ•°æ®é¡¹è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS data_items (
                    id TEXT PRIMARY KEY,
                    data_type TEXT NOT NULL,
                    content TEXT NOT NULL,
                    metadata TEXT,
                    created_at TEXT NOT NULL,
                    last_accessed TEXT NOT NULL,
                    access_count INTEGER DEFAULT 0,
                    priority INTEGER DEFAULT 3,
                    tags TEXT,
                    size_bytes INTEGER DEFAULT 0,
                    checksum TEXT
                )
            """)
            
            # ä¼šè¯è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS sessions (
                    session_id TEXT PRIMARY KEY,
                    project_id TEXT,
                    user_id TEXT,
                    start_time TEXT NOT NULL,
                    goals TEXT,
                    achievements TEXT,
                    blockers TEXT,
                    preferences TEXT,
                    context_data TEXT,
                    active INTEGER DEFAULT 1
                )
            """)
            
            # ç”¨æˆ·åå¥½è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS user_preferences (
                    user_id TEXT PRIMARY KEY,
                    preferred_thinking_mode TEXT,
                    language_preference TEXT,
                    response_style TEXT,
                    auto_save_frequency INTEGER,
                    cache_retention_days INTEGER,
                    privacy_level TEXT,
                    notification_settings TEXT,
                    custom_settings TEXT,
                    last_updated TEXT
                )
            """)
            
            # æŸ¥è¯¢å†å²è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS query_history (
                    id TEXT PRIMARY KEY,
                    session_id TEXT,
                    query TEXT NOT NULL,
                    context TEXT,
                    response TEXT,
                    timestamp TEXT NOT NULL,
                    response_time REAL,
                    confidence REAL,
                    metadata TEXT
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            conn.execute("CREATE INDEX IF NOT EXISTS idx_data_type ON data_items(data_type)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON data_items(created_at)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_last_accessed ON data_items(last_accessed)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_session_id ON sessions(session_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_query_timestamp ON query_history(timestamp)")
            
            conn.commit()
        
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def _init_legacy_components(self):
        """åˆå§‹åŒ–ä¼ ç»Ÿç»„ä»¶"""
        try:
            if LEGACY_COMPONENTS_AVAILABLE:
                self.legacy_cache_manager = get_ç¼“å­˜ç®¡ç†å™¨()
                self.memory_optimizer = get_memory_optimizer()
                logger.info("âœ… ä¼ ç»Ÿç»„ä»¶é›†æˆæˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ ä¼ ç»Ÿç»„ä»¶é›†æˆå¤±è´¥: {e}")
    
    async def start_auto_sync(self):
        """å¯åŠ¨è‡ªåŠ¨åŒæ­¥"""
        if self.running:
            return
        
        self.running = True
        self.sync_thread = threading.Thread(target=self._auto_sync_worker, daemon=True)
        self.sync_thread.start()
        logger.info("ğŸ”„ è‡ªåŠ¨åŒæ­¥å·²å¯åŠ¨")
    
    def stop_auto_sync(self):
        """åœæ­¢è‡ªåŠ¨åŒæ­¥"""
        self.running = False
        if self.sync_thread:
            self.sync_thread.join(timeout=5)
        logger.info("â¹ï¸ è‡ªåŠ¨åŒæ­¥å·²åœæ­¢")
    
    def _auto_sync_worker(self):
        """è‡ªåŠ¨åŒæ­¥å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                asyncio.run(self._perform_sync())
                time.sleep(self.sync_interval)
            except Exception as e:
                logger.error(f"âŒ è‡ªåŠ¨åŒæ­¥å¤±è´¥: {e}")
                time.sleep(10)
    
    async def _perform_sync(self):
        """æ‰§è¡ŒåŒæ­¥æ“ä½œ"""
        # åŒæ­¥å†…å­˜ç¼“å­˜åˆ°æ•°æ®åº“
        await self._sync_memory_to_db()
        
        # æ¸…ç†è¿‡æœŸæ•°æ®
        await self._cleanup_expired_data()
        
        # ä¼˜åŒ–æ•°æ®åº“
        await self._optimize_database()
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        await self._update_performance_metrics()
    
    async def store_data(self, data: Any, data_type: DataType, 
                        metadata: Optional[Dict] = None,
                        priority: DataPriority = DataPriority.NORMAL,
                        tags: Optional[Set[str]] = None) -> str:
        """
        å­˜å‚¨æ•°æ®
        
        Args:
            data: è¦å­˜å‚¨çš„æ•°æ®
            data_type: æ•°æ®ç±»å‹
            metadata: å…ƒæ•°æ®
            priority: ä¼˜å…ˆçº§
            tags: æ ‡ç­¾é›†åˆ
            
        Returns:
            æ•°æ®ID
        """
        try:
            # ç”Ÿæˆæ•°æ®ID
            data_id = str(uuid.uuid4())
            
            # åºåˆ—åŒ–æ•°æ®
            content = json.dumps(data, ensure_ascii=False, default=str)
            content_bytes = content.encode('utf-8')
            
            # è®¡ç®—æ ¡éªŒå’Œ
            checksum = hashlib.md5(content_bytes).hexdigest()
            
            # åˆ›å»ºæ•°æ®é¡¹
            data_item = DataItem(
                id=data_id,
                data_type=data_type,
                content=data,
                metadata=metadata or {},
                priority=priority,
                tags=tags or set(),
                size_bytes=len(content_bytes),
                checksum=checksum
            )
            
            # å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜
            with self.cache_lock:
                self.memory_cache[data_id] = data_item
            
            # å­˜å‚¨åˆ°æ•°æ®åº“
            await self._store_to_db(data_item)
            
            # å­˜å‚¨åˆ°ä¼ ç»Ÿç¼“å­˜
            if self.legacy_cache_manager:
                await self.legacy_cache_manager.è®¾ç½®ç¼“å­˜(
                    f"arq_data_{data_id}", 
                    data_item,
                    timedelta(days=30)
                )
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self.performance_metrics["data_writes"] += 1
            
            logger.debug(f"ğŸ’¾ æ•°æ®å·²å­˜å‚¨: {data_id} ({data_type.value})")
            return data_id
            
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨æ•°æ®å¤±è´¥: {e}")
            raise
    
    async def retrieve_data(self, data_id: str) -> Optional[DataItem]:
        """
        æ£€ç´¢æ•°æ®
        
        Args:
            data_id: æ•°æ®ID
            
        Returns:
            æ•°æ®é¡¹æˆ–None
        """
        try:
            # é¦–å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
            with self.cache_lock:
                if data_id in self.memory_cache:
                    data_item = self.memory_cache[data_id]
                    data_item.last_accessed = datetime.now()
                    data_item.access_count += 1
                    self.performance_metrics["cache_hits"] += 1
                    return data_item
            
            # æ£€æŸ¥ä¼ ç»Ÿç¼“å­˜
            if self.legacy_cache_manager:
                cached_data = await self.legacy_cache_manager.è·å–ç¼“å­˜(f"arq_data_{data_id}")
                if cached_data:
                    # è½¬æ¢ä¸ºDataItem
                    data_item = DataItem(**cached_data)
                    data_item.last_accessed = datetime.now()
                    data_item.access_count += 1
                    
                    # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                    with self.cache_lock:
                        self.memory_cache[data_id] = data_item
                    
                    self.performance_metrics["cache_hits"] += 1
                    return data_item
            
            # ä»æ•°æ®åº“åŠ è½½
            data_item = await self._load_from_db(data_id)
            if data_item:
                data_item.last_accessed = datetime.now()
                data_item.access_count += 1
                
                # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                with self.cache_lock:
                    self.memory_cache[data_id] = data_item
                
                self.performance_metrics["cache_misses"] += 1
                return data_item
            
            self.performance_metrics["cache_misses"] += 1
            return None
            
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢æ•°æ®å¤±è´¥ {data_id}: {e}")
            return None
    
    async def create_session(self, project_id: str, user_id: Optional[str] = None,
                           goals: Optional[List[str]] = None) -> str:
        """
        åˆ›å»ºæ–°ä¼šè¯
        
        Args:
            project_id: é¡¹ç›®ID
            user_id: ç”¨æˆ·ID
            goals: ä¼šè¯ç›®æ ‡
            
        Returns:
            ä¼šè¯ID
        """
        try:
            session_id = str(uuid.uuid4())
            
            session = SessionContext(
                session_id=session_id,
                project_id=project_id,
                user_id=user_id,
                goals=goals or []
            )
            
            # å­˜å‚¨ä¼šè¯
            with self.session_lock:
                self.active_sessions[session_id] = session
            
            # å­˜å‚¨åˆ°æ•°æ®åº“
            await self._store_session_to_db(session)
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self.performance_metrics["session_count"] += 1
            
            logger.info(f"ğŸ†• ä¼šè¯å·²åˆ›å»º: {session_id}")
            return session_id
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºä¼šè¯å¤±è´¥: {e}")
            raise
    
    async def get_session(self, session_id: str) -> Optional[SessionContext]:
        """è·å–ä¼šè¯ä¸Šä¸‹æ–‡"""
        try:
            # æ£€æŸ¥æ´»è·ƒä¼šè¯
            with self.session_lock:
                if session_id in self.active_sessions:
                    return self.active_sessions[session_id]
            
            # ä»æ•°æ®åº“åŠ è½½
            session = await self._load_session_from_db(session_id)
            if session:
                with self.session_lock:
                    self.active_sessions[session_id] = session
            
            return session
            
        except Exception as e:
            logger.error(f"âŒ è·å–ä¼šè¯å¤±è´¥ {session_id}: {e}")
            return None
    
    async def update_session(self, session_id: str, **kwargs) -> bool:
        """æ›´æ–°ä¼šè¯ä¸Šä¸‹æ–‡"""
        try:
            session = await self.get_session(session_id)
            if not session:
                return False
            
            # æ›´æ–°å±æ€§
            for key, value in kwargs.items():
                if hasattr(session, key):
                    setattr(session, key, value)
            
            # å­˜å‚¨åˆ°æ•°æ®åº“
            await self._store_session_to_db(session)
            
            logger.debug(f"ğŸ“ ä¼šè¯å·²æ›´æ–°: {session_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä¼šè¯å¤±è´¥ {session_id}: {e}")
            return False
    
    async def record_query(self, session_id: str, query: str, 
                          context: Optional[str] = None,
                          response: Optional[str] = None,
                          response_time: Optional[float] = None,
                          confidence: Optional[float] = None,
                          metadata: Optional[Dict] = None) -> str:
        """
        è®°å½•æŸ¥è¯¢
        
        Args:
            session_id: ä¼šè¯ID
            query: æŸ¥è¯¢å†…å®¹
            context: ä¸Šä¸‹æ–‡
            response: å“åº”å†…å®¹
            response_time: å“åº”æ—¶é—´
            confidence: ç½®ä¿¡åº¦
            metadata: å…ƒæ•°æ®
            
        Returns:
            æŸ¥è¯¢ID
        """
        try:
            query_id = str(uuid.uuid4())
            
            # å­˜å‚¨åˆ°æ•°æ®åº“
            with sqlite3.connect(self.main_db_path) as conn:
                conn.execute("""
                    INSERT INTO query_history 
                    (id, session_id, query, context, response, timestamp, response_time, confidence, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    query_id,
                    session_id,
                    query,
                    context,
                    response,
                    datetime.now().isoformat(),
                    response_time,
                    confidence,
                    json.dumps(metadata or {}, ensure_ascii=False)
                ))
                conn.commit()
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self.performance_metrics["query_count"] += 1
            
            logger.debug(f"ğŸ“ æŸ¥è¯¢å·²è®°å½•: {query_id}")
            return query_id
            
        except Exception as e:
            logger.error(f"âŒ è®°å½•æŸ¥è¯¢å¤±è´¥: {e}")
            raise
    
    async def get_user_preferences(self, user_id: str) -> Optional[UserPreferences]:
        """è·å–ç”¨æˆ·åå¥½"""
        try:
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            if user_id in self.user_preferences:
                return self.user_preferences[user_id]
            
            # ä»æ•°æ®åº“åŠ è½½
            with sqlite3.connect(self.main_db_path) as conn:
                cursor = conn.execute("""
                    SELECT * FROM user_preferences WHERE user_id = ?
                """, (user_id,))
                
                row = cursor.fetchone()
                if row:
                    preferences = UserPreferences(
                        user_id=row[0],
                        preferred_thinking_mode=row[1] or "hyperdimensional_singularity",
                        language_preference=row[2] or "zh-CN",
                        response_style=row[3] or "professional",
                        auto_save_frequency=row[4] or 300,
                        cache_retention_days=row[5] or 30,
                        privacy_level=row[6] or "standard",
                        notification_settings=json.loads(row[7] or "{}"),
                        custom_settings=json.loads(row[8] or "{}"),
                        last_updated=datetime.fromisoformat(row[9]) if row[9] else datetime.now()
                    )
                    
                    self.user_preferences[user_id] = preferences
                    return preferences
            
            # åˆ›å»ºé»˜è®¤åå¥½
            preferences = UserPreferences(user_id=user_id)
            await self.save_user_preferences(preferences)
            
            return preferences
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç”¨æˆ·åå¥½å¤±è´¥ {user_id}: {e}")
            return None
    
    async def save_user_preferences(self, preferences: UserPreferences) -> bool:
        """ä¿å­˜ç”¨æˆ·åå¥½"""
        try:
            preferences.last_updated = datetime.now()
            
            with sqlite3.connect(self.main_db_path) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO user_preferences 
                    (user_id, preferred_thinking_mode, language_preference, response_style,
                     auto_save_frequency, cache_retention_days, privacy_level,
                     notification_settings, custom_settings, last_updated)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    preferences.user_id,
                    preferences.preferred_thinking_mode,
                    preferences.language_preference,
                    preferences.response_style,
                    preferences.auto_save_frequency,
                    preferences.cache_retention_days,
                    preferences.privacy_level,
                    json.dumps(preferences.notification_settings, ensure_ascii=False),
                    json.dumps(preferences.custom_settings, ensure_ascii=False),
                    preferences.last_updated.isoformat()
                ))
                conn.commit()
            
            # æ›´æ–°å†…å­˜ç¼“å­˜
            self.user_preferences[preferences.user_id] = preferences
            
            logger.debug(f"ğŸ’¾ ç”¨æˆ·åå¥½å·²ä¿å­˜: {preferences.user_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç”¨æˆ·åå¥½å¤±è´¥: {e}")
            return False
    
    async def search_data(self, query: str, data_types: Optional[List[DataType]] = None,
                         tags: Optional[Set[str]] = None,
                         limit: int = 100) -> List[DataItem]:
        """
        æœç´¢æ•°æ®
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            data_types: æ•°æ®ç±»å‹è¿‡æ»¤
            tags: æ ‡ç­¾è¿‡æ»¤
            limit: ç»“æœé™åˆ¶
            
        Returns:
            åŒ¹é…çš„æ•°æ®é¡¹åˆ—è¡¨
        """
        try:
            results = []
            
            # æ„å»ºSQLæŸ¥è¯¢
            sql_conditions = []
            sql_params = []
            
            if data_types:
                type_placeholders = ",".join(["?" for _ in data_types])
                sql_conditions.append(f"data_type IN ({type_placeholders})")
                sql_params.extend([dt.value for dt in data_types])
            
            if tags:
                for tag in tags:
                    sql_conditions.append("tags LIKE ?")
                    sql_params.append(f"%{tag}%")
            
            where_clause = " AND ".join(sql_conditions) if sql_conditions else "1=1"
            
            with sqlite3.connect(self.main_db_path) as conn:
                cursor = conn.execute(f"""
                    SELECT * FROM data_items 
                    WHERE {where_clause}
                    ORDER BY last_accessed DESC
                    LIMIT ?
                """, sql_params + [limit])
                
                rows = cursor.fetchall()
                
                for row in rows:
                    try:
                        data_item = DataItem(
                            id=row[0],
                            data_type=DataType(row[1]),
                            content=json.loads(row[2]),
                            metadata=json.loads(row[3] or "{}"),
                            created_at=datetime.fromisoformat(row[4]),
                            last_accessed=datetime.fromisoformat(row[5]),
                            access_count=row[6],
                            priority=DataPriority(row[7]),
                            tags=set(json.loads(row[8] or "[]")),
                            size_bytes=row[9],
                            checksum=row[10]
                        )
                        
                        # ç®€å•çš„å†…å®¹åŒ¹é…
                        if query.lower() in json.dumps(data_item.content, ensure_ascii=False).lower():
                            results.append(data_item)
                            
                    except Exception as e:
                        logger.warning(f"âš ï¸ è§£ææ•°æ®é¡¹å¤±è´¥: {e}")
                        continue
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def get_session_history(self, session_id: str, limit: int = 100) -> List[Dict]:
        """è·å–ä¼šè¯å†å²"""
        try:
            with sqlite3.connect(self.main_db_path) as conn:
                cursor = conn.execute("""
                    SELECT * FROM query_history 
                    WHERE session_id = ?
                    ORDER BY timestamp DESC
                    LIMIT ?
                """, (session_id, limit))
                
                rows = cursor.fetchall()
                
                history = []
                for row in rows:
                    history.append({
                        "id": row[0],
                        "session_id": row[1],
                        "query": row[2],
                        "context": row[3],
                        "response": row[4],
                        "timestamp": row[5],
                        "response_time": row[6],
                        "confidence": row[7],
                        "metadata": json.loads(row[8] or "{}")
                    })
                
                return history
                
        except Exception as e:
            logger.error(f"âŒ è·å–ä¼šè¯å†å²å¤±è´¥ {session_id}: {e}")
            return []
    
    async def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        try:
            summary = {
                "performance_metrics": self.performance_metrics.copy(),
                "cache_hit_rate": 0.0,
                "active_sessions": len(self.active_sessions),
                "memory_usage": 0,
                "database_size": 0
            }
            
            # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
            total_requests = summary["performance_metrics"]["cache_hits"] + summary["performance_metrics"]["cache_misses"]
            if total_requests > 0:
                summary["cache_hit_rate"] = summary["performance_metrics"]["cache_hits"] / total_requests
            
            # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
            if self.memory_optimizer:
                stats = self.memory_optimizer.get_memory_stats()
                summary["memory_usage"] = stats.process_mb
            
            # è·å–æ•°æ®åº“å¤§å°
            try:
                summary["database_size"] = os.path.getsize(self.main_db_path) / (1024 * 1024)  # MB
            except:
                pass
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ€§èƒ½æ‘˜è¦å¤±è´¥: {e}")
            return {}
    
    async def _store_to_db(self, data_item: DataItem):
        """å­˜å‚¨æ•°æ®é¡¹åˆ°æ•°æ®åº“"""
        with sqlite3.connect(self.main_db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO data_items 
                (id, data_type, content, metadata, created_at, last_accessed, 
                 access_count, priority, tags, size_bytes, checksum)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                data_item.id,
                data_item.data_type.value,
                json.dumps(data_item.content, ensure_ascii=False, default=str),
                json.dumps(data_item.metadata, ensure_ascii=False),
                data_item.created_at.isoformat(),
                data_item.last_accessed.isoformat(),
                data_item.access_count,
                data_item.priority.value,
                json.dumps(list(data_item.tags), ensure_ascii=False),
                data_item.size_bytes,
                data_item.checksum
            ))
            conn.commit()
    
    async def _load_from_db(self, data_id: str) -> Optional[DataItem]:
        """ä»æ•°æ®åº“åŠ è½½æ•°æ®é¡¹"""
        with sqlite3.connect(self.main_db_path) as conn:
            cursor = conn.execute("""
                SELECT * FROM data_items WHERE id = ?
            """, (data_id,))
            
            row = cursor.fetchone()
            if row:
                return DataItem(
                    id=row[0],
                    data_type=DataType(row[1]),
                    content=json.loads(row[2]),
                    metadata=json.loads(row[3] or "{}"),
                    created_at=datetime.fromisoformat(row[4]),
                    last_accessed=datetime.fromisoformat(row[5]),
                    access_count=row[6],
                    priority=DataPriority(row[7]),
                    tags=set(json.loads(row[8] or "[]")),
                    size_bytes=row[9],
                    checksum=row[10]
                )
        
        return None
    
    async def _store_session_to_db(self, session: SessionContext):
        """å­˜å‚¨ä¼šè¯åˆ°æ•°æ®åº“"""
        with sqlite3.connect(self.main_db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO sessions 
                (session_id, project_id, user_id, start_time, goals, achievements,
                 blockers, preferences, context_data, active)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                session.session_id,
                session.project_id,
                session.user_id,
                session.start_time.isoformat(),
                json.dumps(session.goals, ensure_ascii=False),
                json.dumps(session.achievements, ensure_ascii=False),
                json.dumps(session.blockers, ensure_ascii=False),
                json.dumps(session.preferences, ensure_ascii=False),
                json.dumps(session.context_data, ensure_ascii=False),
                int(session.active)
            ))
            conn.commit()
    
    async def _load_session_from_db(self, session_id: str) -> Optional[SessionContext]:
        """ä»æ•°æ®åº“åŠ è½½ä¼šè¯"""
        with sqlite3.connect(self.main_db_path) as conn:
            cursor = conn.execute("""
                SELECT * FROM sessions WHERE session_id = ?
            """, (session_id,))
            
            row = cursor.fetchone()
            if row:
                return SessionContext(
                    session_id=row[0],
                    project_id=row[1],
                    user_id=row[2],
                    start_time=datetime.fromisoformat(row[3]),
                    goals=json.loads(row[4] or "[]"),
                    achievements=json.loads(row[5] or "[]"),
                    blockers=json.loads(row[6] or "[]"),
                    preferences=json.loads(row[7] or "{}"),
                    context_data=json.loads(row[8] or "{}"),
                    active=bool(row[9])
                )
        
        return None
    
    async def _sync_memory_to_db(self):
        """åŒæ­¥å†…å­˜ç¼“å­˜åˆ°æ•°æ®åº“"""
        try:
            with self.cache_lock:
                items_to_sync = list(self.memory_cache.values())
            
            for item in items_to_sync:
                await self._store_to_db(item)
                
            logger.debug("ğŸ’¾ å†…å­˜ç¼“å­˜å·²åŒæ­¥åˆ°æ•°æ®åº“")
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥å†…å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
    
    async def _cleanup_expired_data(self):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        try:
            cutoff_date = datetime.now() - timedelta(days=30)
            
            with sqlite3.connect(self.main_db_path) as conn:
                # æ¸…ç†è¿‡æœŸçš„ä½ä¼˜å…ˆçº§æ•°æ®
                conn.execute("""
                    DELETE FROM data_items 
                    WHERE created_at < ? AND priority >= 3
                """, (cutoff_date.isoformat(),))
                
                conn.commit()
                
            logger.debug("ğŸ§¹ è¿‡æœŸæ•°æ®æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†è¿‡æœŸæ•°æ®å¤±è´¥: {e}")
    
    async def _optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“"""
        try:
            with sqlite3.connect(self.main_db_path) as conn:
                conn.execute("VACUUM")
                conn.execute("ANALYZE")
                conn.commit()
                
            logger.debug("âš¡ æ•°æ®åº“ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}")
    
    async def _update_performance_metrics(self):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        try:
            # å®šæœŸé‡ç½®è®¡æ•°å™¨
            if self.performance_metrics["data_reads"] > 10000:
                logger.info("ğŸ“Š æ€§èƒ½æŒ‡æ ‡å·²é‡ç½®")
                self.performance_metrics = {
                    "data_reads": 0,
                    "data_writes": 0,
                    "cache_hits": 0,
                    "cache_misses": 0,
                    "query_count": 0,
                    "session_count": 0
                }
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # åœæ­¢è‡ªåŠ¨åŒæ­¥
            self.stop_auto_sync()
            
            # åŒæ­¥å‰©ä½™æ•°æ®
            await self._sync_memory_to_db()
            
            # å…³é—­çº¿ç¨‹æ± 
            if self.executor:
                self.executor.shutdown(wait=True)
            
            logger.info("ğŸ§¹ ARQæ•°æ®ç®¡ç†å™¨V17èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†èµ„æºå¤±è´¥: {e}")

# å…¨å±€å®ä¾‹
_global_data_manager: Optional[ARQDataManagerV17] = None

def get_arq_data_manager() -> ARQDataManagerV17:
    """è·å–å…¨å±€æ•°æ®ç®¡ç†å™¨å®ä¾‹"""
    global _global_data_manager
    if _global_data_manager is None:
        _global_data_manager = ARQDataManagerV17()
        # å¯åŠ¨è‡ªåŠ¨åŒæ­¥
        asyncio.create_task(_global_data_manager.start_auto_sync())
    return _global_data_manager

# ä¾¿æ·å‡½æ•°
async def store_arq_data(data: Any, data_type: DataType, **kwargs) -> str:
    """ä¾¿æ·çš„æ•°æ®å­˜å‚¨å‡½æ•°"""
    manager = get_arq_data_manager()
    return await manager.store_data(data, data_type, **kwargs)

async def retrieve_arq_data(data_id: str) -> Optional[DataItem]:
    """ä¾¿æ·çš„æ•°æ®æ£€ç´¢å‡½æ•°"""
    manager = get_arq_data_manager()
    return await manager.retrieve_data(data_id)

async def create_arq_session(project_id: str, **kwargs) -> str:
    """ä¾¿æ·çš„ä¼šè¯åˆ›å»ºå‡½æ•°"""
    manager = get_arq_data_manager()
    return await manager.create_session(project_id, **kwargs)

async def record_arq_query(session_id: str, query: str, **kwargs) -> str:
    """ä¾¿æ·çš„æŸ¥è¯¢è®°å½•å‡½æ•°"""
    manager = get_arq_data_manager()
    return await manager.record_query(session_id, query, **kwargs)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_data_manager():
        print("ğŸŒŸ æµ‹è¯•ARQæ•°æ®ç®¡ç†å™¨V17")
        
        # è·å–æ•°æ®ç®¡ç†å™¨
        manager = get_arq_data_manager()
        
        # æµ‹è¯•æ•°æ®å­˜å‚¨
        test_data = {
            "message": "Hello ARQ V17",
            "timestamp": datetime.now().isoformat(),
            "metadata": {"version": "17.0", "type": "test"}
        }
        
        data_id = await manager.store_data(
            test_data, 
            DataType.SESSION_DATA,
            tags={"test", "v17"},
            priority=DataPriority.HIGH
        )
        print(f"âœ… æ•°æ®å·²å­˜å‚¨: {data_id}")
        
        # æµ‹è¯•æ•°æ®æ£€ç´¢
        retrieved_data = await manager.retrieve_data(data_id)
        if retrieved_data:
            print(f"âœ… æ•°æ®å·²æ£€ç´¢: {retrieved_data.content}")
        
        # æµ‹è¯•ä¼šè¯åˆ›å»º
        session_id = await manager.create_session(
            project_id="test_project",
            user_id="test_user",
            goals=["æµ‹è¯•æ•°æ®ç®¡ç†å™¨åŠŸèƒ½"]
        )
        print(f"âœ… ä¼šè¯å·²åˆ›å»º: {session_id}")
        
        # æµ‹è¯•æŸ¥è¯¢è®°å½•
        query_id = await manager.record_query(
            session_id=session_id,
            query="æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½",
            response="æµ‹è¯•å“åº”",
            response_time=0.1,
            confidence=0.95
        )
        print(f"âœ… æŸ¥è¯¢å·²è®°å½•: {query_id}")
        
        # æµ‹è¯•ç”¨æˆ·åå¥½
        preferences = await manager.get_user_preferences("test_user")
        print(f"âœ… ç”¨æˆ·åå¥½: {preferences.preferred_thinking_mode}")
        
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        search_results = await manager.search_data("Hello", limit=10)
        print(f"âœ… æœç´¢ç»“æœ: {len(search_results)} é¡¹")
        
        # è·å–æ€§èƒ½æ‘˜è¦
        summary = await manager.get_performance_summary()
        print(f"âœ… æ€§èƒ½æ‘˜è¦: {summary}")
        
        # æ¸…ç†
        await manager.cleanup()
        print("âœ… æµ‹è¯•å®Œæˆ")
    
    asyncio.run(test_data_manager())