#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŸ ä¼˜åŒ–çš„æ™ºèƒ½ä½“èåˆç¼“å­˜ç³»ç»Ÿ V2
é«˜æ•ˆç¼“å­˜å’Œé¢„è®¡ç®—æ™ºèƒ½ä½“èåˆç»“æœï¼Œå¤§å¹…æå‡å·¥ä½œæµæ‰§è¡Œæ•ˆç‡ã€‚
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
import hashlib
import pickle
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict, deque
import threading
import numpy as np
from functools import lru_cache

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

logger = logging.getLogger(__name__)

@dataclass
class FusionCacheEntry:
    """èåˆç¼“å­˜æ¡ç›®"""
    task_hash: str
    task_description: str
    selected_experts: List[str]
    fusion_mode: str
    result: Any
    quality_score: float
    execution_time: float
    timestamp: float
    hit_count: int = 0
    last_accessed: float = field(default_factory=time.time)
    context_similarity: float = 0.0  # ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦

@dataclass
class PrecomputedPattern:
    """é¢„è®¡ç®—æ¨¡å¼"""
    pattern_hash: str
    task_keywords: Set[str]
    common_experts: List[str]
    optimal_fusion_mode: str
    success_rate: float
    avg_quality_score: float
    last_updated: float

class OptimizedFusionCache:
    """
    ä¼˜åŒ–çš„æ™ºèƒ½ä½“èåˆç¼“å­˜ç³»ç»Ÿ
    """
    
    def __init__(self, cache_size: int = 1000, ttl_hours: int = 24):
        self.cache_size = cache_size
        self.ttl_hours = ttl_hours
        
        # ä¸»ç¼“å­˜å­—å…¸
        self.cache: Dict[str, FusionCacheEntry] = {}
        
        # é¢„è®¡ç®—æ¨¡å¼åº“
        self.patterns: Dict[str, PrecomputedPattern] = {}
        
        # è®¿é—®é¢‘ç‡ç»Ÿè®¡
        self.access_frequency: Dict[str, int] = defaultdict(int)
        
        # LRUé˜Ÿåˆ—
        self.lru_queue: deque = deque()
        
        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            "hits": 0,
            "misses": 0,
            "precomputed_hits": 0,
            "total_requests": 0,
            "avg_response_time": 0.0,
            "cache_efficiency": 0.0
        }
        
        # é”æœºåˆ¶
        self._lock = threading.RLock()
        
        # åŠ è½½æŒä¹…åŒ–ç¼“å­˜
        self._load_persisted_cache()
        
        logger.info("ä¼˜åŒ–çš„æ™ºèƒ½ä½“èåˆç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _generate_task_hash(self, task: str, context: Optional[Dict[str, Any]] = None) -> str:
        """ç”Ÿæˆä»»åŠ¡å“ˆå¸Œ"""
        context_str = json.dumps(context or {}, sort_keys=True)
        combined = f"{task}:{context_str}"
        return hashlib.md5(combined.encode('utf-8')).hexdigest()
    
    def _calculate_similarity(self, task1: str, task2: str) -> float:
        """è®¡ç®—ä»»åŠ¡ç›¸ä¼¼åº¦"""
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        words1 = set(task1.lower().split())
        words2 = set(task2.lower().split())
        
        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union)
    
    def get_cached_result(self, task: str, context: Optional[Dict[str, Any]] = None) -> Optional[FusionCacheEntry]:
        """è·å–ç¼“å­˜ç»“æœ"""
        with self._lock:
            task_hash = self._generate_task_hash(task, context)
            self.stats["total_requests"] += 1
            
            if task_hash in self.cache:
                entry = self.cache[task_hash]
                
                # æ£€æŸ¥TTL
                if time.time() - entry.timestamp > self.ttl_hours * 3600:
                    self._remove_entry(task_hash)
                    self.stats["misses"] += 1
                    return None
                
                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                entry.hit_count += 1
                entry.last_accessed = time.time()
                self._update_lru(task_hash)
                self.stats["hits"] += 1
                
                logger.info(f"ç¼“å­˜å‘½ä¸­: {task[:50]}...")
                return entry
            
            self.stats["misses"] += 1
            return None
    
    def _update_lru(self, task_hash: str):
        """æ›´æ–°LRUé˜Ÿåˆ—"""
        if task_hash in self.lru_queue:
            self.lru_queue.remove(task_hash)
        self.lru_queue.append(task_hash)
    
    def _remove_entry(self, task_hash: str):
        """ç§»é™¤ç¼“å­˜æ¡ç›®"""
        if task_hash in self.cache:
            del self.cache[task_hash]
        if task_hash in self.access_frequency:
            del self.access_frequency[task_hash]
    
    def _evict_lru_entries(self):
        """LRUæ·˜æ±°æœºåˆ¶"""
        while len(self.cache) >= self.cache_size and self.lru_queue:
            lru_hash = self.lru_queue.popleft()
            if lru_hash in self.cache:
                del self.cache[lru_hash]
    
    def put_cache_result(self, task: str, context: Optional[Dict[str, Any]], 
                        selected_experts: List[str], fusion_mode: str,
                        result: Any, quality_score: float, execution_time: float):
        """å­˜å‚¨ç¼“å­˜ç»“æœ"""
        with self._lock:
            task_hash = self._generate_task_hash(task, context)
            
            # åˆ›å»ºç¼“å­˜æ¡ç›®
            entry = FusionCacheEntry(
                task_hash=task_hash,
                task_description=task,
                selected_experts=selected_experts,
                fusion_mode=fusion_mode,
                result=result,
                quality_score=quality_score,
                execution_time=execution_time,
                timestamp=time.time()
            )
            
            # æ·˜æ±°æ—§æ¡ç›®
            if len(self.cache) >= self.cache_size:
                self._evict_lru_entries()
            
            # å­˜å‚¨æ–°æ¡ç›®
            self.cache[task_hash] = entry
            self._update_lru(task_hash)
            
            # æ›´æ–°è®¿é—®é¢‘ç‡
            self.access_frequency[task_hash] += 1
            
            logger.info(f"ç¼“å­˜å­˜å‚¨: {task[:50]}... (è´¨é‡: {quality_score:.2f})")
    
    def find_similar_tasks(self, task: str, threshold: float = 0.7) -> List[FusionCacheEntry]:
        """æŸ¥æ‰¾ç›¸ä¼¼ä»»åŠ¡"""
        with self._lock:
            similar_tasks = []
            
            for entry in self.cache.values():
                similarity = self._calculate_similarity(task, entry.task_description)
                if similarity >= threshold:
                    entry.context_similarity = similarity
                    similar_tasks.append(entry)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similar_tasks.sort(key=lambda x: x.context_similarity, reverse=True)
            return similar_tasks[:10]  # è¿”å›å‰10ä¸ªæœ€ç›¸ä¼¼çš„
    
    def get_precomputed_pattern(self, task_keywords: Set[str]) -> Optional[PrecomputedPattern]:
        """è·å–é¢„è®¡ç®—æ¨¡å¼"""
        with self._lock:
            pattern_hash = hashlib.md5(str(sorted(task_keywords)).encode()).hexdigest()
            
            if pattern_hash in self.patterns:
                pattern = self.patterns[pattern_hash]
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                if time.time() - pattern.last_updated > 3600:  # 1å°æ—¶æ›´æ–°ä¸€æ¬¡
                    return None
                
                self.stats["precomputed_hits"] += 1
                logger.info(f"é¢„è®¡ç®—æ¨¡å¼å‘½ä¸­: {task_keywords}")
                return pattern
            
            return None
    
    def update_precomputed_pattern(self, task_keywords: Set[str], 
                                 common_experts: List[str], 
                                 fusion_mode: str,
                                 success_rate: float,
                                 quality_score: float):
        """æ›´æ–°é¢„è®¡ç®—æ¨¡å¼"""
        with self._lock:
            pattern_hash = hashlib.md5(str(sorted(task_keywords)).encode()).hexdigest()
            
            if pattern_hash in self.patterns:
                pattern = self.patterns[pattern_hash]
                # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°
                alpha = 0.1
                pattern.success_rate = alpha * success_rate + (1 - alpha) * pattern.success_rate
                pattern.avg_quality_score = alpha * quality_score + (1 - alpha) * pattern.avg_quality_score
                pattern.last_updated = time.time()
            else:
                pattern = PrecomputedPattern(
                    pattern_hash=pattern_hash,
                    task_keywords=task_keywords,
                    common_experts=common_experts,
                    optimal_fusion_mode=fusion_mode,
                    success_rate=success_rate,
                    avg_quality_score=quality_score,
                    last_updated=time.time()
                )
                self.patterns[pattern_hash] = pattern
    
    def get_cache_statistics(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            total_requests = self.stats["total_requests"]
            if total_requests == 0:
                cache_hit_rate = 0.0
            else:
                cache_hit_rate = self.stats["hits"] / total_requests
            
            # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
            avg_response_time = self.stats["avg_response_time"]
            
            # ç¼“å­˜æ•ˆç‡
            cache_efficiency = (self.stats["hits"] + self.stats["precomputed_hits"]) / max(total_requests, 1)
            
            return {
                "cache_hit_rate": cache_hit_rate,
                "precomputed_hit_rate": self.stats["precomputed_hits"] / max(total_requests, 1),
                "total_cache_entries": len(self.cache),
                "total_patterns": len(self.patterns),
                "total_requests": total_requests,
                "avg_response_time": avg_response_time,
                "cache_efficiency": cache_efficiency,
                "memory_usage_mb": self._estimate_memory_usage()
            }
    
    def _estimate_memory_usage(self) -> float:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡"""
        try:
            # ç®€å•ä¼°ç®—
            cache_size = len(pickle.dumps(self.cache, protocol=pickle.HIGHEST_PROTOCOL))
            pattern_size = len(pickle.dumps(self.patterns, protocol=pickle.HIGHEST_PROTOCOL))
            total_bytes = cache_size + pattern_size
            return total_bytes / (1024 * 1024)  # è½¬æ¢ä¸ºMB
        except:
            return 0.0
    
    def cleanup_expired_entries(self):
        """æ¸…ç†è¿‡æœŸæ¡ç›®"""
        with self._lock:
            current_time = time.time()
            expired_keys = []
            
            for task_hash, entry in self.cache.items():
                if current_time - entry.timestamp > self.ttl_hours * 3600:
                    expired_keys.append(task_hash)
            
            for key in expired_keys:
                self._remove_entry(key)
            
            if expired_keys:
                logger.info(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜æ¡ç›®")
    
    def _load_persisted_cache(self):
        """åŠ è½½æŒä¹…åŒ–ç¼“å­˜"""
        cache_file = PROJECT_ROOT / ".iflow" / "cache" / "fusion_cache.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                    self.cache = data.get('cache', {})
                    self.patterns = data.get('patterns', {})
                    self.access_frequency = defaultdict(int, data.get('access_frequency', {}))
                logger.info("æŒä¹…åŒ–ç¼“å­˜åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"åŠ è½½æŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")
    
    def persist_cache(self):
        """æŒä¹…åŒ–ç¼“å­˜"""
        cache_dir = PROJECT_ROOT / ".iflow" / "cache"
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        cache_file = cache_dir / "fusion_cache.pkl"
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'cache': self.cache,
                    'patterns': self.patterns,
                    'access_frequency': dict(self.access_frequency)
                }, f)
            logger.info("ç¼“å­˜æŒä¹…åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"æŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self.cache.clear()
            self.patterns.clear()
            self.access_frequency.clear()
            self.lru_queue.clear()
            logger.info("ç¼“å­˜å·²æ¸…ç©º")
    
    async def background_maintenance(self):
        """åå°ç»´æŠ¤ä»»åŠ¡"""
        while True:
            try:
                # æ¸…ç†è¿‡æœŸæ¡ç›®
                self.cleanup_expired_entries()
                
                # æŒä¹…åŒ–ç¼“å­˜
                self.persist_cache()
                
                # æ›´æ–°ç»Ÿè®¡
                stats = self.get_cache_statistics()
                logger.debug(f"ç¼“å­˜ç»Ÿè®¡: {stats}")
                
                # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
                await asyncio.sleep(3600)
                
            except Exception as e:
                logger.error(f"åå°ç»´æŠ¤ä»»åŠ¡å‡ºé”™: {e}")
                await asyncio.sleep(600)  # å‡ºé”™åç­‰å¾…10åˆ†é’Ÿ

class IntelligentFusionOptimizer:
    """
    æ™ºèƒ½èåˆä¼˜åŒ–å™¨
    """
    
    def __init__(self, cache: OptimizedFusionCache):
        self.cache = cache
        self.prediction_model = {}
        self.optimization_history = deque(maxlen=1000)
    
    def predict_optimal_experts(self, task: str, context: Optional[Dict[str, Any]] = None) -> List[str]:
        """é¢„æµ‹æœ€ä¼˜ä¸“å®¶ç»„åˆ"""
        # ä»ç›¸ä¼¼ä»»åŠ¡ä¸­å­¦ä¹ 
        similar_tasks = self.cache.find_similar_tasks(task, threshold=0.6)
        
        if not similar_tasks:
            return []
        
        # ç»Ÿè®¡ä¸“å®¶ä½¿ç”¨é¢‘ç‡
        expert_frequency = defaultdict(int)
        for task_entry in similar_tasks:
            for expert in task_entry.selected_experts:
                expert_frequency[expert] += 1
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_experts = sorted(expert_frequency.items(), key=lambda x: x[1], reverse=True)
        return [expert for expert, freq in sorted_experts[:5]]
    
    def predict_optimal_fusion_mode(self, task_complexity: str, expert_count: int) -> str:
        """é¢„æµ‹æœ€ä¼˜èåˆæ¨¡å¼"""
        # åŸºäºå†å²æ•°æ®çš„ç®€å•é¢„æµ‹
        mode_scores = {
            "sequential": 0.8,
            "parallel": 0.9,
            "collaborative": 0.85,
            "hierarchical": 0.95,
            "adaptive": 1.0
        }
        
        # æ ¹æ®å¤æ‚åº¦å’Œä¸“å®¶æ•°é‡è°ƒæ•´
        if task_complexity in ["simple", "moderate"]:
            mode_scores["sequential"] += 0.1
        elif task_complexity in ["complex", "expert"]:
            mode_scores["hierarchical"] += 0.1
            mode_scores["adaptive"] += 0.1
        
        if expert_count <= 2:
            mode_scores["collaborative"] += 0.1
        elif expert_count > 5:
            mode_scores["parallel"] += 0.1
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„æ¨¡å¼
        return max(mode_scores.items(), key=lambda x: x[1])[0]
    
    def optimize_fusion_parameters(self, task: str, base_experts: List[str], 
                                 base_mode: str) -> Dict[str, Any]:
        """ä¼˜åŒ–èåˆå‚æ•°"""
        # é¢„æµ‹ä¼˜åŒ–çš„ä¸“å®¶ç»„åˆ
        predicted_experts = self.predict_optimal_experts(task)
        
        # å¦‚æœé¢„æµ‹çš„ä¸“å®¶ç»„åˆæ›´å¥½ï¼Œä½¿ç”¨é¢„æµ‹ç»“æœ
        if len(predicted_experts) > len(base_experts) * 0.5:
            optimized_experts = list(set(base_experts + predicted_experts))
        else:
            optimized_experts = base_experts
        
        # é¢„æµ‹ä¼˜åŒ–çš„èåˆæ¨¡å¼
        task_complexity = self._infer_complexity(task)
        optimized_mode = self.predict_optimal_fusion_mode(task_complexity, len(optimized_experts))
        
        return {
            "optimized_experts": optimized_experts,
            "optimized_mode": optimized_mode,
            "confidence": 0.8 if predicted_experts else 0.6,
            "optimization_reason": "åŸºäºå†å²ç›¸ä¼¼ä»»åŠ¡çš„ä¼˜åŒ–å»ºè®®" if predicted_experts else "ä½¿ç”¨é»˜è®¤ä¼˜åŒ–ç­–ç•¥"
        }
    
    def _infer_complexity(self, task: str) -> str:
        """æ¨æ–­ä»»åŠ¡å¤æ‚åº¦"""
        task_lower = task.lower()
        
        if any(keyword in task_lower for keyword in ["ç®€å•", "åŸºç¡€", "å¿«é€Ÿ"]):
            return "simple"
        elif any(keyword in task_lower for keyword in ["åˆ†æ", "è®¾è®¡", "å®ç°"]):
            return "moderate"
        elif any(keyword in task_lower for keyword in ["æ¶æ„", "ç³»ç»Ÿ", "é›†æˆ"]):
            return "complex"
        elif any(keyword in task_lower for keyword in ["é«˜çº§", "æ·±åº¦", "ä¸“å®¶"]):
            return "expert"
        else:
            return "moderate"

# --- ä½¿ç”¨ç¤ºä¾‹ ---
async def main():
    """ç¤ºä¾‹ä½¿ç”¨"""
    # åˆ›å»ºç¼“å­˜ç³»ç»Ÿ
    cache = OptimizedFusionCache(cache_size=500, ttl_hours=12)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = IntelligentFusionOptimizer(cache)
    
    # æ¨¡æ‹Ÿç¼“å­˜ä½¿ç”¨
    task = "è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„ç”µå•†ç³»ç»Ÿæ¶æ„"
    result = cache.get_cached_result(task)
    
    if not result:
        # æ¨¡æ‹Ÿè®¡ç®—ç»“æœ
        cache.put_cache_result(
            task=task,
            context={"domain": "ç”µå•†", "scale": "å¤§å‹"},
            selected_experts=["æ¶æ„å¸ˆ", "æ€§èƒ½ä¸“å®¶", "å®‰å…¨ä¸“å®¶"],
            fusion_mode="hierarchical",
            result="æ¶æ„è®¾è®¡æ–¹æ¡ˆ...",
            quality_score=0.95,
            execution_time=2.5
        )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = cache.get_cache_statistics()
    print(f"ç¼“å­˜ç»Ÿè®¡: {json.dumps(stats, indent=2)}")
    
    # é¢„æµ‹ä¼˜åŒ–
    optimization = optimizer.optimize_fusion_parameters(
        task, ["æ¶æ„å¸ˆ"], "sequential"
    )
    print(f"ä¼˜åŒ–å»ºè®®: {optimization}")

if __name__ == "__main__":
    asyncio.run(main())