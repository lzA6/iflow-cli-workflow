#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“š æ”¹è¿›ç‰ˆæœ¬åœ°çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ V3.0 (é‡æ„ç‰ˆ)
================================

é‡æ„ç‰ˆæœ¬ï¼Œä¸“æ³¨äºï¼š
- ä½å¤æ‚åº¦å’Œé«˜å¯ç»´æŠ¤æ€§
- å•ä¸€èŒè´£åŸåˆ™
- ä¾èµ–æ³¨å…¥å’Œè®¾è®¡æ¨¡å¼åº”ç”¨
- æ¨¡å—åŒ–å’Œå¯æµ‹è¯•æ€§

ä½œè€…: AIæ¶æ„å¸ˆå›¢é˜Ÿ
ç‰ˆæœ¬: 3.0.0
æ—¥æœŸ: 2025-11-16
"""

import asyncio
import gc
import json
import logging
import time
import threading
from abc import ABC, abstractmethod
from collections import defaultdict
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import (
    Any, Callable, Dict, List, Optional, Set, Type, TypeVar, Union,
    Protocol, runtime_checkable
)

import faiss
import numpy as np
import psutil

# é¡¹ç›®é…ç½®
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
KNOWLEDGE_BASE_ROOT = PROJECT_ROOT / "knowledge_base"

# ç±»å‹å®šä¹‰
T = TypeVar('T')


class iFlowException(Exception):
    """iFlowåŸºç¡€å¼‚å¸¸ç±»"""
    
    def __init__(
        self, 
        message: str, 
        error_code: Optional[str] = None, 
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        super().__init__(message)
        self.message = message
        self.error_code = error_code
        self.details = details or {}
        self.timestamp = datetime.now()


class ConfigurationError(iFlowException):
    """é…ç½®é”™è¯¯"""
    pass


class ValidationError(iFlowException):
    """éªŒè¯é”™è¯¯"""
    pass


class SearchError(iFlowException):
    """æœç´¢é”™è¯¯"""
    pass


class ComponentError(iFlowException):
    """ç»„ä»¶é”™è¯¯"""
    pass


class ComponentStatus(Enum):
    """ç»„ä»¶çŠ¶æ€"""
    INITIALIZING = "initializing"
    RUNNING = "running"
    STOPPED = "stopped"
    ERROR = "error"


class EventType(Enum):
    """äº‹ä»¶ç±»å‹"""
    SYSTEM_START = "system_start"
    SYSTEM_STOP = "system_stop"
    COMPONENT_ADDED = "component_added"
    DOCUMENT_ADDED = "document_added"
    SEARCH_PERFORMED = "search_performed"
    ERROR_OCCURRED = "error_occurred"


@dataclass
class BaseConfig:
    """åŸºç¡€é…ç½®ç±»"""
    name: str
    description: str = ""
    version: str = "1.0.0"
    enabled: bool = True
    
    def validate(self) -> List[str]:
        """éªŒè¯é…ç½®"""
        errors = []
        if not self.name or not self.name.strip():
            errors.append("åç§°ä¸èƒ½ä¸ºç©º")
        return errors
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


@dataclass
class KnowledgeBaseConfig(BaseConfig):
    """çŸ¥è¯†åº“é…ç½®"""
    path: str = ""
    file_types: List[str] = field(
        default_factory=lambda: [".txt", ".md", ".pdf", ".docx", ".doc", ".html", ".py", ".js", ".json", ".xml"]
    )
    max_file_size: int = 100 * 1024 * 1024  # 100MB
    auto_index: bool = True
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    index_batch_size: int = 100
    chunk_size: int = 1000
    chunk_overlap: int = 200
    max_memory_usage_mb: int = 512
    cache_size: int = 1000
    index_cache_ttl: int = 3600
    auto_cleanup_interval: int = 300
    
    def validate(self) -> List[str]:
        """éªŒè¯é…ç½®"""
        errors = super().validate()
        
        if not self.path:
            errors.append("è·¯å¾„ä¸èƒ½ä¸ºç©º")
        elif not Path(self.path).exists():
            errors.append(f"è·¯å¾„ä¸å­˜åœ¨: {self.path}")
        
        if self.max_file_size <= 0:
            errors.append("æœ€å¤§æ–‡ä»¶å¤§å°å¿…é¡»å¤§äº0")
        
        if self.chunk_size <= 0:
            errors.append("å—å¤§å°å¿…é¡»å¤§äº0")
        
        if self.max_memory_usage_mb <= 0:
            errors.append("æœ€å¤§å†…å­˜ä½¿ç”¨é‡å¿…é¡»å¤§äº0")
        
        return errors


@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—"""
    chunk_id: str
    doc_id: str
    group_id: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None
    created_at: datetime = field(default_factory=datetime.now)
    
    def __post_init__(self) -> None:
        """åˆå§‹åŒ–åå¤„ç†"""
        if not self.chunk_id:
            raise ValidationError("å—IDä¸èƒ½ä¸ºç©º")
        if not self.content:
            raise ValidationError("å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        if self.embedding is not None:
            result['embedding'] = self.embedding.tolist()
        return result


@dataclass
class KnowledgeGroup:
    """çŸ¥è¯†åº“ç»„"""
    group_id: str
    name: str
    description: str
    path: str
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    document_count: int = 0
    total_size: int = 0
    tags: List[str] = field(default_factory=list)
    
    def __post_init__(self) -> None:
        """åˆå§‹åŒ–åå¤„ç†"""
        if not self.group_id:
            raise ValidationError("ç»„IDä¸èƒ½ä¸ºç©º")
        if not self.name:
            raise ValidationError("ç»„åä¸èƒ½ä¸ºç©º")
    
    def update_timestamp(self) -> None:
        """æ›´æ–°æ—¶é—´æˆ³"""
        self.updated_at = datetime.now()
    
    def add_document(self, file_size: int) -> None:
        """æ·»åŠ æ–‡æ¡£"""
        self.document_count += 1
        self.total_size += file_size
        self.update_timestamp()
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        result['created_at'] = self.created_at.isoformat()
        result['updated_at'] = self.updated_at.isoformat()
        return result


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    chunk_id: str
    doc_id: str
    group_id: str
    content: str
    score: float
    metadata: Dict[str, Any]
    highlights: List[str] = field(default_factory=list)
    
    def __post_init__(self) -> None:
        """åˆå§‹åŒ–åå¤„ç†"""
        if not 0 <= self.score <= 1:
            raise ValidationError("åˆ†æ•°å¿…é¡»åœ¨0-1ä¹‹é—´")
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


# ============================================================================
# æ¥å£å®šä¹‰
# ============================================================================

@runtime_checkable
class EventListener(Protocol):
    """äº‹ä»¶ç›‘å¬å™¨åè®®"""
    
    def handle_event(self, event_type: EventType, data: Any) -> None:
        """å¤„ç†äº‹ä»¶"""
        ...


class IConfigValidator(ABC):
    """é…ç½®éªŒè¯å™¨æ¥å£"""
    
    @abstractmethod
    def validate(self, config: BaseConfig) -> List[str]:
        """éªŒè¯é…ç½®"""
        pass


class IIndexManager(ABC):
    """ç´¢å¼•ç®¡ç†å™¨æ¥å£"""
    
    @abstractmethod
    def initialize_index(self, embedding_dimension: int) -> None:
        """åˆå§‹åŒ–ç´¢å¼•"""
        pass
    
    @abstractmethod
    def add_embeddings(self, embeddings: np.ndarray) -> None:
        """æ·»åŠ åµŒå…¥å‘é‡"""
        pass
    
    @abstractmethod
    def save_index(self) -> None:
        """ä¿å­˜ç´¢å¼•"""
        pass


class IDocumentProcessor(ABC):
    """æ–‡æ¡£å¤„ç†å™¨æ¥å£"""
    
    @abstractmethod
    def create_chunks(self, content: str, doc_id: str, group_id: str, file_path: str) -> List[DocumentChunk]:
        """åˆ›å»ºæ–‡æ¡£å—"""
        pass


class ISearchStrategy(ABC):
    """æœç´¢ç­–ç•¥æ¥å£"""
    
    @abstractmethod
    def search(self, query: str, data: List[Any], top_k: int = 10) -> List[SearchResult]:
        """æ‰§è¡Œæœç´¢"""
        pass


# ============================================================================
# æ ¸å¿ƒç»„ä»¶å®ç°
# ============================================================================

class ConfigValidator(IConfigValidator):
    """é…ç½®éªŒè¯å™¨"""
    
    def validate(self, config: BaseConfig) -> List[str]:
        """éªŒè¯é…ç½®"""
        return config.validate()


class EventDispatcher:
    """äº‹ä»¶åˆ†å‘å™¨"""
    
    def __init__(self) -> None:
        self._listeners: Dict[EventType, List[EventListener]] = defaultdict(list)
        self._lock = threading.RLock()
        self._logger = logging.getLogger(__name__)
    
    def add_listener(self, event_type: EventType, listener: EventListener) -> None:
        """æ·»åŠ äº‹ä»¶ç›‘å¬å™¨"""
        with self._lock:
            self._listeners[event_type].append(listener)
    
    def remove_listener(self, event_type: EventType, listener: EventListener) -> None:
        """ç§»é™¤äº‹ä»¶ç›‘å¬å™¨"""
        with self._lock:
            if listener in self._listeners[event_type]:
                self._listeners[event_type].remove(listener)
    
    def emit_event(self, event_type: EventType, data: Any = None) -> None:
        """å‘å¸ƒäº‹ä»¶"""
        with self._lock:
            listeners = self._listeners[event_type].copy()
        
        for listener in listeners:
            try:
                listener.handle_event(event_type, data)
            except Exception as e:
                self._logger.error(f"äº‹ä»¶å¤„ç†å¤±è´¥: {e}")


class ErrorHandler:
    """é”™è¯¯å¤„ç†å™¨"""
    
    def __init__(self, logger: logging.Logger) -> None:
        self._logger = logger
    
    def handle_error(self, error: Exception, context: str = "") -> None:
        """å¤„ç†é”™è¯¯"""
        error_msg = f"{context}: {error}" if context else str(error)
        self._logger.error(error_msg)
        
        # å¯ä»¥æ·»åŠ æ›´å¤šé”™è¯¯å¤„ç†é€»è¾‘ï¼Œå¦‚å‘é€é€šçŸ¥ã€è®°å½•åˆ°æ•°æ®åº“ç­‰
        if isinstance(error, (ConfigurationError, ValidationError)):
            self._logger.warning(f"é…ç½®/éªŒè¯é”™è¯¯: {error_msg}")
        elif isinstance(error, (SearchError, ComponentError)):
            self._logger.error(f"ä¸šåŠ¡é”™è¯¯: {error_msg}")
        else:
            self._logger.critical(f"æœªçŸ¥é”™è¯¯: {error_msg}")


class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_memory_mb: int, cleanup_interval: int = 300) -> None:
        self.max_memory_mb = max_memory_mb
        self.cleanup_interval = cleanup_interval
        self._last_cleanup = time.time()
        self._logger = logging.getLogger(__name__)
        self._monitor_thread: Optional[threading.Thread] = None
    
    def start_monitoring(self) -> None:
        """å¯åŠ¨å†…å­˜ç›‘æ§"""
        if self._monitor_thread is None:
            self._monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
            self._monitor_thread.start()
            self._logger.info("å†…å­˜ç›‘æ§å·²å¯åŠ¨")
    
    def _monitor_loop(self) -> None:
        """ç›‘æ§å¾ªç¯"""
        while True:
            try:
                if self._check_memory_limit():
                    self._logger.warning(f"å†…å­˜ä½¿ç”¨è¶…é™: {self._get_memory_usage():.2f}MB")
                    self._cleanup()
                    gc.collect()
                
                # å®šæœŸæ¸…ç†
                if time.time() - self._last_cleanup > self.cleanup_interval:
                    self._cleanup()
                    self._last_cleanup = time.time()
                
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                self._logger.error(f"å†…å­˜ç›‘æ§é”™è¯¯: {e}")
                time.sleep(60)
    
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except Exception:
            return 0.0
    
    def _check_memory_limit(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…è¿‡å†…å­˜é™åˆ¶"""
        return self._get_memory_usage() > self.max_memory_mb
    
    def _cleanup(self) -> None:
        """æ¸…ç†å†…å­˜"""
        gc.collect()
        self._logger.debug("æ‰§è¡Œå†…å­˜æ¸…ç†")


class IndexManager(IIndexManager):
    """ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(self, index_path: Path) -> None:
        self.index_path = index_path
        self.index: Optional[faiss.Index] = None
        self._lock = threading.RLock()
        self._logger = logging.getLogger(__name__)
    
    def initialize_index(self, embedding_dimension: int) -> None:
        """åˆå§‹åŒ–ç´¢å¼•"""
        with self._lock:
            try:
                if self.index_path.exists():
                    self.index = faiss.read_index(str(self.index_path))
                    self._logger.info(f"ğŸ“– åŠ è½½ç°æœ‰ç´¢å¼•ï¼ŒåŒ…å« {self.index.ntotal} ä¸ªå‘é‡")
                else:
                    self.index = faiss.IndexFlatIP(embedding_dimension)
                    self._logger.info("ğŸ†• åˆ›å»ºæ–°çš„Faissç´¢å¼•")
            except Exception as e:
                self._logger.error(f"ç´¢å¼•åˆå§‹åŒ–å¤±è´¥: {e}")
                self.index = faiss.IndexFlatIP(embedding_dimension)
    
    def add_embeddings(self, embeddings: np.ndarray) -> None:
        """æ·»åŠ åµŒå…¥å‘é‡"""
        with self._lock:
            if self.index is None:
                raise ComponentError("ç´¢å¼•æœªåˆå§‹åŒ–")
            
            # å½’ä¸€åŒ–åµŒå…¥å‘é‡
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            normalized_embeddings = embeddings / norms
            
            self.index.add(normalized_embeddings)
            self._logger.info(f"æ·»åŠ äº† {len(embeddings)} ä¸ªå‘é‡åˆ°ç´¢å¼•")
    
    def save_index(self) -> None:
        """ä¿å­˜ç´¢å¼•"""
        if self.index is None:
            return
        
        self.index_path.parent.mkdir(parents=True, exist_ok=True)
        try:
            faiss.write_index(self.index, str(self.index_path))
            self._logger.info(f"ğŸ’¾ ä¿å­˜ç´¢å¼•ï¼ŒåŒ…å« {self.index.ntotal} ä¸ªå‘é‡")
        except Exception as e:
            self._logger.error(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
    
    def get_index_size(self) -> int:
        """è·å–ç´¢å¼•å¤§å°"""
        return self.index.ntotal if self.index else 0


class DocumentProcessor(IDocumentProcessor):
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, chunk_size: int, chunk_overlap: int) -> None:
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self._logger = logging.getLogger(__name__)
    
    def create_chunks(self, content: str, doc_id: str, group_id: str, file_path: str) -> List[DocumentChunk]:
        """åˆ›å»ºæ–‡æ¡£å—"""
        if not content.strip():
            self._logger.warning(f"æ–‡æ¡£å†…å®¹ä¸ºç©º: {file_path}")
            return []
        
        chunks = []
        words = content.split()
        
        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):
            chunk_words = words[i:i + self.chunk_size]
            chunk_content = " ".join(chunk_words)
            
            chunk = DocumentChunk(
                chunk_id=f"chunk_{doc_id}_{i}",
                doc_id=doc_id,
                group_id=group_id,
                content=chunk_content,
                metadata={
                    "file_path": file_path,
                    "chunk_index": i,
                    "word_count": len(chunk_words),
                    "char_count": len(chunk_content)
                }
            )
            
            chunks.append(chunk)
        
        self._logger.debug(f"ä¸ºæ–‡æ¡£ {doc_id} åˆ›å»ºäº† {len(chunks)} ä¸ªå—")
        return chunks


class KeywordSearchStrategy(ISearchStrategy):
    """å…³é”®è¯æœç´¢ç­–ç•¥"""
    
    def search(self, query: str, data: List[Any], top_k: int = 10) -> List[SearchResult]:
        """æ‰§è¡Œå…³é”®è¯æœç´¢"""
        query_words = set(query.lower().split())
        results = []
        
        for item in data:
            content = item.get('content', '').lower()
            content_words = set(content.split())
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            match_count = len(query_words & content_words)
            if match_count > 0:
                score = match_count / len(query_words)
                result = SearchResult(
                    chunk_id=item.get('chunk_id', ''),
                    doc_id=item.get('doc_id', ''),
                    group_id=item.get('group_id', ''),
                    content=item.get('content', ''),
                    score=score,
                    metadata=item.get('metadata', {}),
                    highlights=self._extract_highlights(content, query_words)
                )
                results.append(result)
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:top_k]
    
    def _extract_highlights(self, content: str, query_words: Set[str]) -> List[str]:
        """æå–é«˜äº®ç‰‡æ®µ"""
        highlights = []
        for word in query_words:
            if word in content:
                start = content.find(word)
                if start != -1:
                    context_start = max(0, start - 20)
                    context_end = min(len(content), start + len(word) + 20)
                    highlight = content[context_start:context_end]
                    if highlight not in highlights:
                        highlights.append(highlight)
        return highlights


class VectorSearchStrategy(ISearchStrategy):
    """å‘é‡æœç´¢ç­–ç•¥"""
    
    def __init__(self, embedding_dimension: int = 384):
        self.embedding_dimension = embedding_dimension
    
    def search(self, query: str, data: List[Any], top_k: int = 10) -> List[SearchResult]:
        """æ‰§è¡Œå‘é‡æœç´¢"""
        # ç®€åŒ–çš„å‘é‡æœç´¢å®ç°
        results = []
        for item in data[:top_k]:
            result = SearchResult(
                chunk_id=item.get('chunk_id', ''),
                doc_id=item.get('doc_id', ''),
                group_id=item.get('group_id', ''),
                content=item.get('content', ''),
                score=np.random.random(),  # ç®€åŒ–çš„åˆ†æ•°è®¡ç®—
                metadata=item.get('metadata', {})
            )
            results.append(result)
        return results


class SearchEngine:
    """æœç´¢å¼•æ“"""
    
    def __init__(self, default_strategy: ISearchStrategy) -> None:
        self._strategy = default_strategy
        self._logger = logging.getLogger(__name__)
    
    def set_strategy(self, strategy: ISearchStrategy) -> None:
        """è®¾ç½®æœç´¢ç­–ç•¥"""
        self._strategy = strategy
    
    def search(self, query: str, data: List[Any], top_k: int = 10) -> List[SearchResult]:
        """æ‰§è¡Œæœç´¢"""
        if not query or not query.strip():
            raise ValidationError("æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")
        
        if top_k <= 0:
            raise ValidationError("top_kå¿…é¡»å¤§äº0")
        
        try:
            results = self._strategy.search(query, data, top_k)
            self._logger.info(f"ğŸ” æœç´¢å®Œæˆ: æŸ¥è¯¢='{query}'ï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
            return results
        except Exception as e:
            raise SearchError(f"æœç´¢å¤±è´¥: {e}")


# ============================================================================
# ä¾èµ–æ³¨å…¥å®¹å™¨
# ============================================================================

class DIContainer:
    """ä¾èµ–æ³¨å…¥å®¹å™¨"""
    
    def __init__(self) -> None:
        self._services: Dict[str, Any] = {}
        self._singletons: Dict[str, Any] = {}
    
    def register(self, name: str, factory: Callable[[], Any], singleton: bool = True) -> None:
        """æ³¨å†ŒæœåŠ¡"""
        self._services[name] = (factory, singleton)
    
    def get(self, name: str) -> Any:
        """è·å–æœåŠ¡"""
        if name not in self._services:
            raise ValueError(f"æœåŠ¡æœªæ³¨å†Œ: {name}")
        
        factory, singleton = self._services[name]
        
        if singleton:
            if name not in self._singletons:
                self._singletons[name] = factory()
            return self._singletons[name]
        else:
            return factory()
    
    def has(self, name: str) -> bool:
        """æ£€æŸ¥æœåŠ¡æ˜¯å¦å­˜åœ¨"""
        return name in self._services


# ============================================================================
# é‡æ„åçš„ä¸»ç®¡ç†å™¨
# ============================================================================

class KnowledgeBaseManager:
    """é‡æ„ç‰ˆçŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, config: Optional[KnowledgeBaseConfig] = None) -> None:
        self.config = config or self._create_default_config()
        self._logger = self._setup_logging()
        
        # åˆå§‹åŒ–ä¾èµ–æ³¨å…¥å®¹å™¨
        self._container = self._setup_container()
        
        # è·å–ä¾èµ–
        self._config_validator = self._container.get("config_validator")
        self._error_handler = self._container.get("error_handler")
        self._event_dispatcher = self._container.get("event_dispatcher")
        self._memory_manager = self._container.get("memory_manager")
        self._index_manager = self._container.get("index_manager")
        self._document_processor = self._container.get("document_processor")
        self._search_engine = self._container.get("search_engine")
        
        # æ•°æ®å­˜å‚¨
        self.groups: Dict[str, KnowledgeGroup] = {}
        self.documents: Dict[str, Dict[str, Any]] = {}
        self.chunks: List[DocumentChunk] = []
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        self._initialize()
    
    def _create_default_config(self) -> KnowledgeBaseConfig:
        """åˆ›å»ºé»˜è®¤é…ç½®"""
        return KnowledgeBaseConfig(
            name="default",
            description="é»˜è®¤çŸ¥è¯†åº“",
            path=str(KNOWLEDGE_BASE_ROOT / "documents")
        )
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        log_dir = KNOWLEDGE_BASE_ROOT / "logs"
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / "knowledge_manager.log"),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def _setup_container(self) -> DIContainer:
        """è®¾ç½®ä¾èµ–æ³¨å…¥å®¹å™¨"""
        container = DIContainer()
        
        # æ³¨å†ŒæœåŠ¡
        container.register("config_validator", lambda: ConfigValidator())
        container.register("error_handler", lambda: ErrorHandler(self._logger))
        container.register("event_dispatcher", lambda: EventDispatcher())
        container.register(
            "memory_manager", 
            lambda: MemoryManager(self.config.max_memory_usage_mb, self.config.auto_cleanup_interval)
        )
        container.register(
            "index_manager",
            lambda: IndexManager(KNOWLEDGE_BASE_ROOT / "indexes" / "faiss_index.bin")
        )
        container.register(
            "document_processor",
            lambda: DocumentProcessor(self.config.chunk_size, self.config.chunk_overlap)
        )
        container.register(
            "search_engine",
            lambda: SearchEngine(VectorSearchStrategy())
        )
        
        return container
    
    def _initialize(self) -> None:
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        try:
            # éªŒè¯é…ç½®
            errors = self._config_validator.validate(self.config)
            if errors:
                raise ConfigurationError(f"é…ç½®éªŒè¯å¤±è´¥: {', '.join(errors)}")
            
            # åˆ›å»ºå¿…è¦çš„ç›®å½•
            self._create_directories()
            
            # åŠ è½½æ•°æ®
            self._load_groups()
            
            # åˆå§‹åŒ–ç´¢å¼•
            self._index_manager.initialize_index(384)
            
            # å¯åŠ¨åå°æœåŠ¡
            self._memory_manager.start_monitoring()
            
            # å‘é€å¯åŠ¨äº‹ä»¶
            self._event_dispatcher.emit_event(EventType.SYSTEM_START)
            self._logger.info("ğŸ“š çŸ¥è¯†åº“ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self._error_handler.handle_error(e, "åˆå§‹åŒ–å¤±è´¥")
            raise ComponentError(f"åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _create_directories(self) -> None:
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = ["groups", "indexes", "logs", "config", "cache"]
        for dir_name in directories:
            (KNOWLEDGE_BASE_ROOT / dir_name).mkdir(exist_ok=True)
    
    def _load_groups(self) -> None:
        """åŠ è½½ç»„ä¿¡æ¯"""
        groups_file = KNOWLEDGE_BASE_ROOT / "config" / "groups.json"
        if groups_file.exists():
            try:
                with open(groups_file, 'r', encoding='utf-8') as f:
                    groups_data = json.load(f)
                    for group_data in groups_data:
                        group = KnowledgeGroup(
                            group_id=group_data["group_id"],
                            name=group_data["name"],
                            description=group_data["description"],
                            path=group_data["path"],
                            created_at=datetime.fromisoformat(group_data["created_at"]),
                            updated_at=datetime.fromisoformat(group_data["updated_at"]),
                            document_count=group_data.get("document_count", 0),
                            total_size=group_data.get("total_size", 0),
                            tags=group_data.get("tags", [])
                        )
                        self.groups[group.group_id] = group
            except Exception as e:
                self._error_handler.handle_error(e, "åŠ è½½ç»„ä¿¡æ¯å¤±è´¥")
    
    def _save_groups(self) -> None:
        """ä¿å­˜ç»„ä¿¡æ¯"""
        groups_file = KNOWLEDGE_BASE_ROOT / "config" / "groups.json"
        try:
            groups_data = [group.to_dict() for group in self.groups.values()]
            with open(groups_file, 'w', encoding='utf-8') as f:
                json.dump(groups_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self._error_handler.handle_error(e, "ä¿å­˜ç»„ä¿¡æ¯å¤±è´¥")
    
    def create_group(
        self, 
        name: str, 
        description: str, 
        path: str, 
        tags: Optional[List[str]] = None
    ) -> str:
        """åˆ›å»ºçŸ¥è¯†åº“ç»„"""
        if not name or not name.strip():
            raise ValidationError("ç»„åä¸èƒ½ä¸ºç©º")
        
        if not path:
            raise ValidationError("è·¯å¾„ä¸èƒ½ä¸ºç©º")
        
        try:
            group_id = f"group_{int(time.time() * 1000)}"
            
            # åˆ›å»ºç›®å½•
            group_path = Path(path)
            group_path.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºç»„å¯¹è±¡
            group = KnowledgeGroup(
                group_id=group_id,
                name=name.strip(),
                description=description.strip(),
                path=str(group_path),
                tags=tags or []
            )
            
            self.groups[group_id] = group
            self._save_groups()
            
            self._event_dispatcher.emit_event(
                EventType.COMPONENT_ADDED, 
                {"type": "group", "id": group_id, "name": name}
            )
            
            self._logger.info(f"âœ… åˆ›å»ºçŸ¥è¯†åº“ç»„: {name} (ID: {group_id})")
            return group_id
            
        except Exception as e:
            self._error_handler.handle_error(e, f"åˆ›å»ºç»„å¤±è´¥: {name}")
            raise ComponentError(f"åˆ›å»ºç»„å¤±è´¥: {e}")
    
    def add_documents_from_path(
        self, 
        path: str, 
        group_id: str, 
        recursive: bool = True
    ) -> Dict[str, Any]:
        """ä»è·¯å¾„æ·»åŠ æ–‡æ¡£"""
        path_obj = Path(path)
        
        if not path_obj.exists():
            raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
        
        if group_id not in self.groups:
            raise ValidationError(f"ç»„IDä¸å­˜åœ¨: {group_id}")
        
        group = self.groups[group_id]
        added_files = []
        errors = []
        
        try:
            # æŸ¥æ‰¾æ”¯æŒçš„æ–‡ä»¶
            pattern = "**/*" if recursive else "*"
            
            for file_path in path_obj.glob(pattern):
                if file_path.is_file() and file_path.suffix.lower() in self.config.file_types:
                    try:
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°
                        if file_path.stat().st_size > self.config.max_file_size:
                            errors.append(f"{file_path.name}: æ–‡ä»¶è¿‡å¤§")
                            continue
                        
                        # è¯»å–æ–‡ä»¶å†…å®¹
                        content = file_path.read_text(encoding='utf-8', errors='ignore')
                        
                        # åˆ›å»ºæ–‡æ¡£å’Œå—
                        doc_id = f"doc_{int(time.time() * 1000)}_{len(self.documents)}"
                        chunks = self._document_processor.create_chunks(content, doc_id, group_id, str(file_path))
                        
                        # ä¿å­˜æ–‡æ¡£ä¿¡æ¯
                        self.documents[doc_id] = {
                            "doc_id": doc_id,
                            "file_path": str(file_path),
                            "group_id": group_id,
                            "file_name": file_path.name,
                            "file_size": file_path.stat().st_size,
                            "created_at": datetime.now().isoformat(),
                            "chunks": [chunk.chunk_id for chunk in chunks]
                        }
                        
                        self.chunks.extend(chunks)
                        added_files.append(str(file_path))
                        
                        # æ›´æ–°ç»„ç»Ÿè®¡
                        group.add_document(file_path.stat().st_size)
                        
                    except Exception as e:
                        errors.append(f"{file_path.name}: {str(e)}")
            
            # ä¿å­˜æ›´æ–°
            self._save_groups()
            self._update_index()
            
            self._event_dispatcher.emit_event(
                EventType.DOCUMENT_ADDED,
                {"count": len(added_files), "group_id": group_id}
            )
            
            result = {
                "added_files": added_files,
                "errors": errors,
                "total_chunks": len([c for c in self.chunks if c.group_id == group_id])
            }
            
            self._logger.info(f"ğŸ“„ æ·»åŠ æ–‡æ¡£å®Œæˆ: {len(added_files)} æˆåŠŸ, {len(errors)} å¤±è´¥")
            return result
            
        except Exception as e:
            self._error_handler.handle_error(e, "æ·»åŠ æ–‡æ¡£å¤±è´¥")
            raise ComponentError(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
    
    def _update_index(self) -> None:
        """æ›´æ–°ç´¢å¼•"""
        if not self.chunks:
            return
        
        # ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        embeddings = []
        for chunk in self.chunks[-self.config.index_batch_size:]:
            embedding = np.random.rand(384).astype('float32')
            embeddings.append(embedding)
            chunk.embedding = embedding
        
        if embeddings:
            embeddings_array = np.vstack(embeddings)
            self._index_manager.add_embeddings(embeddings_array)
            self._index_manager.save_index()
    
    def set_search_strategy(self, strategy: ISearchStrategy) -> None:
        """è®¾ç½®æœç´¢ç­–ç•¥"""
        self._search_engine.set_strategy(strategy)
    
    async def search(
        self, 
        query: str, 
        top_k: int = 10, 
        group_id: Optional[str] = None
    ) -> List[SearchResult]:
        """æœç´¢çŸ¥è¯†åº“"""
        search_start = time.time()
        
        try:
            # å‡†å¤‡æœç´¢æ•°æ®
            search_data = []
            for chunk in self.chunks:
                if group_id and chunk.group_id != group_id:
                    continue
                search_data.append({
                    "chunk_id": chunk.chunk_id,
                    "doc_id": chunk.doc_id,
                    "group_id": chunk.group_id,
                    "content": chunk.content,
                    "metadata": chunk.metadata
                })
            
            # æ‰§è¡Œæœç´¢
            results = self._search_engine.search(query, search_data, top_k)
            
            # è®°å½•æœç´¢äº‹ä»¶
            self._event_dispatcher.emit_event(
                EventType.SEARCH_PERFORMED,
                {
                    "query": query,
                    "top_k": top_k,
                    "group_id": group_id,
                    "results_count": len(results),
                    "search_time": time.time() - search_start
                }
            )
            
            return results
            
        except Exception as e:
            self._error_handler.handle_error(e, "æœç´¢å¤±è´¥")
            raise
    
    def get_knowledge_base_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        total_docs = len(self.documents)
        total_chunks = len(self.chunks)
        total_size = sum(doc.get("file_size", 0) for doc in self.documents.values())
        
        group_stats = []
        for group in self.groups.values():
            group_stats.append(group.to_dict())
        
        return {
            "total_documents": total_docs,
            "total_chunks": total_chunks,
            "total_size_mb": total_size / (1024 * 1024),
            "total_groups": len(self.groups),
            "index_size": self._index_manager.get_index_size(),
            "memory_usage_mb": self._memory_manager._get_memory_usage(),
            "groups": group_stats,
            "last_updated": datetime.now().isoformat()
        }


# ============================================================================
# å·¥å‚å‡½æ•°
# ============================================================================

def create_knowledge_base_manager(config: Optional[KnowledgeBaseConfig] = None) -> KnowledgeBaseManager:
    """åˆ›å»ºçŸ¥è¯†åº“ç®¡ç†å™¨å®ä¾‹"""
    return KnowledgeBaseManager(config)


# ============================================================================
# æµ‹è¯•å‡½æ•°
# ============================================================================

async def test_refactored_knowledge_base() -> None:
    """æµ‹è¯•é‡æ„ç‰ˆçŸ¥è¯†åº“"""
    try:
        # åˆ›å»ºç®¡ç†å™¨
        manager = create_knowledge_base_manager()
        
        # åˆ›å»ºæµ‹è¯•ç»„
        group_id = manager.create_group(
            name="é‡æ„æµ‹è¯•çŸ¥è¯†åº“",
            description="ç”¨äºé‡æ„éªŒè¯çš„çŸ¥è¯†åº“",
            path=str(KNOWLEDGE_BASE_ROOT / "test_docs"),
            tags=["æµ‹è¯•", "é‡æ„"]
        )
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        test_doc_path = KNOWLEDGE_BASE_ROOT / "test_docs" / "test.txt"
        test_doc_path.parent.mkdir(exist_ok=True)
        test_doc_path.write_text(
            "è¿™æ˜¯é‡æ„ç‰ˆæœ¬çš„æµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯é‡æ„æ•ˆæœã€‚"
            "åŒ…å«ä¸€äº›ç¤ºä¾‹å†…å®¹å’Œæµ‹è¯•æ•°æ®ã€‚"
            "æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æ··åˆæœç´¢ã€‚",
            encoding='utf-8'
        )
        
        # æ·»åŠ æ–‡æ¡£
        result = manager.add_documents_from_path(
            path=str(test_doc_path.parent),
            group_id=group_id
        )
        
        print("æ·»åŠ ç»“æœ:", result)
        
        # è®¾ç½®å…³é”®è¯æœç´¢ç­–ç•¥
        manager.set_search_strategy(KeywordSearchStrategy())
        
        # æœç´¢æµ‹è¯•
        search_results = await manager.search("æµ‹è¯•", top_k=5)
        
        print("\næœç´¢ç»“æœ:")
        for result in search_results:
            print(f"- {result.doc_id}: {result.score:.3f}")
            print(f"  å†…å®¹: {result.content[:100]}...")
            if result.highlights:
                print(f"  é«˜äº®: {result.highlights}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_knowledge_base_stats()
        print("\nç»Ÿè®¡ä¿¡æ¯:")
        print(json.dumps(stats, indent=2, ensure_ascii=False))
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(test_refactored_knowledge_base())