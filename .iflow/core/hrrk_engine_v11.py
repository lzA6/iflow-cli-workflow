#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” æ··åˆæ£€ç´¢ä¸é‡æ’åºå†…æ ¸ V11 (ä»£å·ï¼š"æ´å¯Ÿè€…")
===========================================================

è¿™æ˜¯ T-MIA æ¶æ„ä¸‹çš„æ ¸å¿ƒæ£€ç´¢å¼•æ“ï¼Œå®ç°äº†å¯†é›†å‘é‡æœç´¢ã€ç¨€ç–æ£€ç´¢å’ŒçŸ¥è¯†å›¾è°±çš„èåˆã€‚
V11ç‰ˆæœ¬åœ¨V10åŸºç¡€ä¸Šå…¨é¢é‡æ„ï¼Œå®ç°äº†è‡ªé€‚åº”åˆ‡åˆ†ã€å¤šæ¨¡æ€åµŒå…¥å’ŒåŠ¨æ€é‡åŒ–å‹ç¼©ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
- æ··åˆæ£€ç´¢ - èåˆå‘é‡ã€ç¨€ç–å’ŒçŸ¥è¯†å›¾è°±æ£€ç´¢
- æ™ºèƒ½é‡æ’åº - ä½¿ç”¨å…ˆè¿›æ¨¡å‹è¿›è¡ŒäºŒæ¬¡æ’åº
- è‡ªé€‚åº”åˆ‡åˆ† - æ ¹æ®æ–‡æ¡£ç±»å‹åŠ¨æ€è°ƒæ•´å—å¤§å°
- å¤šæ¨¡æ€åµŒå…¥ - ç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€ä»£ç å’Œå›¾è¡¨
- åŠ¨æ€é‡åŒ– - è‡ªé€‚åº”å‹ç¼©ä»¥ä¼˜åŒ–å­˜å‚¨å’Œå¬å›

ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚

ä½œè€…: AIæ¶æ„å¸ˆå›¢é˜Ÿ
ç‰ˆæœ¬: 11.0.0 (ä»£å·ï¼š"æ´å¯Ÿè€…")
æ—¥æœŸ: 2025-11-15
"""

import os
import sys
import json
import asyncio
import logging
import time
import uuid
import hashlib
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union, Set
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque
from enum import Enum
import pickle
import re
import math
from concurrent.futures import ThreadPoolExecutor

# é¡¹ç›®æ ¹è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("HRREngineV11")

class RetrievalMode(Enum):
    """æ£€ç´¢æ¨¡å¼"""
    DENSE = "dense"  # å¯†é›†å‘é‡æ£€ç´¢
    SPARSE = "sparse"  # ç¨€ç–æ£€ç´¢
    HYBRID = "hybrid"  # æ··åˆæ£€ç´¢
    GRAPH = "graph"  # çŸ¥è¯†å›¾è°±æ£€ç´¢
    MULTI_MODAL = "multi_modal"  # å¤šæ¨¡æ€æ£€ç´¢

class DocumentType(Enum):
    """æ–‡æ¡£ç±»å‹"""
    TEXT = "text"
    CODE = "code"
    MARKDOWN = "markdown"
    JSON = "json"
    YAML = "yaml"
    IMAGE = "image"
    DIAGRAM = "diagram"

@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—"""
    chunk_id: str
    document_id: str
    content: str
    content_type: DocumentType
    chunk_index: int
    total_chunks: int
    metadata: Dict[str, Any] = field(default_factory=dict)
    embedding: Optional[np.ndarray] = None
    sparse_vector: Optional[Dict[str, float]] = None
    entities: List[Dict[str, Any]] = field(default_factory=list)
    creation_time: datetime = field(default_factory=datetime.now)
    last_accessed: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    size_bytes: int = 0

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    chunk_id: str
    document_id: str
    content: str
    score: float
    retrieval_method: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    explanation: Optional[str] = None

@dataclass
class KnowledgeTriple:
    """çŸ¥è¯†ä¸‰å…ƒç»„"""
    subject: str
    predicate: str
    object: str
    confidence: float
    source: str
    metadata: Dict[str, Any] = field(default_factory=dict)

class HRREngineV11:
    """æ··åˆæ£€ç´¢ä¸é‡æ’åºå¼•æ“ V11"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        
        # æ–‡æ¡£å­˜å‚¨
        self.documents: Dict[str, DocumentChunk] = {}
        self.document_index: Dict[str, List[str]] = defaultdict(list)  # document_id -> chunk_ids
        
        # å‘é‡å­˜å‚¨
        self.dense_embeddings: Dict[str, np.ndarray] = {}
        self.sparse_vectors: Dict[str, Dict[str, float]] = {}
        
        # çŸ¥è¯†å›¾è°±
        self.knowledge_graph = nx.MultiDiGraph()
        self.entity_index: Dict[str, List[str]] = defaultdict(list)
        
        # æ£€ç´¢ç¼“å­˜
        self.retrieval_cache: Dict[str, List[RetrievalResult]] = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿ
        
        # æ€§èƒ½ä¼˜åŒ–
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.embedding_cache = {}
        
        # é…ç½®å‚æ•°
        self.chunk_size_limits = {
            DocumentType.TEXT: (100, 500),
            DocumentType.CODE: (50, 300),
            DocumentType.MARKDOWN: (200, 800),
            DocumentType.JSON: (100, 400),
            DocumentType.YAML: (100, 400)
        }
        
        # RRFå‚æ•°
        self.rrf_k = 60  # Reciprocal Rank Fusionå‚æ•°
        
        logger.info("HRRKå¼•æ“V11åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–"""
        logger.info("æ­£åœ¨åˆå§‹åŒ–HRRKå¼•æ“...")
        
        # åŠ è½½ç°æœ‰æ–‡æ¡£
        await self._load_existing_documents()
        
        # æ„å»ºçŸ¥è¯†å›¾è°±
        await self._build_knowledge_graph()
        
        # é¢„çƒ­åµŒå…¥æ¨¡å‹
        await self._warmup_embedding_models()
        
        # å¯åŠ¨ç»´æŠ¤ä»»åŠ¡
        asyncio.create_task(self._cache_cleanup_loop())
        asyncio.create_task(self._index_optimization_loop())
        
        logger.info("HRRKå¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    async def add_document(self, 
                         document_id: str,
                         content: str,
                         content_type: DocumentType = DocumentType.TEXT,
                         metadata: Optional[Dict[str, Any]] = None) -> List[str]:
        """æ·»åŠ æ–‡æ¡£"""
        try:
            # è‡ªé€‚åº”åˆ‡åˆ†
            chunks = await self._adaptive_chunking(document_id, content, content_type)
            
            chunk_ids = []
            for chunk in chunks:
                # ç”ŸæˆåµŒå…¥
                await self._generate_embeddings(chunk)
                
                # æå–å®ä½“
                await self._extract_entities(chunk)
                
                # å­˜å‚¨
                self.documents[chunk.chunk_id] = chunk
                self.document_index[document_id].append(chunk.chunk_id)
                
                # æ›´æ–°ç´¢å¼•
                if chunk.embedding is not None:
                    self.dense_embeddings[chunk.chunk_id] = chunk.embedding
                
                if chunk.sparse_vector is not None:
                    self.sparse_vectors[chunk.chunk_id] = chunk.sparse_vector
                
                chunk_ids.append(chunk.chunk_id)
            
            logger.info(f"æ·»åŠ æ–‡æ¡£æˆåŠŸ: {document_id}, ç”Ÿæˆ {len(chunk_ids)} ä¸ªå—")
            return chunk_ids
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥ {document_id}: {e}")
            return []
    
    async def retrieve(self, 
                      query: str,
                      mode: RetrievalMode = RetrievalMode.HYBRID,
                      top_k: int = 10,
                      filters: Optional[Dict[str, Any]] = None) -> List[RetrievalResult]:
        """æ£€ç´¢æ–‡æ¡£"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(query, mode, top_k, filters)
        if cache_key in self.retrieval_cache:
            cached_results = self.retrieval_cache[cache_key]
            # æ›´æ–°è®¿é—®ç»Ÿè®¡
            for result in cached_results:
                if result.chunk_id in self.documents:
                    chunk = self.documents[result.chunk_id]
                    chunk.last_accessed = datetime.now()
                    chunk.access_count += 1
            return cached_results
        
        results = []
        
        if mode == RetrievalMode.DENSE:
            results = await self._dense_retrieval(query, top_k, filters)
        elif mode == RetrievalMode.SPARSE:
            results = await self._sparse_retrieval(query, top_k, filters)
        elif mode == RetrievalMode.HYBRID:
            results = await self._hybrid_retrieval(query, top_k, filters)
        elif mode == RetrievalMode.GRAPH:
            results = await self._graph_retrieval(query, top_k, filters)
        elif mode == RetrievalMode.MULTI_MODAL:
            results = await self._multi_modal_retrieval(query, top_k, filters)
        
        # é‡æ’åº
        if len(results) > 1:
            results = await self._rerank_results(query, results)
        
        # ç¼“å­˜ç»“æœ
        self.retrieval_cache[cache_key] = results
        
        return results[:top_k]
    
    async def _adaptive_chunking(self, 
                               document_id: str,
                               content: str,
                               content_type: DocumentType) -> List[DocumentChunk]:
        """è‡ªé€‚åº”åˆ‡åˆ†"""
        # è·å–ç±»å‹ç‰¹å®šçš„åˆ‡åˆ†å‚æ•°
        min_size, max_size = self.chunk_size_limits.get(content_type, (100, 500))
        
        # æ ¹æ®å†…å®¹ç‰¹å¾è°ƒæ•´
        content_features = await self._analyze_content_features(content, content_type)
        
        if content_features['has_code_blocks']:
            min_size = max(min_size, 50)
            max_size = min(max_size, 300)
        
        if content_features['has_complex_structure']:
            max_size = min(max_size, 400)
        
        # æ‰§è¡Œåˆ‡åˆ†
        if content_type == DocumentType.CODE:
            chunks = await self._chunk_code(content, document_id, min_size, max_size)
        elif content_type == DocumentType.MARKDOWN:
            chunks = await self._chunk_markdown(content, document_id, min_size, max_size)
        else:
            chunks = await self._chunk_text(content, document_id, min_size, max_size)
        
        # çŸ¥è¯†å›¾è°±æ„ŸçŸ¥åˆ‡åˆ†
        chunks = await self._kg_aware_chunking(chunks)
        
        return chunks
    
    async def _analyze_content_features(self, content: str, content_type: DocumentType) -> Dict[str, Any]:
        """åˆ†æå†…å®¹ç‰¹å¾"""
        features = {
            'has_code_blocks': False,
            'has_complex_structure': False,
            'avg_sentence_length': 0,
            'entity_density': 0
        }
        
        # æ£€æµ‹ä»£ç å—
        if '```' in content or content_type == DocumentType.CODE:
            features['has_code_blocks'] = True
        
        # æ£€æµ‹å¤æ‚ç»“æ„
        if content_type in [DocumentType.JSON, DocumentType.YAML]:
            features['has_complex_structure'] = True
        
        # è®¡ç®—å¹³å‡å¥å­é•¿åº¦
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        if sentences:
            features['avg_sentence_length'] = sum(len(s.split()) for s in sentences) / len(sentences)
        
        return features
    
    async def _chunk_text(self, 
                         content: str,
                         document_id: str,
                         min_size: int,
                         max_size: int) -> List[DocumentChunk]:
        """æ–‡æœ¬åˆ‡åˆ†"""
        chunks = []
        
        # æŒ‰æ®µè½åˆ‡åˆ†
        paragraphs = content.split('\n\n')
        current_chunk = ""
        chunk_index = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°å—
            if len(current_chunk) + len(paragraph) > max_size and current_chunk:
                # åˆ›å»ºå—
                chunk = DocumentChunk(
                    chunk_id=f"{document_id}_chunk_{chunk_index}",
                    document_id=document_id,
                    content=current_chunk.strip(),
                    content_type=DocumentType.TEXT,
                    chunk_index=chunk_index,
                    total_chunks=0,  # ç¨åæ›´æ–°
                    size_bytes=len(current_chunk.encode('utf-8'))
                )
                chunks.append(chunk)
                chunk_index += 1
                current_chunk = paragraph
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
        
        # å¤„ç†æœ€åä¸€å—
        if current_chunk.strip():
            chunk = DocumentChunk(
                chunk_id=f"{document_id}_chunk_{chunk_index}",
                document_id=document_id,
                content=current_chunk.strip(),
                content_type=DocumentType.TEXT,
                chunk_index=chunk_index,
                total_chunks=0,
                size_bytes=len(current_chunk.encode('utf-8'))
            )
            chunks.append(chunk)
        
        # æ›´æ–°æ€»å—æ•°
        total_chunks = len(chunks)
        for chunk in chunks:
            chunk.total_chunks = total_chunks
        
        return chunks
    
    async def _chunk_code(self, 
                         content: str,
                         document_id: str,
                         min_size: int,
                         max_size: int) -> List[DocumentChunk]:
        """ä»£ç åˆ‡åˆ†"""
        chunks = []
        
        # æŒ‰å‡½æ•°/ç±»åˆ‡åˆ†
        functions = re.finditer(r'\n(def|class)\s+(\w+)', content)
        
        positions = [0]
        for match in functions:
            positions.append(match.start())
        positions.append(len(content))
        
        for i in range(len(positions) - 1):
            start = positions[i]
            end = positions[i + 1]
            chunk_content = content[start:end].strip()
            
            if chunk_content:
                chunk = DocumentChunk(
                    chunk_id=f"{document_id}_code_chunk_{i}",
                    document_id=document_id,
                    content=chunk_content,
                    content_type=DocumentType.CODE,
                    chunk_index=i,
                    total_chunks=0,
                    size_bytes=len(chunk_content.encode('utf-8'))
                )
                chunks.append(chunk)
        
        # æ›´æ–°æ€»å—æ•°
        total_chunks = len(chunks)
        for chunk in chunks:
            chunk.total_chunks = total_chunks
        
        return chunks
    
    async def _chunk_markdown(self, 
                             content: str,
                             document_id: str,
                             min_size: int,
                             max_size: int) -> List[DocumentChunk]:
        """Markdownåˆ‡åˆ†"""
        chunks = []
        
        # æŒ‰æ ‡é¢˜åˆ‡åˆ†
        headers = re.finditer(r'^(#{1,6})\s+(.+)$', content, re.MULTILINE)
        
        positions = [0]
        for match in headers:
            positions.append(match.start())
        positions.append(len(content))
        
        for i in range(len(positions) - 1):
            start = positions[i]
            end = positions[i + 1]
            chunk_content = content[start:end].strip()
            
            if chunk_content:
                chunk = DocumentChunk(
                    chunk_id=f"{document_id}_md_chunk_{i}",
                    document_id=document_id,
                    content=chunk_content,
                    content_type=DocumentType.MARKDOWN,
                    chunk_index=i,
                    total_chunks=0,
                    size_bytes=len(chunk_content.encode('utf-8'))
                )
                chunks.append(chunk)
        
        # æ›´æ–°æ€»å—æ•°
        total_chunks = len(chunks)
        for chunk in chunks:
            chunk.total_chunks = total_chunks
        
        return chunks
    
    async def _kg_aware_chunking(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """çŸ¥è¯†å›¾è°±æ„ŸçŸ¥åˆ‡åˆ†"""
        for chunk in chunks:
            # è¯†åˆ«å®ä½“
            entities = await self._extract_entities_from_text(chunk.content)
            
            # ç¡®ä¿å®ä½“ä¸è¢«åˆ†å‰²
            if entities:
                # æ£€æŸ¥æ˜¯å¦æœ‰å®ä½“è¢«æˆªæ–­
                chunk.entities = entities
        
        return chunks
    
    async def _extract_entities(self, chunk: DocumentChunk):
        """æå–å®ä½“"""
        entities = await self._extract_entities_from_text(chunk.content)
        chunk.entities = entities
        
        # æ›´æ–°å®ä½“ç´¢å¼•
        for entity in entities:
            entity_name = entity.get('name', '')
            if entity_name:
                self.entity_index[entity_name].append(chunk.chunk_id)
    
    async def _extract_entities_from_text(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        entities = []
        
        # ç®€å•çš„å®ä½“è¯†åˆ«ï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨æ›´å¤æ‚çš„NLPæ¨¡å‹ï¼‰
        # è¯†åˆ«ä¸“æœ‰åè¯ï¼ˆå¤§å†™å¼€å¤´çš„è¯ï¼‰
        proper_nouns = re.findall(r'\b[A-Z][a-zA-Z]+\b', text)
        
        for noun in set(proper_nouns):
            if len(noun) > 2:  # è¿‡æ»¤çŸ­è¯
                entities.append({
                    'name': noun,
                    'type': 'proper_noun',
                    'confidence': 0.7,
                    'positions': [m.start() for m in re.finditer(rf'\b{re.escape(noun)}\b', text)]
                })
        
        # è¯†åˆ«ä»£ç ç›¸å…³çš„å®ä½“
        code_patterns = [
            (r'\b(def|class|function)\s+(\w+)', 'function'),
            (r'\b(import|from)\s+(\w+)', 'module'),
            (r'\b(\w+)\s*\(', 'function_call')
        ]
        
        for pattern, entity_type in code_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                entity_name = match.group(2) if match.lastindex >= 2 else match.group(1)
                entities.append({
                    'name': entity_name,
                    'type': entity_type,
                    'confidence': 0.8,
                    'positions': [match.start()]
                })
        
        return entities
    
    async def _generate_embeddings(self, chunk: DocumentChunk):
        """ç”ŸæˆåµŒå…¥å‘é‡"""
        # å¯†é›†å‘é‡åµŒå…¥ï¼ˆç®€åŒ–å®ç°ï¼‰
        content_hash = hashlib.md5(chunk.content.encode()).hexdigest()
        
        if content_hash in self.embedding_cache:
            chunk.embedding = self.embedding_cache[content_hash]
        else:
            # æ¨¡æ‹ŸåµŒå…¥ç”Ÿæˆï¼ˆå®é™…åº”ä½¿ç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹ï¼‰
            embedding = np.random.rand(768)  # å‡è®¾768ç»´åµŒå…¥
            embedding = embedding / np.linalg.norm(embedding)  # å½’ä¸€åŒ–
            
            chunk.embedding = embedding
            self.embedding_cache[content_hash] = embedding
        
        # ç¨€ç–å‘é‡ï¼ˆTF-IDFç®€åŒ–ç‰ˆï¼‰
        words = re.findall(r'\b\w+\b', chunk.content.lower())
        word_counts = defaultdict(int)
        for word in words:
            word_counts[word] += 1
        
        # è®¡ç®—TF-IDFï¼ˆç®€åŒ–ç‰ˆï¼‰
        total_words = sum(word_counts.values())
        sparse_vector = {}
        for word, count in word_counts.items():
            tf = count / total_words
            sparse_vector[word] = tf
        
        chunk.sparse_vector = sparse_vector
    
    async def _dense_retrieval(self, 
                             query: str,
                             top_k: int,
                             filters: Optional[Dict[str, Any]]) -> List[RetrievalResult]:
        """å¯†é›†å‘é‡æ£€ç´¢"""
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = await self._generate_query_embedding(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for chunk_id, embedding in self.dense_embeddings.items():
            if chunk_id not in self.documents:
                continue
            
            # åº”ç”¨è¿‡æ»¤å™¨
            chunk = self.documents[chunk_id]
            if not self._passes_filters(chunk, filters):
                continue
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = np.dot(query_embedding, embedding)
            similarities.append((chunk_id, similarity))
        
        # æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # ç”Ÿæˆç»“æœ
        results = []
        for chunk_id, similarity in similarities[:top_k]:
            chunk = self.documents[chunk_id]
            result = RetrievalResult(
                chunk_id=chunk_id,
                document_id=chunk.document_id,
                content=chunk.content,
                score=float(similarity),
                retrieval_method="dense_vector",
                metadata=chunk.metadata,
                explanation=f"å‘é‡ç›¸ä¼¼åº¦: {similarity:.3f}"
            )
            results.append(result)
        
        return results
    
    async def _sparse_retrieval(self, 
                              query: str,
                              top_k: int,
                              filters: Optional[Dict[str, Any]]) -> List[RetrievalResult]:
        """ç¨€ç–æ£€ç´¢"""
        # å¤„ç†æŸ¥è¯¢
        query_words = re.findall(r'\b\w+\b', query.lower())
        query_vector = defaultdict(int)
        for word in query_words:
            query_vector[word] += 1
        
        # è®¡ç®—BM25åˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        scores = []
        for chunk_id, sparse_vector in self.sparse_vectors.items():
            if chunk_id not in self.documents:
                continue
            
            # åº”ç”¨è¿‡æ»¤å™¨
            chunk = self.documents[chunk_id]
            if not self._passes_filters(chunk, filters):
                continue
            
            # è®¡ç®—BM25åˆ†æ•°
            score = 0.0
            for word, qf in query_vector.items():
                if word in sparse_vector:
                    df = sum(1 for sv in self.sparse_vectors.values() if word in sv)
                    idf = math.log((len(self.sparse_vectors) - df + 0.5) / (df + 0.5))
                    tf = sparse_vector[word]
                    score += tf * idf * qf
            
            scores.append((chunk_id, score))
        
        # æ’åº
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # ç”Ÿæˆç»“æœ
        results = []
        for chunk_id, score in scores[:top_k]:
            chunk = self.documents[chunk_id]
            result = RetrievalResult(
                chunk_id=chunk_id,
                document_id=chunk.document_id,
                content=chunk.content,
                score=score,
                retrieval_method="sparse_bm25",
                metadata=chunk.metadata,
                explanation=f"BM25åˆ†æ•°: {score:.3f}"
            )
            results.append(result)
        
        return results
    
    async def _hybrid_retrieval(self, 
                              query: str,
                              top_k: int,
                              filters: Optional[Dict[str, Any]]) -> List[RetrievalResult]:
        """æ··åˆæ£€ç´¢"""
        # å¹¶è¡Œæ‰§è¡Œå¯†é›†å’Œç¨€ç–æ£€ç´¢
        dense_task = self._dense_retrieval(query, top_k * 2, filters)
        sparse_task = self._sparse_retrieval(query, top_k * 2, filters)
        
        dense_results, sparse_results = await asyncio.gather(dense_task, sparse_task)
        
        # RRFèåˆ
        fused_results = self._reciprocal_rank_fusion(dense_results, sparse_results)
        
        return fused_results[:top_k]
    
    async def _graph_retrieval(self, 
                             query: str,
                             top_k: int,
                             filters: Optional[Dict[str, Any]]) -> List[RetrievalResult]:
        """çŸ¥è¯†å›¾è°±æ£€ç´¢"""
        # è¯†åˆ«æŸ¥è¯¢ä¸­çš„å®ä½“
        query_entities = await self._extract_entities_from_text(query)
        entity_names = [e['name'] for e in query_entities]
        
        # åœ¨çŸ¥è¯†å›¾è°±ä¸­æŸ¥æ‰¾ç›¸å…³å®ä½“
        related_chunks = set()
        for entity_name in entity_names:
            if entity_name in self.entity_index:
                related_chunks.update(self.entity_index[entity_name])
        
        # æ‰©å±•åˆ°ç›¸å…³å®ä½“
        expanded_chunks = set(related_chunks)
        for chunk_id in related_chunks:
            if chunk_id in self.documents:
                chunk = self.documents[chunk_id]
                for entity in chunk.entities:
                    entity_name = entity.get('name', '')
                    if entity_name and entity_name in self.entity_index:
                        expanded_chunks.update(self.entity_index[entity_name])
        
        # ç”Ÿæˆç»“æœ
        results = []
        for chunk_id in expanded_chunks:
            if chunk_id not in self.documents:
                continue
            
            chunk = self.documents[chunk_id]
            if not self._passes_filters(chunk, filters):
                continue
            
            # è®¡ç®—å›¾è°±ç›¸å…³æ€§åˆ†æ•°
            score = await self._calculate_graph_relevance(query, chunk, query_entities)
            
            result = RetrievalResult(
                chunk_id=chunk_id,
                document_id=chunk.document_id,
                content=chunk.content,
                score=score,
                retrieval_method="knowledge_graph",
                metadata=chunk.metadata,
                explanation=f"å›¾è°±ç›¸å…³æ€§: {score:.3f}"
            )
            results.append(result)
        
        # æ’åº
        results.sort(key=lambda x: x.score, reverse=True)
        
        return results[:top_k]
    
    async def _multi_modal_retrieval(self, 
                                   query: str,
                                   top_k: int,
                                   filters: Optional[Dict[str, Any]]) -> List[RetrievalResult]:
        """å¤šæ¨¡æ€æ£€ç´¢"""
        # è¯†åˆ«æŸ¥è¯¢ç±»å‹
        query_type = await self._classify_query_type(query)
        
        # æ ¹æ®ç±»å‹é€‰æ‹©æ£€ç´¢ç­–ç•¥
        if query_type == 'code':
            # ä¼˜å…ˆæ£€ç´¢ä»£ç æ–‡æ¡£
            code_filters = filters or {}
            code_filters['content_type'] = DocumentType.CODE
            results = await self._hybrid_retrieval(query, top_k, code_filters)
        elif query_type == 'visual':
            # ä¼˜å…ˆæ£€ç´¢å›¾è¡¨ç›¸å…³å†…å®¹
            visual_filters = filters or {}
            visual_filters['has_diagrams'] = True
            results = await self._hybrid_retrieval(query, top_k, visual_filters)
        else:
            # æ ‡å‡†æ··åˆæ£€ç´¢
            results = await self._hybrid_retrieval(query, top_k, filters)
        
        return results
    
    async def _rerank_results(self, query: str, results: List[RetrievalResult]) -> List[RetrievalResult]:
        """é‡æ’åºç»“æœ"""
        if len(results) <= 1:
            return results
        
        # è®¡ç®—é‡æ’åºåˆ†æ•°
        reranked = []
        for result in results:
            # å¤šå› ç´ è¯„åˆ†
            original_score = result.score
            freshness_score = await self._calculate_freshness_score(result.chunk_id)
            diversity_score = await self._calculate_diversity_score(result, reranked)
            authority_score = await self._calculate_authority_score(result.document_id)
            
            # ç»„åˆåˆ†æ•°
            final_score = (
                original_score * 0.5 +
                freshness_score * 0.2 +
                diversity_score * 0.2 +
                authority_score * 0.1
            )
            
            result.score = final_score
            reranked.append(result)
        
        # é‡æ–°æ’åº
        reranked.sort(key=lambda x: x.score, reverse=True)
        
        return reranked
    
    def _reciprocal_rank_fusion(self, 
                               dense_results: List[RetrievalResult],
                               sparse_results: List[RetrievalResult]) -> List[RetrievalResult]:
        """RRFèåˆ"""
        fused_scores = defaultdict(float)
        result_map = {}
        
        # å¤„ç†å¯†é›†æ£€ç´¢ç»“æœ
        for rank, result in enumerate(dense_results):
            score = 1.0 / (self.rrf_k + rank + 1)
            fused_scores[result.chunk_id] += score
            result_map[result.chunk_id] = result
        
        # å¤„ç†ç¨€ç–æ£€ç´¢ç»“æœ
        for rank, result in enumerate(sparse_results):
            score = 1.0 / (self.rrf_k + rank + 1)
            fused_scores[result.chunk_id] += score
            if result.chunk_id not in result_map:
                result_map[result.chunk_id] = result
        
        # ç”Ÿæˆèåˆç»“æœ
        fused_results = []
        for chunk_id, score in fused_scores.items():
            result = result_map[chunk_id]
            result.score = score
            result.retrieval_method = "rrf_fusion"
            fused_results.append(result)
        
        # æ’åº
        fused_results.sort(key=lambda x: x.score, reverse=True)
        
        return fused_results
    
    async def _generate_query_embedding(self, query: str) -> np.ndarray:
        """ç”ŸæˆæŸ¥è¯¢åµŒå…¥"""
        # ç®€åŒ–å®ç°ï¼ˆå®é™…åº”ä½¿ç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹ï¼‰
        query_hash = hashlib.md5(query.encode()).hexdigest()
        
        if query_hash in self.embedding_cache:
            return self.embedding_cache[query_hash]
        
        # æ¨¡æ‹ŸåµŒå…¥ç”Ÿæˆ
        embedding = np.random.rand(768)
        embedding = embedding / np.linalg.norm(embedding)
        
        self.embedding_cache[query_hash] = embedding
        
        return embedding
    
    def _passes_filters(self, chunk: DocumentChunk, filters: Optional[Dict[str, Any]]) -> bool:
        """æ£€æŸ¥æ˜¯å¦é€šè¿‡è¿‡æ»¤å™¨"""
        if not filters:
            return True
        
        # å†…å®¹ç±»å‹è¿‡æ»¤
        if 'content_type' in filters:
            if chunk.content_type != filters['content_type']:
                return False
        
        # æ–‡æ¡£IDè¿‡æ»¤
        if 'document_id' in filters:
            if chunk.document_id != filters['document_id']:
                return False
        
        # å¤§å°è¿‡æ»¤
        if 'min_size' in filters:
            if chunk.size_bytes < filters['min_size']:
                return False
        
        if 'max_size' in filters:
            if chunk.size_bytes > filters['max_size']:
                return False
        
        return True
    
    async def _classify_query_type(self, query: str) -> str:
        """åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        # ç®€å•çš„æŸ¥è¯¢åˆ†ç±»
        code_keywords = ['function', 'class', 'def', 'import', 'code', 'algorithm']
        visual_keywords = ['diagram', 'chart', 'graph', 'image', 'figure', 'visual']
        
        query_lower = query.lower()
        
        code_score = sum(1 for keyword in code_keywords if keyword in query_lower)
        visual_score = sum(1 for keyword in visual_keywords if keyword in query_lower)
        
        if code_score > visual_score:
            return 'code'
        elif visual_score > 0:
            return 'visual'
        else:
            return 'text'
    
    async def _calculate_freshness_score(self, chunk_id: str) -> float:
        """è®¡ç®—æ–°é²œåº¦åˆ†æ•°"""
        if chunk_id not in self.documents:
            return 0.0
        
        chunk = self.documents[chunk_id]
        now = datetime.now()
        age_hours = (now - chunk.creation_time).total_seconds() / 3600
        
        # è¶Šæ–°åˆ†æ•°è¶Šé«˜
        freshness = math.exp(-age_hours / 24)  # 24å°æ—¶åŠè¡°æœŸ
        
        return freshness
    
    async def _calculate_diversity_score(self, 
                                       result: RetrievalResult,
                                       existing_results: List[RetrievalResult]) -> float:
        """è®¡ç®—å¤šæ ·æ€§åˆ†æ•°"""
        if not existing_results:
            return 1.0
        
        # è®¡ç®—ä¸å·²æœ‰ç»“æœçš„æ–‡æ¡£å·®å¼‚
        existing_docs = {r.document_id for r in existing_results}
        
        if result.document_id not in existing_docs:
            return 1.0
        else:
            # åŒä¸€æ–‡æ¡£çš„ä¸åŒå—ï¼Œç»™äºˆè¾ƒä½çš„å¤šæ ·æ€§åˆ†æ•°
            return 0.5
    
    async def _calculate_authority_score(self, document_id: str) -> float:
        """è®¡ç®—æƒå¨åˆ†æ•°"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºæ–‡æ¡£çš„è®¿é—®æ¬¡æ•°
        total_access = 0
        chunk_count = 0
        
        for chunk_id in self.document_index.get(document_id, []):
            if chunk_id in self.documents:
                chunk = self.documents[chunk_id]
                total_access += chunk.access_count
                chunk_count += 1
        
        if chunk_count == 0:
            return 0.5
        
        avg_access = total_access / chunk_count
        
        # å½’ä¸€åŒ–åˆ†æ•°
        authority = min(1.0, avg_access / 10.0)
        
        return authority
    
    async def _calculate_graph_relevance(self, 
                                        query: str,
                                        chunk: DocumentChunk,
                                        query_entities: List[Dict[str, Any]]) -> float:
        """è®¡ç®—å›¾è°±ç›¸å…³æ€§"""
        if not query_entities or not chunk.entities:
            return 0.0
        
        # è®¡ç®—å®ä½“é‡å 
        query_entity_names = {e['name'] for e in query_entities}
        chunk_entity_names = {e['name'] for e in chunk.entities}
        
        overlap = len(query_entity_names & chunk_entity_names)
        union = len(query_entity_names | chunk_entity_names)
        
        if union == 0:
            return 0.0
        
        jaccard_similarity = overlap / union
        
        return jaccard_similarity
    
    def _generate_cache_key(self, 
                           query: str,
                           mode: RetrievalMode,
                           top_k: int,
                           filters: Optional[Dict[str, Any]]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        filter_str = json.dumps(filters, sort_keys=True) if filters else ""
        cache_data = f"{query}_{mode.value}_{top_k}_{filter_str}"
        return hashlib.md5(cache_data.encode()).hexdigest()
    
    async def _load_existing_documents(self):
        """åŠ è½½ç°æœ‰æ–‡æ¡£"""
        # è¿™é‡Œå¯ä»¥å®ç°ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½æ–‡æ¡£
        pass
    
    async def _build_knowledge_graph(self):
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        # åŸºäºå®ä½“å…³ç³»æ„å»ºå›¾è°±
        for chunk_id, chunk in self.documents.items():
            for entity in chunk.entities:
                entity_name = entity.get('name', '')
                if not entity_name:
                    continue
                
                # æ·»åŠ å®ä½“èŠ‚ç‚¹
                if not self.knowledge_graph.has_node(entity_name):
                    self.knowledge_graph.add_node(
                        entity_name,
                        type=entity.get('type', 'unknown'),
                        chunk_ids=[]
                    )
                
                # å…³è”å—ID
                self.knowledge_graph.nodes[entity_name]['chunk_ids'].append(chunk_id)
        
        logger.info(f"æ„å»ºçŸ¥è¯†å›¾è°±å®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {self.knowledge_graph.number_of_nodes()}")
    
    async def _warmup_embedding_models(self):
        """é¢„çƒ­åµŒå…¥æ¨¡å‹"""
        # é¢„ç”Ÿæˆä¸€äº›å¸¸ç”¨åµŒå…¥
        common_queries = [
            "what is",
            "how to",
            "example of",
            "definition",
            "implementation"
        ]
        
        for query in common_queries:
            await self._generate_query_embedding(query)
        
        logger.info("åµŒå…¥æ¨¡å‹é¢„çƒ­å®Œæˆ")
    
    async def _cache_cleanup_loop(self):
        """ç¼“å­˜æ¸…ç†å¾ªç¯"""
        while True:
            try:
                await asyncio.sleep(300)  # 5åˆ†é’Ÿ
                
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                current_time = time.time()
                expired_keys = [
                    key for key in self.retrieval_cache.keys()
                    if current_time - hash(key) > self.cache_ttl
                ]
                
                for key in expired_keys:
                    del self.retrieval_cache[key]
                
                if expired_keys:
                    logger.debug(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜é¡¹")
                
            except Exception as e:
                logger.error(f"ç¼“å­˜æ¸…ç†é”™è¯¯: {e}")
    
    async def _index_optimization_loop(self):
        """ç´¢å¼•ä¼˜åŒ–å¾ªç¯"""
        while True:
            try:
                await asyncio.sleep(3600)  # 1å°æ—¶
                
                # æ¸…ç†æœªä½¿ç”¨çš„åµŒå…¥
                await self._cleanup_unused_embeddings()
                
                # ä¼˜åŒ–ç¨€ç–å‘é‡
                await self._optimize_sparse_vectors()
                
            except Exception as e:
                logger.error(f"ç´¢å¼•ä¼˜åŒ–é”™è¯¯: {e}")
    
    async def _cleanup_unused_embeddings(self):
        """æ¸…ç†æœªä½¿ç”¨çš„åµŒå…¥"""
        # è¯†åˆ«æ´»è·ƒçš„åµŒå…¥
        active_chunk_ids = set(self.documents.keys())
        
        # æ¸…ç†æœªä½¿ç”¨çš„åµŒå…¥
        unused_dense = [
            chunk_id for chunk_id in self.dense_embeddings.keys()
            if chunk_id not in active_chunk_ids
        ]
        
        unused_sparse = [
            chunk_id for chunk_id in self.sparse_vectors.keys()
            if chunk_id not in active_chunk_ids
        ]
        
        for chunk_id in unused_dense:
            del self.dense_embeddings[chunk_id]
        
        for chunk_id in unused_sparse:
            del self.sparse_vectors[chunk_id]
        
        if unused_dense or unused_sparse:
            logger.info(f"æ¸…ç†äº† {len(unused_dense)} ä¸ªå¯†é›†å‘é‡å’Œ {len(unused_sparse)} ä¸ªç¨€ç–å‘é‡")
    
    async def _optimize_sparse_vectors(self):
        """ä¼˜åŒ–ç¨€ç–å‘é‡"""
        # ç§»é™¤ä½é¢‘è¯
        word_frequency = defaultdict(int)
        for sparse_vector in self.sparse_vectors.values():
            for word in sparse_vector:
                word_frequency[word] += 1
        
        # ç§»é™¤å‡ºç°æ¬¡æ•°å°‘äº3æ¬¡çš„è¯
        min_frequency = 3
        low_freq_words = {word for word, freq in word_frequency.items() if freq < min_frequency}
        
        for chunk_id, sparse_vector in self.sparse_vectors.items():
            # è¿‡æ»¤ä½é¢‘è¯
            filtered_vector = {
                word: score for word, score in sparse_vector.items()
                if word not in low_freq_words
            }
            
            # é‡æ–°å½’ä¸€åŒ–
            if filtered_vector:
                total_score = sum(filtered_vector.values())
                if total_score > 0:
                    filtered_vector = {
                        word: score / total_score
                        for word, score in filtered_vector.items()
                    }
            
            self.sparse_vectors[chunk_id] = filtered_vector
        
        if low_freq_words:
            logger.info(f"ä¼˜åŒ–ç¨€ç–å‘é‡ï¼Œç§»é™¤äº† {len(low_freq_words)} ä¸ªä½é¢‘è¯")
    
    async def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'total_documents': len(self.document_index),
            'total_chunks': len(self.documents),
            'dense_embeddings': len(self.dense_embeddings),
            'sparse_vectors': len(self.sparse_vectors),
            'knowledge_graph_nodes': self.knowledge_graph.number_of_nodes(),
            'knowledge_graph_edges': self.knowledge_graph.number_of_edges(),
            'cache_size': len(self.retrieval_cache),
            'embedding_cache_size': len(self.embedding_cache)
        }
    
    async def shutdown(self):
        """ä¼˜é›…å…³é—­"""
        logger.info("æ­£åœ¨å…³é—­HRRKå¼•æ“...")
        
        # ä¿å­˜ç´¢å¼•
        await self._save_indices()
        
        # å…³é—­çº¿ç¨‹æ± 
        self.executor.shutdown(wait=True)
        
        logger.info("HRRKå¼•æ“å·²å…³é—­")
    
    async def _save_indices(self):
        """ä¿å­˜ç´¢å¼•"""
        indices_file = PROJECT_ROOT / ".iflow" / "data" / "hrrk_indices_v11.pkl"
        indices_file.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            indices = {
                'documents': {
                    chunk_id: asdict(chunk) for chunk_id, chunk in self.documents.items()
                },
                'document_index': dict(self.document_index),
                'entity_index': dict(self.entity_index),
                'knowledge_graph_edges': list(self.knowledge_graph.edges(data=True))
            }
            
            # å¤„ç†numpyæ•°ç»„
            indices['dense_embeddings'] = {
                chunk_id: embedding.tolist() 
                for chunk_id, embedding in self.dense_embeddings.items()
            }
            
            with open(indices_file, 'wb') as f:
                pickle.dump(indices, f)
            
            logger.info("ç´¢å¼•ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")

# å…¨å±€å®ä¾‹
_hrrk_engine: Optional[HRREngineV11] = None

async def get_hrrk_engine() -> HRREngineV11:
    """è·å–HRRKå¼•æ“å®ä¾‹"""
    global _hrrk_engine
    if _hrrk_engine is None:
        _hrrk_engine = HRREngineV11()
        await _hrrk_engine.initialize()
    return _hrrk_engine

async def add_document(document_id: str,
                     content: str,
                     content_type: DocumentType = DocumentType.TEXT,
                     metadata: Optional[Dict[str, Any]] = None) -> List[str]:
    """æ·»åŠ æ–‡æ¡£çš„ä¾¿æ·å‡½æ•°"""
    engine = await get_hrrk_engine()
    return await engine.add_document(document_id, content, content_type, metadata)

async def retrieve(query: str,
                  mode: RetrievalMode = RetrievalMode.HYBRID,
                  top_k: int = 10,
                  filters: Optional[Dict[str, Any]] = None) -> List[RetrievalResult]:
    """æ£€ç´¢çš„ä¾¿æ·å‡½æ•°"""
    engine = await get_hrrk_engine()
    return await engine.retrieve(query, mode, top_k, filters)