#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ ç»¼åˆè¯„ä¼°ç³»ç»Ÿ V5 (Comprehensive Evaluation System V5)
å¯¹Aé¡¹ç›®è¿›è¡Œå…¨é¢çš„åŠŸèƒ½æµ‹è¯•å’Œæ€§èƒ½è¯„ä¼°ã€‚
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
import psutil
import traceback
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import unittest
from dataclasses import dataclass, field

# åŠ¨æ€æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    name: str
    status: str  # passed, failed, error
    duration: float
    message: str
    details: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    cpu_usage: float
    memory_usage: float
    response_time: float
    throughput: float
    error_rate: float
    timestamp: datetime = field(default_factory=datetime.now)

class ComprehensiveEvaluationV5:
    """
    ç»¼åˆè¯„ä¼°ç³»ç»Ÿ V5
    """
    
    def __init__(self):
        self.test_results: List[TestResult] = []
        self.performance_metrics: List[PerformanceMetrics] = []
        self.start_time = datetime.now()
        
        # æµ‹è¯•ç»„ä»¶
        self.components = {
            "model_adapter": None,
            "consciousness_system": None,
            "arq_engine": None,
            "fusion_agent": None,
            "workflow_engine": None,
            "test_heal_system": None,
            "maintenance_system": None,
            "context_cache": None,
            "hook_integration": None,
            "evolution_engine": None
        }
        
        # æ€§èƒ½åŸºå‡†
        self.benchmarks = {
            "response_time": 1.0,  # ç§’
            "memory_usage": 512,   # MB
            "cpu_usage": 80,       # ç™¾åˆ†æ¯”
            "success_rate": 0.95   # ç™¾åˆ†æ¯”
        }
        
        logger.info("ç»¼åˆè¯„ä¼°ç³»ç»ŸV5åˆå§‹åŒ–å®Œæˆ")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå…¨é¢æµ‹è¯•å’Œæ€§èƒ½è¯„ä¼°...")
        
        # 1. å¯¼å…¥æµ‹è¯•
        await self._test_imports()
        
        # 2. åˆå§‹åŒ–ç»„ä»¶æµ‹è¯•
        await self._test_initialization()
        
        # 3. åŠŸèƒ½æµ‹è¯•
        await self._test_functionality()
        
        # 4. æ€§èƒ½æµ‹è¯•
        await self._test_performance()
        
        # 5. é›†æˆæµ‹è¯•
        await self._test_integration()
        
        # 6. ç¨³å®šæ€§æµ‹è¯•
        await self._test_stability()
        
        # 7. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        report = await self._generate_evaluation_report()
        
        return report
    
    async def _test_imports(self):
        """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
        logger.info("ğŸ“¦ æµ‹è¯•æ¨¡å—å¯¼å…¥...")
        
        imports_to_test = [
            ("universal_llm_adapter_v13", "iflow.adapters"),
            ("ultimate_consciousness_system_v5", "iflow.core"),
            ("ultimate_arq_engine_v5", "iflow.core"),
            ("ultimate_fusion_agent_v5", "iflow.agents"),
            ("ultimate_workflow_engine_v4", "iflow.core"),
            ("auto_test_heal_system_v5", "iflow.core"),
            ("predictive_maintenance_system_v5", "iflow.core"),
            ("intelligent_context_cache_v5", "iflow.core"),
            ("comprehensive_hook_manager_v4", "iflow.hooks"),
            ("self_evolution_engine_v4", "iflow.core")
        ]
        
        for module_name, package in imports_to_test:
            start_time = time.time()
            try:
                module = __import__(f"{package}.{module_name}", fromlist=[module_name])
                duration = time.time() - start_time
                
                self.test_results.append(TestResult(
                    name=f"å¯¼å…¥_{module_name}",
                    status="passed",
                    duration=duration,
                    message=f"æˆåŠŸå¯¼å…¥{module_name}",
                    details={"module": module_name, "package": package}
                ))
                
                logger.info(f"âœ… {module_name} å¯¼å…¥æˆåŠŸ")
                
            except Exception as e:
                duration = time.time() - start_time
                
                self.test_results.append(TestResult(
                    name=f"å¯¼å…¥_{module_name}",
                    status="error",
                    duration=duration,
                    message=f"å¯¼å…¥å¤±è´¥: {str(e)}",
                    details={"error": traceback.format_exc()}
                ))
                
                logger.error(f"âŒ {module_name} å¯¼å…¥å¤±è´¥: {e}")
    
    async def _test_initialization(self):
        """æµ‹è¯•ç»„ä»¶åˆå§‹åŒ–"""
        logger.info("ğŸ”§ æµ‹è¯•ç»„ä»¶åˆå§‹åŒ–...")
        
        # æµ‹è¯•å·¥ä½œæµå¼•æ“åˆå§‹åŒ–
        await self._test_workflow_engine_initialization()
        
        # æµ‹è¯•å„ç»„ä»¶åˆå§‹åŒ–
        if self.components["workflow_engine"]:
            await self._test_component_initialization()
    
    async def _test_workflow_engine_initialization(self):
        """æµ‹è¯•å·¥ä½œæµå¼•æ“åˆå§‹åŒ–"""
        start_time = time.time()
        
        try:
            from iflow.core.ultimate_workflow_engine_v4 import UltimateWorkflowEngineV4
            
            # åˆ›å»ºå¼•æ“å®ä¾‹
            engine = UltimateWorkflowEngineV4()
            
            # åˆå§‹åŒ–
            await engine.initialize()
            
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="å·¥ä½œæµå¼•æ“åˆå§‹åŒ–",
                status="passed",
                duration=duration,
                message="å·¥ä½œæµå¼•æ“åˆå§‹åŒ–æˆåŠŸ",
                details={"initialized": engine._initialized}
            ))
            
            # ä¿å­˜å¼•æ“å¼•ç”¨
            self.components["workflow_engine"] = engine
            
            # è·å–å…¶ä»–ç»„ä»¶å¼•ç”¨
            self.components["model_adapter"] = engine.model_adapter
            self.components["consciousness_system"] = engine.consciousness_system
            self.components["arq_engine"] = engine.arq_engine
            self.components["fusion_agent"] = engine.fusion_agent
            self.components["test_heal_system"] = getattr(engine, 'test_heal_system', None)
            self.components["maintenance_system"] = getattr(engine, 'maintenance_system', None)
            self.components["context_cache"] = getattr(engine, 'context_cache', None)
            self.components["hook_integration"] = engine.hook_integration
            self.components["evolution_engine"] = engine.evolution_engine
            
            logger.info("âœ… å·¥ä½œæµå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="å·¥ä½œæµå¼•æ“åˆå§‹åŒ–",
                status="error",
                duration=duration,
                message=f"åˆå§‹åŒ–å¤±è´¥: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ å·¥ä½œæµå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def _test_component_initialization(self):
        """æµ‹è¯•å„ç»„ä»¶åˆå§‹åŒ–çŠ¶æ€"""
        components_to_test = [
            ("æ¨¡å‹é€‚é…å™¨", "model_adapter"),
            ("æ„è¯†ç³»ç»Ÿ", "consciousness_system"),
            ("ARQå¼•æ“", "arq_engine"),
            ("èåˆæ™ºèƒ½ä½“", "fusion_agent"),
            ("è‡ªåŠ¨æµ‹è¯•ä¿®å¤ç³»ç»Ÿ", "test_heal_system"),
            ("é¢„æµ‹æ€§ç»´æŠ¤ç³»ç»Ÿ", "maintenance_system"),
            ("æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ", "context_cache"),
            ("Hooké›†æˆç³»ç»Ÿ", "hook_integration"),
            ("è¿›åŒ–å¼•æ“", "evolution_engine")
        ]
        
        for name, key in components_to_test:
            start_time = time.time()
            
            if self.components.get(key):
                duration = time.time() - start_time
                
                self.test_results.append(TestResult(
                    name=f"{name}çŠ¶æ€",
                    status="passed",
                    duration=duration,
                    message=f"{name}å·²åˆå§‹åŒ–",
                    details={"component": key}
                ))
                
                logger.info(f"âœ… {name}å·²åˆå§‹åŒ–")
            else:
                duration = time.time() - start_time
                
                self.test_results.append(TestResult(
                    name=f"{name}çŠ¶æ€",
                    status="failed",
                    duration=duration,
                    message=f"{name}æœªåˆå§‹åŒ–",
                    details={"component": key}
                ))
                
                logger.warning(f"âš ï¸ {name}æœªåˆå§‹åŒ–")
    
    async def _test_functionality(self):
        """æµ‹è¯•åŠŸèƒ½"""
        logger.info("âš™ï¸ æµ‹è¯•åŠŸèƒ½...")
        
        # æµ‹è¯•ä»»åŠ¡æ‰§è¡Œ
        await self._test_task_execution()
        
        # æµ‹è¯•æ„è¯†ç³»ç»Ÿ
        await self._test_consciousness_system()
        
        # æµ‹è¯•ARQå¼•æ“
        await self._test_arq_engine()
        
        # æµ‹è¯•èåˆæ™ºèƒ½ä½“
        await self._test_fusion_agent()
        
        # æµ‹è¯•ç¼“å­˜ç³»ç»Ÿ
        await self._test_context_cache()
        
        # æµ‹è¯•ç»´æŠ¤ç³»ç»Ÿ
        await self._test_maintenance_system()
    
    async def _test_task_execution(self):
        """æµ‹è¯•ä»»åŠ¡æ‰§è¡Œ"""
        if not self.components["workflow_engine"]:
            return
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œç®€å•ä»»åŠ¡
            result = await self.components["workflow_engine"].execute_task(
                "åˆ†æ1+1ç­‰äºå¤šå°‘",
                priority="medium"
            )
            
            duration = time.time() - start_time
            
            if result.get("success"):
                self.test_results.append(TestResult(
                    name="ä»»åŠ¡æ‰§è¡Œ",
                    status="passed",
                    duration=duration,
                    message="ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ",
                    details={"task": "åˆ†æ1+1ç­‰äºå¤šå°‘", "result": result}
                ))
                
                logger.info("âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
            else:
                self.test_results.append(TestResult(
                    name="ä»»åŠ¡æ‰§è¡Œ",
                    status="failed",
                    duration=duration,
                    message=f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {result.get('error')}",
                    details={"result": result}
                ))
                
                logger.warning(f"âš ï¸ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {result.get('error')}")
                
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="ä»»åŠ¡æ‰§è¡Œ",
                status="error",
                duration=duration,
                message=f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
    
    async def _test_consciousness_system(self):
        """æµ‹è¯•æ„è¯†ç³»ç»Ÿ"""
        if not self.components["consciousness_system"]:
            return
        
        start_time = time.time()
        
        try:
            from iflow.core.ultimate_consciousness_system_v4 import ThoughtType
            
            # è®°å½•æµ‹è¯•æ€æƒ³
            thought = await self.components["consciousness_system"].record_thought(
                content="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ€æƒ³",
                thought_type=ThoughtType.ANALYTICAL,
                confidence=0.9,
                importance=0.8
            )
            
            # è·å–æ„è¯†ä¸Šä¸‹æ–‡
            context = await self.components["consciousness_system"].get_consciousness_context()
            
            duration = time.time() - start_time
            
            if thought and context:
                self.test_results.append(TestResult(
                    name="æ„è¯†ç³»ç»ŸåŠŸèƒ½",
                    status="passed",
                    duration=duration,
                    message="æ„è¯†ç³»ç»ŸåŠŸèƒ½æ­£å¸¸",
                    details={"thought_id": thought.id, "context_keys": list(context.keys())}
                ))
                
                logger.info("âœ… æ„è¯†ç³»ç»ŸåŠŸèƒ½æ­£å¸¸")
            else:
                self.test_results.append(TestResult(
                    name="æ„è¯†ç³»ç»ŸåŠŸèƒ½",
                    status="failed",
                    duration=duration,
                    message="æ„è¯†ç³»ç»ŸåŠŸèƒ½å¼‚å¸¸",
                    details={"thought": thought, "context": context}
                ))
                
                logger.warning("âš ï¸ æ„è¯†ç³»ç»ŸåŠŸèƒ½å¼‚å¸¸")
                
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="æ„è¯†ç³»ç»ŸåŠŸèƒ½",
                status="error",
                duration=duration,
                message=f"æ„è¯†ç³»ç»Ÿæµ‹è¯•å¼‚å¸¸: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ æ„è¯†ç³»ç»Ÿæµ‹è¯•å¼‚å¸¸: {e}")
    
    async def _test_arq_engine(self):
        """æµ‹è¯•ARQå¼•æ“"""
        if not self.components["arq_engine"] or not self.components["model_adapter"]:
            return
        
        start_time = time.time()
        
        try:
            # å¤„ç†ç®€å•æ¨ç†
            result = await self.components["arq_engine"].process_reasoning(
                task="æµ‹è¯•ARQæ¨ç†",
                context=[{"type": "test"}],
                llm_adapter=self.components["model_adapter"]
            )
            
            duration = time.time() - start_time
            
            if result.get("success"):
                self.test_results.append(TestResult(
                    name="ARQå¼•æ“åŠŸèƒ½",
                    status="passed",
                    duration=duration,
                    message="ARQå¼•æ“åŠŸèƒ½æ­£å¸¸",
                    details={"result": result}
                ))
                
                logger.info("âœ… ARQå¼•æ“åŠŸèƒ½æ­£å¸¸")
            else:
                self.test_results.append(TestResult(
                    name="ARQå¼•æ“åŠŸèƒ½",
                    status="failed",
                    duration=duration,
                    message=f"ARQå¼•æ“åŠŸèƒ½å¼‚å¸¸: {result.get('error')}",
                    details={"result": result}
                ))
                
                logger.warning(f"âš ï¸ ARQå¼•æ“åŠŸèƒ½å¼‚å¸¸: {result.get('error')}")
                
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="ARQå¼•æ“åŠŸèƒ½",
                status="error",
                duration=duration,
                message=f"ARQå¼•æ“æµ‹è¯•å¼‚å¸¸: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ ARQå¼•æ“æµ‹è¯•å¼‚å¸¸: {e}")
    
    async def _test_fusion_agent(self):
        """æµ‹è¯•èåˆæ™ºèƒ½ä½“"""
        if not self.components["fusion_agent"]:
            return
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œåˆ†æä»»åŠ¡
            analysis = await self.components["fusion_agent"].analyze_task("æµ‹è¯•ä»»åŠ¡åˆ†æ")
            
            duration = time.time() - start_time
            
            if analysis:
                self.test_results.append(TestResult(
                    name="èåˆæ™ºèƒ½ä½“åŠŸèƒ½",
                    status="passed",
                    duration=duration,
                    message="èåˆæ™ºèƒ½ä½“åŠŸèƒ½æ­£å¸¸",
                    details={"analysis": analysis}
                ))
                
                logger.info("âœ… èåˆæ™ºèƒ½ä½“åŠŸèƒ½æ­£å¸¸")
            else:
                self.test_results.append(TestResult(
                    name="èåˆæ™ºèƒ½ä½“åŠŸèƒ½",
                    status="failed",
                    duration=duration,
                    message="èåˆæ™ºèƒ½ä½“åŠŸèƒ½å¼‚å¸¸",
                    details={"analysis": analysis}
                ))
                
                logger.warning("âš ï¸ èåˆæ™ºèƒ½ä½“åŠŸèƒ½å¼‚å¸¸")
                
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="èåˆæ™ºèƒ½ä½“åŠŸèƒ½",
                status="error",
                duration=duration,
                message=f"èåˆæ™ºèƒ½ä½“æµ‹è¯•å¼‚å¸¸: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ èåˆæ™ºèƒ½ä½“æµ‹è¯•å¼‚å¸¸: {e}")
    
    async def _test_context_cache(self):
        """æµ‹è¯•ç¼“å­˜ç³»ç»Ÿ"""
        if not self.components["context_cache"]:
            return
        
        start_time = time.time()
        
        try:
            from iflow.core.intelligent_context_cache_v5 import ContextType
            
            # å­˜å‚¨æµ‹è¯•æ•°æ®
            cache_id = await self.components["context_cache"].put(
                key="test_key",
                value="test_value",
                context_type=ContextType.TASK
            )
            
            # è·å–æ•°æ®
            cached_value = await self.components["context_cache"].get("test_key")
            
            # è·å–ç¼“å­˜ç»Ÿè®¡
            stats = await self.components["context_cache"].get_cache_stats()
            
            duration = time.time() - start_time
            
            if cache_id and cached_value == "test_value":
                self.test_results.append(TestResult(
                    name="ç¼“å­˜ç³»ç»ŸåŠŸèƒ½",
                    status="passed",
                    duration=duration,
                    message="ç¼“å­˜ç³»ç»ŸåŠŸèƒ½æ­£å¸¸",
                    details={"cache_id": cache_id, "stats": stats}
                ))
                
                logger.info("âœ… ç¼“å­˜ç³»ç»ŸåŠŸèƒ½æ­£å¸¸")
            else:
                self.test_results.append(TestResult(
                    name="ç¼“å­˜ç³»ç»ŸåŠŸèƒ½",
                    status="failed",
                    duration=duration,
                    message="ç¼“å­˜ç³»ç»ŸåŠŸèƒ½å¼‚å¸¸",
                    details={"cache_id": cache_id, "cached_value": cached_value}
                ))
                
                logger.warning("âš ï¸ ç¼“å­˜ç³»ç»ŸåŠŸèƒ½å¼‚å¸¸")
                
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="ç¼“å­˜ç³»ç»ŸåŠŸèƒ½",
                status="error",
                duration=duration,
                message=f"ç¼“å­˜ç³»ç»Ÿæµ‹è¯•å¼‚å¸¸: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ ç¼“å­˜ç³»ç»Ÿæµ‹è¯•å¼‚å¸¸: {e}")
    
    async def _test_maintenance_system(self):
        """æµ‹è¯•ç»´æŠ¤ç³»ç»Ÿ"""
        if not self.components["maintenance_system"]:
            return
        
        start_time = time.time()
        
        try:
            # è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
            health = await self.components["maintenance_system"].get_system_health()
            
            duration = time.time() - start_time
            
            if health:
                self.test_results.append(TestResult(
                    name="ç»´æŠ¤ç³»ç»ŸåŠŸèƒ½",
                    status="passed",
                    duration=duration,
                    message="ç»´æŠ¤ç³»ç»ŸåŠŸèƒ½æ­£å¸¸",
                    details={"health": health}
                ))
                
                logger.info("âœ… ç»´æŠ¤ç³»ç»ŸåŠŸèƒ½æ­£å¸¸")
            else:
                self.test_results.append(TestResult(
                    name="ç»´æŠ¤ç³»ç»ŸåŠŸèƒ½",
                    status="failed",
                    duration=duration,
                    message="ç»´æŠ¤ç³»ç»ŸåŠŸèƒ½å¼‚å¸¸",
                    details={"health": health}
                ))
                
                logger.warning("âš ï¸ ç»´æŠ¤ç³»ç»ŸåŠŸèƒ½å¼‚å¸¸")
                
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="ç»´æŠ¤ç³»ç»ŸåŠŸèƒ½",
                status="error",
                duration=duration,
                message=f"ç»´æŠ¤ç³»ç»Ÿæµ‹è¯•å¼‚å¸¸: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ ç»´æŠ¤ç³»ç»Ÿæµ‹è¯•å¼‚å¸¸: {e}")
    
    async def _test_performance(self):
        """æµ‹è¯•æ€§èƒ½"""
        logger.info("ğŸ“Š æµ‹è¯•æ€§èƒ½...")
        
        # æ”¶é›†ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
        await self._collect_performance_metrics()
        
        # æµ‹è¯•å“åº”æ—¶é—´
        await self._test_response_time()
        
        # æµ‹è¯•ååé‡
        await self._test_throughput()
        
        # æµ‹è¯•å†…å­˜ä½¿ç”¨
        await self._test_memory_usage()
    
    async def _collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # å†…å­˜ä½¿ç”¨
            memory = psutil.virtual_memory()
            memory_mb = memory.used / (1024 * 1024)
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            self.performance_metrics.append(PerformanceMetrics(
                cpu_usage=cpu_percent,
                memory_usage=memory_mb,
                response_time=0.0,  # å°†åœ¨å…¶ä»–æµ‹è¯•ä¸­å¡«å……
                throughput=0.0,    # å°†åœ¨å…¶ä»–æµ‹è¯•ä¸­å¡«å……
                error_rate=0.0     # å°†åœ¨å…¶ä»–æµ‹è¯•ä¸­å¡«å……
            ))
            
            logger.info(f"ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ - CPU: {cpu_percent:.1f}%, å†…å­˜: {memory_mb:.1f}MB")
            
        except Exception as e:
            logger.error(f"æ”¶é›†æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
    
    async def _test_response_time(self):
        """æµ‹è¯•å“åº”æ—¶é—´"""
        if not self.components["workflow_engine"]:
            return
        
        response_times = []
        test_tasks = [
            "è®¡ç®—2+2",
            "åˆ†æå¤©æ°”",
            "æ¨èä¹¦ç±",
            "è§£é‡Šæ¦‚å¿µ",
            "ç”Ÿæˆä»£ç "
        ]
        
        for task in test_tasks:
            start_time = time.time()
            
            try:
                await self.components["workflow_engine"].execute_task(task)
                response_time = time.time() - start_time
                response_times.append(response_time)
                
            except Exception as e:
                logger.warning(f"ä»»åŠ¡'{task}'æ‰§è¡Œå¤±è´¥: {e}")
        
        if response_times:
            avg_response_time = sum(response_times) / len(response_times)
            
            # æ›´æ–°æœ€æ–°çš„æ€§èƒ½æŒ‡æ ‡
            if self.performance_metrics:
                self.performance_metrics[-1].response_time = avg_response_time
            
            # è®°å½•æµ‹è¯•ç»“æœ
            status = "passed" if avg_response_time < self.benchmarks["response_time"] else "failed"
            
            self.test_results.append(TestResult(
                name="å“åº”æ—¶é—´",
                status=status,
                duration=avg_response_time,
                message=f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’",
                details={
                    "avg_response_time": avg_response_time,
                    "benchmark": self.benchmarks["response_time"],
                    "response_times": response_times
                }
            ))
            
            logger.info(f"â±ï¸ å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
    
    async def _test_throughput(self):
        """æµ‹è¯•ååé‡"""
        if not self.components["workflow_engine"]:
            return
        
        start_time = time.time()
        task_count = 10
        completed_tasks = 0
        
        # å¹¶å‘æ‰§è¡Œä»»åŠ¡
        tasks = []
        for i in range(task_count):
            task = self.components["workflow_engine"].execute_task(f"æµ‹è¯•ä»»åŠ¡{i}")
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡æˆåŠŸä»»åŠ¡
        for result in results:
            if isinstance(result, dict) and result.get("success"):
                completed_tasks += 1
        
        total_time = time.time() - start_time
        throughput = completed_tasks / total_time if total_time > 0 else 0
        
        # æ›´æ–°æœ€æ–°çš„æ€§èƒ½æŒ‡æ ‡
        if self.performance_metrics:
            self.performance_metrics[-1].throughput = throughput
        
        # è®°å½•æµ‹è¯•ç»“æœ
        self.test_results.append(TestResult(
            name="ååé‡",
            status="passed",
            duration=total_time,
            message=f"ååé‡: {throughput:.2f}ä»»åŠ¡/ç§’",
            details={
                "throughput": throughput,
                "completed_tasks": completed_tasks,
                "total_tasks": task_count,
                "total_time": total_time
            }
        ))
        
        logger.info(f"ğŸš€ ååé‡: {throughput:.2f}ä»»åŠ¡/ç§’")
    
    async def _test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        if not self.performance_metrics:
            return
        
        memory_usage = self.performance_metrics[-1].memory_usage
        benchmark = self.benchmarks["memory_usage"]
        
        status = "passed" if memory_usage < benchmark else "failed"
        
        self.test_results.append(TestResult(
            name="å†…å­˜ä½¿ç”¨",
            status=status,
            duration=0.0,
            message=f"å†…å­˜ä½¿ç”¨: {memory_usage:.1f}MB",
            details={
                "memory_usage": memory_usage,
                "benchmark": benchmark
            }
        ))
        
        logger.info(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {memory_usage:.1f}MB")
    
    async def _test_integration(self):
        """æµ‹è¯•é›†æˆ"""
        logger.info("ğŸ”— æµ‹è¯•é›†æˆ...")
        
        # æµ‹è¯•ç»„ä»¶é—´åä½œ
        await self._test_component_collaboration()
        
        # æµ‹è¯•å·¥ä½œæµæ‰§è¡Œ
        await self._test_workflow_execution()
    
    async def _test_component_collaboration(self):
        """æµ‹è¯•ç»„ä»¶åä½œ"""
        if not all([self.components["workflow_engine"], 
                   self.components["consciousness_system"],
                   self.components["fusion_agent"]]):
            return
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œä¸€ä¸ªéœ€è¦å¤šä¸ªç»„ä»¶åä½œçš„ä»»åŠ¡
            result = await self.components["workflow_engine"].execute_task(
                "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
                priority="high"
            )
            
            duration = time.time() - start_time
            
            if result.get("success"):
                self.test_results.append(TestResult(
                    name="ç»„ä»¶åä½œ",
                    status="passed",
                    duration=duration,
                    message="ç»„ä»¶åä½œæ­£å¸¸",
                    details={"result": result}
                ))
                
                logger.info("âœ… ç»„ä»¶åä½œæ­£å¸¸")
            else:
                self.test_results.append(TestResult(
                    name="ç»„ä»¶åä½œ",
                    status="failed",
                    duration=duration,
                    message=f"ç»„ä»¶åä½œå¤±è´¥: {result.get('error')}",
                    details={"result": result}
                ))
                
                logger.warning(f"âš ï¸ ç»„ä»¶åä½œå¤±è´¥: {result.get('error')}")
                
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="ç»„ä»¶åä½œ",
                status="error",
                duration=duration,
                message=f"ç»„ä»¶åä½œå¼‚å¸¸: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ ç»„ä»¶åä½œå¼‚å¸¸: {e}")
    
    async def _test_workflow_execution(self):
        """æµ‹è¯•å·¥ä½œæµæ‰§è¡Œ"""
        if not self.components["workflow_engine"]:
            return
        
        start_time = time.time()
        
        try:
            # å®šä¹‰å¤æ‚å·¥ä½œæµ
            workflow = {
                "name": "æµ‹è¯•å·¥ä½œæµ",
                "steps": [
                    {"description": "åˆ†æé—®é¢˜", "critical": False},
                    {"description": "è®¾è®¡æ–¹æ¡ˆ", "critical": False},
                    {"description": "å®ç°ä»£ç ", "critical": True},
                    {"description": "æµ‹è¯•éªŒè¯", "critical": False}
                ],
                "context": {"type": "test"}
            }
            
            # æ‰§è¡Œå·¥ä½œæµ
            result = await self.components["workflow_engine"].execute_complex_workflow(workflow)
            
            duration = time.time() - start_time
            
            if result.get("success"):
                self.test_results.append(TestResult(
                    name="å·¥ä½œæµæ‰§è¡Œ",
                    status="passed",
                    duration=duration,
                    message="å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ",
                    details={"result": result}
                ))
                
                logger.info("âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
            else:
                self.test_results.append(TestResult(
                    name="å·¥ä½œæµæ‰§è¡Œ",
                    status="failed",
                    duration=duration,
                    message=f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {result.get('error')}",
                    details={"result": result}
                ))
                
                logger.warning(f"âš ï¸ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {result.get('error')}")
                
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="å·¥ä½œæµæ‰§è¡Œ",
                status="error",
                duration=duration,
                message=f"å·¥ä½œæµæ‰§è¡Œå¼‚å¸¸: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¼‚å¸¸: {e}")
    
    async def _test_stability(self):
        """æµ‹è¯•ç¨³å®šæ€§"""
        logger.info("ğŸ›¡ï¸ æµ‹è¯•ç¨³å®šæ€§...")
        
        # å‹åŠ›æµ‹è¯•
        await self._test_stress()
        
 # é”™è¯¯æ¢å¤æµ‹è¯•
        await self._test_error_recovery()
    
    async def _test_stress(self):
        """å‹åŠ›æµ‹è¯•"""
        if not self.components["workflow_engine"]:
            return
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œå¤§é‡ä»»åŠ¡
            task_count = 50
            success_count = 0
            
            for i in range(task_count):
                try:
                    result = await self.components["workflow_engine"].execute_task(
                        f"å‹åŠ›æµ‹è¯•ä»»åŠ¡{i}",
                        priority="low"
                    )
                    
                    if result.get("success"):
                        success_count += 1
                        
                except Exception as e:
                    logger.warning(f"å‹åŠ›æµ‹è¯•ä»»åŠ¡{i}å¤±è´¥: {e}")
            
            duration = time.time() - start_time
            success_rate = success_count / task_count
            
            # æ›´æ–°æœ€æ–°çš„æ€§èƒ½æŒ‡æ ‡
            if self.performance_metrics:
                self.performance_metrics[-1].error_rate = 1 - success_rate
            
            status = "passed" if success_rate > 0.8 else "failed"
            
            self.test_results.append(TestResult(
                name="å‹åŠ›æµ‹è¯•",
                status=status,
                duration=duration,
                message=f"æˆåŠŸç‡: {success_rate:.2%}",
                details={
                    "task_count": task_count,
                    "success_count": success_count,
                    "success_rate": success_rate
                }
            ))
            
            logger.info(f"ğŸ’ª å‹åŠ›æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.2%}")
            
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="å‹åŠ›æµ‹è¯•",
                status="error",
                duration=duration,
                message=f"å‹åŠ›æµ‹è¯•å¼‚å¸¸: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ å‹åŠ›æµ‹è¯•å¼‚å¸¸: {e}")
    
    async def _test_error_recovery(self):
        """é”™è¯¯æ¢å¤æµ‹è¯•"""
        if not self.components["workflow_engine"]:
            return
        
        start_time = time.time()
        
        try:
            # æ•…æ„æ‰§è¡Œä¸€ä¸ªå¯èƒ½å¤±è´¥çš„ä»»åŠ¡
            result = await self.components["workflow_engine"].execute_task(
                "è¿™æ˜¯ä¸€ä¸ªæ•…æ„è®¾è®¡çš„å¯èƒ½å¯¼è‡´é”™è¯¯çš„æµ‹è¯•ä»»åŠ¡ï¼ŒåŒ…å«æ— æ•ˆè¾“å…¥å’Œç‰¹æ®Šå­—ç¬¦ï¼š@#$%^&*()",
                priority="low"
            )
            
            duration = time.time() - start_time
            
            # æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦ä»ç„¶å“åº”
            recovery_result = await self.components["workflow_engine"].execute_task(
                "ç³»ç»Ÿæ¢å¤æµ‹è¯•",
                priority="low"
            )
            
            if recovery_result.get("success"):
                self.test_results.append(TestResult(
                    name="é”™è¯¯æ¢å¤",
                    status="passed",
                    duration=duration,
                    message="é”™è¯¯æ¢å¤æˆåŠŸ",
                    details={
                        "original_result": result,
                        "recovery_result": recovery_result
                    }
                ))
                
                logger.info("âœ… é”™è¯¯æ¢å¤æˆåŠŸ")
            else:
                self.test_results.append(TestResult(
                    name="é”™è¯¯æ¢å¤",
                    status="failed",
                    duration=duration,
                    message="é”™è¯¯æ¢å¤å¤±è´¥",
                    details={
                        "original_result": result,
                        "recovery_result": recovery_result
                    }
                ))
                
                logger.warning("âš ï¸ é”™è¯¯æ¢å¤å¤±è´¥")
                
        except Exception as e:
            duration = time.time() - start_time
            
            self.test_results.append(TestResult(
                name="é”™è¯¯æ¢å¤",
                status="error",
                duration=duration,
                message=f"é”™è¯¯æ¢å¤æµ‹è¯•å¼‚å¸¸: {str(e)}",
                details={"error": traceback.format_exc()}
            ))
            
            logger.error(f"âŒ é”™è¯¯æ¢å¤æµ‹è¯•å¼‚å¸¸: {e}")
    
    async def _generate_evaluation_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        logger.info("ğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r.status == "passed")
        failed_tests = sum(1 for r in self.test_results if r.status == "failed")
        error_tests = sum(1 for r in self.test_results if r.status == "error")
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = passed_tests / total_tests if total_tests > 0 else 0
        
        # æ€§èƒ½åˆ†æ
        performance_summary = {}
        if self.performance_metrics:
            latest_metrics = self.performance_metrics[-1]
            performance_summary = {
                "cpu_usage": latest_metrics.cpu_usage,
                "memory_usage": latest_metrics.memory_usage,
                "response_time": latest_metrics.response_time,
                "throughput": latest_metrics.throughput,
                "error_rate": latest_metrics.error_rate
            }
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "evaluation_timestamp": datetime.now().isoformat(),
            "evaluation_duration": (datetime.now() - self.start_time).total_seconds(),
            "test_summary": {
                "total_tests": total_tests,
                "passed": passed_tests,
                "failed": failed_tests,
                "errors": error_tests,
                "success_rate": success_rate
            },
            "performance_summary": performance_summary,
            "benchmarks": self.benchmarks,
            "test_results": [
                {
                    "name": r.name,
                    "status": r.status,
                    "duration": r.duration,
                    "message": r.message,
                    "timestamp": r.timestamp.isoformat()
                } for r in self.test_results
            ],
            "component_status": {
                name: (component is not None).__str__()
                for name, component in self.components.items()
            },
            "recommendations": self._generate_recommendations()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = project_root / "Aé¡¹ç›®" / "iflow" / "reports" / "comprehensive_evaluation_v5.json"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        # æ˜¾ç¤ºæ‘˜è¦
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦:")
        logger.info(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.info(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
        logger.info(f"å¤±è´¥æµ‹è¯•: {failed_tests}")
        logger.info(f"é”™è¯¯æµ‹è¯•: {error_tests}")
        logger.info(f"æˆåŠŸç‡: {success_rate:.2%}")
        
        if performance_summary:
            logger.info(f"CPUä½¿ç”¨ç‡: {performance_summary['cpu_usage']:.1f}%")
            logger.info(f"å†…å­˜ä½¿ç”¨: {performance_summary['memory_usage']:.1f}MB")
            logger.info(f"å“åº”æ—¶é—´: {performance_summary['response_time']:.3f}ç§’")
            logger.info(f"ååé‡: {performance_summary['throughput']:.2f}ä»»åŠ¡/ç§’")
            logger.info(f"é”™è¯¯ç‡: {performance_summary['error_rate']:.2%}")
        
        logger.info("="*50)
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæµ‹è¯•ç»“æœçš„å»ºè®®
        failed_tests = [r for r in self.test_results if r.status == "failed"]
        error_tests = [r for r in self.test_results if r.status == "error"]
        
        if failed_tests:
            recommendations.append(f"ä¿®å¤{len(failed_tests)}ä¸ªå¤±è´¥çš„æµ‹è¯•")
        
        if error_tests:
            recommendations.append(f"è§£å†³{len(error_tests)}ä¸ªé”™è¯¯æµ‹è¯•")
        
        # åŸºäºæ€§èƒ½çš„å»ºè®®
        if self.performance_metrics:
            latest_metrics = self.performance_metrics[-1]
            
            if latest_metrics.response_time > self.benchmarks["response_time"]:
                recommendations.append("ä¼˜åŒ–å“åº”æ—¶é—´ï¼Œå½“å‰è¶…è¿‡åŸºå‡†å€¼")
            
            if latest_metrics.memory_usage > self.benchmarks["memory_usage"]:
                recommendations.append("ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œå½“å‰è¶…è¿‡åŸºå‡†å€¼")
            
            if latest_metrics.cpu_usage > self.benchmarks["cpu_usage"]:
                recommendations.append("ä¼˜åŒ–CPUä½¿ç”¨ç‡ï¼Œå½“å‰è¶…è¿‡åŸºå‡†å€¼")
            
            if latest_metrics.error_rate > (1 - self.benchmarks["success_rate"]):
                recommendations.append("é™ä½é”™è¯¯ç‡ï¼Œæé«˜ç³»ç»Ÿç¨³å®šæ€§")
        
        # åŸºäºç»„ä»¶çŠ¶æ€çš„å»ºè®®
        missing_components = [
            name for name, component in self.components.items() 
            if component is None
        ]
        
        if missing_components:
            recommendations.append(f"åˆå§‹åŒ–ç¼ºå¤±çš„ç»„ä»¶: {', '.join(missing_components)}")
        
        # é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")
        
        return recommendations

async def main():
    """ä¸»å‡½æ•°"""
    evaluator = ComprehensiveEvaluationV5()
    report = await evaluator.run_all_tests()
    
    # è¿”å›é€€å‡ºç 
    success_rate = report["test_summary"]["success_rate"]
    exit_code = 0 if success_rate > 0.8 else 1
    
    return exit_code

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)