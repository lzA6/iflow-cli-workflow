#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§ª è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶ V9 (Automated Testing Framework V9)
ä¼ä¸šçº§æµ‹è¯•è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒå•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€æ€§èƒ½æµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•

æ ¸å¿ƒç‰¹æ€§ï¼š
1. æ™ºèƒ½æµ‹è¯•ç”Ÿæˆ - AIé©±åŠ¨çš„æµ‹è¯•ç”¨ä¾‹è‡ªåŠ¨ç”Ÿæˆ
2. å¹¶è¡Œæµ‹è¯•æ‰§è¡Œ - å¤šçº¿ç¨‹/å¤šè¿›ç¨‹æµ‹è¯•ä¼˜åŒ–
3. å®æ—¶æ€§èƒ½ç›‘æ§ - æµ‹è¯•è¿‡ç¨‹ä¸­çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†
4. æ™ºèƒ½ç¼ºé™·åˆ†æ - è‡ªåŠ¨åŒ–ç¼ºé™·å®šä½å’Œåˆ†æ
5. æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ - è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šå’Œå¯è§†åŒ–
"""

import os
import sys
import json
import asyncio
import logging
import time
import unittest
import threading
import multiprocessing
import pytest
import coverage
import psutil
import gc
import tracemalloc
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import numpy as np
from collections import defaultdict, deque
import inspect
import importlib.util

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- æ ¸å¿ƒæšä¸¾å’Œæ•°æ®ç»“æ„ ---

class TestType(Enum):
    """æµ‹è¯•ç±»å‹"""
    UNIT = "unit"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    END_TO_END = "end_to_end"
    SECURITY = "security"
    COMPATIBILITY = "compatibility"

class TestStatus(Enum):
    """æµ‹è¯•çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    ERROR = "error"

class Priority(Enum):
    """ä¼˜å…ˆçº§"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹"""
    id: str
    name: str
    test_type: TestType
    function: Callable
    priority: Priority = Priority.MEDIUM
    timeout: float = 30.0
    expected_result: Any = None
    dependencies: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    status: TestStatus = TestStatus.PENDING
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    execution_time: float = 0.0
    error_message: Optional[str] = None
    performance_metrics: Dict[str, float] = field(default_factory=dict)

@dataclass
class TestSuite:
    """æµ‹è¯•å¥—ä»¶"""
    name: str
    test_cases: List[TestCase] = field(default_factory=list)
    setup_function: Optional[Callable] = None
    teardown_function: Optional[Callable] = None
    parallel: bool = True
    max_workers: int = 4
    status: TestStatus = TestStatus.PENDING
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0
    skipped_tests: int = 0

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    memory_peak: float = 0.0
    execution_time: float = 0.0
    throughput: float = 0.0
    latency_avg: float = 0.0
    latency_p95: float = 0.0
    latency_p99: float = 0.0
    error_rate: float = 0.0
    requests_per_second: float = 0.0

class IntelligentTestGenerator:
    """æ™ºèƒ½æµ‹è¯•ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.code_analyzer = CodeAnalyzer()
        self.pattern_database = TestPatternDatabase()
        self.generation_history = deque(maxlen=1000)
        
    def generate_tests_for_function(self, func: Callable, 
                                  test_type: TestType = TestType.UNIT) -> List[TestCase]:
        """ä¸ºå‡½æ•°ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        test_cases = []
        
        try:
            # åˆ†æå‡½æ•°ç­¾åå’Œæ–‡æ¡£
            func_info = self.code_analyzer.analyze_function(func)
            
            # ç”Ÿæˆæ­£å¸¸æƒ…å†µæµ‹è¯•
            normal_cases = self._generate_normal_cases(func_info)
            test_cases.extend(normal_cases)
            
            # ç”Ÿæˆè¾¹ç•Œæƒ…å†µæµ‹è¯•
            edge_cases = self._generate_edge_cases(func_info)
            test_cases.extend(edge_cases)
            
            # ç”Ÿæˆå¼‚å¸¸æƒ…å†µæµ‹è¯•
            error_cases = self._generate_error_cases(func_info)
            test_cases.extend(error_cases)
            
            # ç”Ÿæˆæ€§èƒ½æµ‹è¯•
            if test_type in [TestType.PERFORMANCE, TestType.INTEGRATION]:
                perf_cases = self._generate_performance_cases(func_info)
                test_cases.extend(perf_cases)
            
        except Exception as e:
            logger.error(f"æµ‹è¯•ç”Ÿæˆå¤±è´¥: {e}")
        
        return test_cases
    
    def _generate_normal_cases(self, func_info: Dict[str, Any]) -> List[TestCase]:
        """ç”Ÿæˆæ­£å¸¸æƒ…å†µæµ‹è¯•"""
        test_cases = []
        
        # åŸºäºå‚æ•°ç±»å‹ç”Ÿæˆæµ‹è¯•æ•°æ®
        for param in func_info.get('parameters', []):
            param_name = param['name']
            param_type = param.get('type', 'any')
            
            # ç”Ÿæˆå…¸å‹å€¼
            test_values = self._get_typical_values(param_type)
            
            for value in test_values:
                test_case = TestCase(
                    id=f"normal_{param_name}_{hash(str(value)) % 10000}",
                    name=f"Test {func_info['name']} with {param_name}={value}",
                    test_type=TestType.UNIT,
                    function=self._create_test_function(func_info['name'], {param_name: value}),
                    priority=Priority.HIGH,
                    tags=["normal", param_name]
                )
                test_cases.append(test_case)
        
        return test_cases
    
    def _generate_edge_cases(self, func_info: Dict[str, Any]) -> List[TestCase]:
        """ç”Ÿæˆè¾¹ç•Œæƒ…å†µæµ‹è¯•"""
        test_cases = []
        
        # ç©ºå€¼æµ‹è¯•
        for param in func_info.get('parameters', []):
            param_name = param['name']
            
            # Noneå€¼æµ‹è¯•
            test_case = TestCase(
                id=f"edge_{param_name}_none",
                name=f"Test {func_info['name']} with {param_name}=None",
                test_type=TestType.UNIT,
                function=self._create_test_function(func_info['name'], {param_name: None}),
                priority=Priority.MEDIUM,
                tags=["edge", "null"]
            )
            test_cases.append(test_case)
            
            # ç©ºå­—ç¬¦ä¸²/ç©ºåˆ—è¡¨æµ‹è¯•
            if param.get('type') in ['str', 'list', 'dict']:
                empty_value = '' if param.get('type') == 'str' else ([] if param.get('type') == 'list' else {})
                test_case = TestCase(
                    id=f"edge_{param_name}_empty",
                    name=f"Test {func_info['name']} with {param_name}=empty",
                    test_type=TestType.UNIT,
                    function=self._create_test_function(func_info['name'], {param_name: empty_value}),
                    priority=Priority.MEDIUM,
                    tags=["edge", "empty"]
                )
                test_cases.append(test_case)
        
        return test_cases
    
    def _generate_error_cases(self, func_info: Dict[str, Any]) -> List[TestCase]:
        """ç”Ÿæˆå¼‚å¸¸æƒ…å†µæµ‹è¯•"""
        test_cases = []
        
        # ç±»å‹é”™è¯¯æµ‹è¯•
        for param in func_info.get('parameters', []):
            param_name = param['name']
            param_type = param.get('type', 'any')
            
            # ç”Ÿæˆé”™è¯¯ç±»å‹çš„å€¼
            wrong_values = self._get_wrong_type_values(param_type)
            
            for wrong_value in wrong_values:
                test_case = TestCase(
                    id=f"error_{param_name}_type",
                    name=f"Test {func_info['name']} with wrong type for {param_name}",
                    test_type=TestType.UNIT,
                    function=self._create_test_function(func_info['name'], {param_name: wrong_value}),
                    priority=Priority.MEDIUM,
                    tags=["error", "type"],
                    expected_result="exception"
                )
                test_cases.append(test_case)
        
        return test_cases
    
    def _generate_performance_cases(self, func_info: Dict[str, Any]) -> List[TestCase]:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•"""
        test_cases = []
        
        # å¤§æ•°æ®é‡æµ‹è¯•
        for param in func_info.get('parameters', []):
            param_name = param['name']
            param_type = param.get('type', 'any')
            
            if param_type in ['list', 'str', 'dict']:
                large_value = self._get_large_value(param_type)
                
                test_case = TestCase(
                    id=f"perf_{param_name}_large",
                    name=f"Performance test {func_info['name']} with large {param_name}",
                    test_type=TestType.PERFORMANCE,
                    function=self._create_test_function(func_info['name'], {param_name: large_value}),
                    priority=Priority.LOW,
                    tags=["performance", "large_data"],
                    timeout=60.0
                )
                test_cases.append(test_case)
        
        return test_cases
    
    def _get_typical_values(self, param_type: str) -> List[Any]:
        """è·å–å…¸å‹å€¼"""
        value_map = {
            'int': [0, 1, -1, 42, 100],
            'float': [0.0, 1.0, -1.0, 3.14, 0.5],
            'str': ['', 'hello', 'test', 'ä¸­æ–‡', 'ğŸš€'],
            'bool': [True, False],
            'list': [[], [1], [1, 2, 3]],
            'dict': [{}, {'key': 'value'}, {'a': 1, 'b': 2}],
            'any': [None, 0, '', [], {}]
        }
        return value_map.get(param_type, [None])
    
    def _get_wrong_type_values(self, param_type: str) -> List[Any]:
        """è·å–é”™è¯¯ç±»å‹çš„å€¼"""
        wrong_type_map = {
            'int': ['string', [], {}, 3.14],
            'float': ['string', [], {}, True],
            'str': [123, [], {}, True],
            'bool': ['string', 123, [], {}],
            'list': ['string', 123, {}, True],
            'dict': ['string', 123, [], True],
            'any': []
        }
        return wrong_type_map.get(param_type, [])
    
    def _get_large_value(self, param_type: str) -> Any:
        """è·å–å¤§æ•°æ®å€¼"""
        if param_type == 'list':
            return list(range(10000))
        elif param_type == 'str':
            return 'x' * 100000
        elif param_type == 'dict':
            return {f'key_{i}': f'value_{i}' for i in range(1000)}
        else:
            return None
    
    def _create_test_function(self, func_name: str, test_args: Dict[str, Any]) -> Callable:
        """åˆ›å»ºæµ‹è¯•å‡½æ•°"""
        def test_function():
            try:
                # åŠ¨æ€å¯¼å…¥å¹¶è°ƒç”¨å‡½æ•°
                module_name = func_name.split('.')[0] if '.' in func_name else '__main__'
                func = getattr(sys.modules.get(module_name), func_name.split('.')[-1])
                
                result = func(**test_args)
                return result
                
            except Exception as e:
                if "exception" in str(test_args.values()):
                    return "exception_caught"
                raise
        
        return test_function

class CodeAnalyzer:
    """ä»£ç åˆ†æå™¨"""
    
    def analyze_function(self, func: Callable) -> Dict[str, Any]:
        """åˆ†æå‡½æ•°"""
        try:
            sig = inspect.signature(func)
            doc = inspect.getdoc(func) or ""
            
            parameters = []
            for name, param in sig.parameters.items():
                param_info = {
                    'name': name,
                    'type': self._get_type_annotation(param),
                    'default': param.default if param.default != inspect.Parameter.empty else None,
                    'required': param.default == inspect.Parameter.empty
                }
                parameters.append(param_info)
            
            return {
                'name': func.__name__,
                'parameters': parameters,
                'docstring': doc,
                'return_type': self._get_return_type_annotation(sig),
                'module': func.__module__
            }
            
        except Exception as e:
            logger.error(f"å‡½æ•°åˆ†æå¤±è´¥: {e}")
            return {
                'name': getattr(func, '__name__', 'unknown'),
                'parameters': [],
                'docstring': '',
                'return_type': 'any',
                'module': getattr(func, '__module__', 'unknown')
            }
    
    def _get_type_annotation(self, param: inspect.Parameter) -> str:
        """è·å–ç±»å‹æ³¨è§£"""
        if param.annotation == inspect.Parameter.empty:
            return 'any'
        
        try:
            return param.annotation.__name__
        except AttributeError:
            return str(param.annotation)
    
    def _get_return_type_annotation(self, sig: inspect.Signature) -> str:
        """è·å–è¿”å›ç±»å‹æ³¨è§£"""
        if sig.return_annotation == inspect.Signature.empty:
            return 'any'
        
        try:
            return sig.return_annotation.__name__
        except AttributeError:
            return str(sig.return_annotation)

class TestPatternDatabase:
    """æµ‹è¯•æ¨¡å¼æ•°æ®åº“"""
    
    def __init__(self):
        self.patterns = {
            'validation': [
                {'description': 'è¾“å…¥éªŒè¯', 'priority': 'high'},
                {'description': 'è¾¹ç•Œæ£€æŸ¥', 'priority': 'medium'}
            ],
            'error_handling': [
                {'description': 'å¼‚å¸¸å¤„ç†', 'priority': 'high'},
                {'description': 'é”™è¯¯æ¢å¤', 'priority': 'medium'}
            ],
            'performance': [
                {'description': 'å“åº”æ—¶é—´', 'priority': 'medium'},
                {'description': 'å†…å­˜ä½¿ç”¨', 'priority': 'medium'}
            ]
        }
    
    def get_patterns_for_type(self, test_type: TestType) -> List[Dict[str, Any]]:
        """è·å–æµ‹è¯•ç±»å‹çš„æ¨¡å¼"""
        return self.patterns.get(test_type.value, [])

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.metrics_history = deque(maxlen=1000)
        self.start_time = None
        self.process = psutil.Process()
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.start_time = time.time()
        tracemalloc.start()
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
    def stop_monitoring(self) -> PerformanceMetrics:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›æŒ‡æ ‡"""
        self.monitoring = False
        
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        
        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = time.time() - self.start_time if self.start_time else 0
        
        # è·å–CPUä½¿ç”¨ç‡
        cpu_usage = self.process.cpu_percent()
        
        # è·å–å†…å­˜ä½¿ç”¨
        memory_info = self.process.memory_info()
        memory_usage = memory_info.rss / 1024 / 1024  # MB
        
        metrics = PerformanceMetrics(
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            memory_peak=peak / 1024 / 1024,  # MB
            execution_time=execution_time
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                # è®°å½•CPUå’Œå†…å­˜ä½¿ç”¨
                cpu_percent = self.process.cpu_percent()
                memory_info = self.process.memory_info()
                
                # å¯ä»¥æ·»åŠ æ›´å¤šç›‘æ§æŒ‡æ ‡
                time.sleep(0.1)  # 100msé‡‡æ ·é—´éš”
                
            except Exception as e:
                logger.error(f"æ€§èƒ½ç›‘æ§é”™è¯¯: {e}")
                break

class AutomatedTestRunner:
    """è‡ªåŠ¨åŒ–æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.test_generator = IntelligentTestGenerator()
        self.performance_monitor = PerformanceMonitor()
        self.test_suites: List[TestSuite] = []
        self.results_history = deque(maxlen=100)
        self.coverage_collector = coverage.Coverage()
        
    def create_test_suite(self, name: str, test_modules: List[str], 
                         test_type: TestType = TestType.UNIT) -> TestSuite:
        """åˆ›å»ºæµ‹è¯•å¥—ä»¶"""
        test_suite = TestSuite(name=name)
        
        for module_name in test_modules:
            try:
                # åŠ¨æ€å¯¼å…¥æ¨¡å—
                module = importlib.import_module(module_name)
                
                # è·å–æ¨¡å—ä¸­çš„æ‰€æœ‰å‡½æ•°
                for name, obj in inspect.getmembers(module):
                    if inspect.isfunction(obj) and not name.startswith('_'):
                        # ä¸ºæ¯ä¸ªå‡½æ•°ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
                        test_cases = self.test_generator.generate_tests_for_function(
                            obj, test_type
                        )
                        test_suite.test_cases.extend(test_cases)
                        
            except Exception as e:
                logger.error(f"æ¨¡å— {module_name} å¯¼å…¥å¤±è´¥: {e}")
        
        test_suite.total_tests = len(test_suite.test_cases)
        return test_suite
    
    def run_test_suite(self, test_suite: TestSuite, 
                      parallel: bool = True) -> TestSuite:
        """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
        test_suite.status = TestStatus.RUNNING
        test_suite.start_time = datetime.now()
        
        logger.info(f"å¼€å§‹è¿è¡Œæµ‹è¯•å¥—ä»¶: {test_suite.name} ({test_suite.total_tests} ä¸ªæµ‹è¯•)")
        
        if parallel and test_suite.parallel:
            self._run_tests_parallel(test_suite)
        else:
            self._run_tests_sequential(test_suite)
        
        test_suite.end_time = datetime.now()
        test_suite.status = TestStatus.PASSED if test_suite.failed_tests == 0 else TestStatus.FAILED
        
        logger.info(f"æµ‹è¯•å¥—ä»¶å®Œæˆ: {test_suite.name} - "
                   f"é€šè¿‡: {test_suite.passed_tests}, "
                   f"å¤±è´¥: {test_suite.failed_tests}, "
                   f"è·³è¿‡: {test_suite.skipped_tests}")
        
        return test_suite
    
    def _run_tests_parallel(self, test_suite: TestSuite):
        """å¹¶è¡Œè¿è¡Œæµ‹è¯•"""
        max_workers = min(test_suite.max_workers, len(test_suite.test_cases))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æµ‹è¯•ä»»åŠ¡
            future_to_test = {
                executor.submit(self._run_single_test, test_case): test_case
                for test_case in test_suite.test_cases
            }
            
            # ç­‰å¾…æ‰€æœ‰æµ‹è¯•å®Œæˆ
            for future in as_completed(future_to_test):
                test_case = future_to_test[future]
                
                try:
                    result = future.result()
                    self._update_test_result(test_suite, test_case, result)
                except Exception as e:
                    test_case.status = TestStatus.ERROR
                    test_case.error_message = str(e)
                    test_suite.failed_tests += 1
    
    def _run_tests_sequential(self, test_suite: TestSuite):
        """é¡ºåºè¿è¡Œæµ‹è¯•"""
        for test_case in test_suite.test_cases:
            try:
                result = self._run_single_test(test_case)
                self._update_test_result(test_suite, test_case, result)
            except Exception as e:
                test_case.status = TestStatus.ERROR
                test_case.error_message = str(e)
                test_suite.failed_tests += 1
    
    def _run_single_test(self, test_case: TestCase) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        test_case.status = TestStatus.RUNNING
        test_case.start_time = datetime.now()
        
        # å¼€å§‹æ€§èƒ½ç›‘æ§
        self.performance_monitor.start_monitoring()
        
        try:
            # æ‰§è¡Œæµ‹è¯•å‡½æ•°
            result = test_case.function()
            
            # æ£€æŸ¥é¢„æœŸç»“æœ
            if test_case.expected_result == "exception":
                if result == "exception_caught":
                    test_case.status = TestStatus.PASSED
                else:
                    test_case.status = TestStatus.FAILED
                    test_case.error_message = "Expected exception but none was raised"
            else:
                test_case.status = TestStatus.PASSED
            
            # è·å–æ€§èƒ½æŒ‡æ ‡
            performance_metrics = self.performance_monitor.stop_monitoring()
            test_case.performance_metrics = {
                'execution_time': performance_metrics.execution_time,
                'memory_usage': performance_metrics.memory_usage,
                'cpu_usage': performance_metrics.cpu_usage
            }
            
        except Exception as e:
            test_case.status = TestStatus.FAILED
            test_case.error_message = str(e)
            
            # å³ä½¿å¤±è´¥ä¹Ÿè¦åœæ­¢æ€§èƒ½ç›‘æ§
            performance_metrics = self.performance_monitor.stop_monitoring()
            test_case.performance_metrics = {
                'execution_time': performance_metrics.execution_time,
                'memory_usage': performance_metrics.memory_usage,
                'cpu_usage': performance_metrics.cpu_usage
            }
        
        finally:
            test_case.end_time = datetime.now()
            if test_case.start_time:
                test_case.execution_time = (test_case.end_time - test_case.start_time).total_seconds()
        
        return {
            'status': test_case.status.value,
            'execution_time': test_case.execution_time,
            'performance_metrics': test_case.performance_metrics,
            'error_message': test_case.error_message
        }
    
    def _update_test_result(self, test_suite: TestSuite, 
                           test_case: TestCase, result: Dict[str, Any]):
        """æ›´æ–°æµ‹è¯•ç»“æœ"""
        if test_case.status == TestStatus.PASSED:
            test_suite.passed_tests += 1
        elif test_case.status == TestStatus.FAILED:
            test_suite.failed_tests += 1
        elif test_case.status == TestStatus.SKIPPED:
            test_suite.skipped_tests += 1
    
    def generate_test_report(self, test_suite: TestSuite) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = {
            'suite_name': test_suite.name,
            'summary': {
                'total_tests': test_suite.total_tests,
                'passed_tests': test_suite.passed_tests,
                'failed_tests': test_suite.failed_tests,
                'skipped_tests': test_suite.skipped_tests,
                'success_rate': test_suite.passed_tests / test_suite.total_tests if test_suite.total_tests > 0 else 0,
                'execution_time': (test_suite.end_time - test_suite.start_time).total_seconds() if test_suite.start_time and test_suite.end_time else 0
            },
            'failed_tests': [],
            'performance_summary': self._generate_performance_summary(test_suite),
            'coverage_info': self._get_coverage_info(),
            'recommendations': self._generate_recommendations(test_suite)
        }
        
        # æ”¶é›†å¤±è´¥çš„æµ‹è¯•
        for test_case in test_suite.test_cases:
            if test_case.status == TestStatus.FAILED:
                report['failed_tests'].append({
                    'name': test_case.name,
                    'error_message': test_case.error_message,
                    'execution_time': test_case.execution_time
                })
        
        return report
    
    def _generate_performance_summary(self, test_suite: TestSuite) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦"""
        execution_times = []
        memory_usages = []
        
        for test_case in test_suite.test_cases:
            if test_case.performance_metrics:
                execution_times.append(test_case.performance_metrics.get('execution_time', 0))
                memory_usages.append(test_case.performance_metrics.get('memory_usage', 0))
        
        if not execution_times:
            return {}
        
        return {
            'avg_execution_time': np.mean(execution_times),
            'max_execution_time': np.max(execution_times),
            'min_execution_time': np.min(execution_times),
            'avg_memory_usage': np.mean(memory_usages),
            'max_memory_usage': np.max(memory_usages)
        }
    
    def _get_coverage_info(self) -> Dict[str, Any]:
        """è·å–è¦†ç›–ç‡ä¿¡æ¯"""
        try:
            self.coverage_collector.stop()
            coverage_data = self.coverage_collector.get_data()
            
            return {
                'total_lines': coverage_data._lines,
                'covered_lines': len(coverage_data._lines),
                'coverage_percentage': coverage_data.report()
            }
        except Exception as e:
            logger.error(f"è¦†ç›–ç‡ä¿¡æ¯è·å–å¤±è´¥: {e}")
            return {}
    
    def _generate_recommendations(self, test_suite: TestSuite) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºå¤±è´¥ç‡çš„å»ºè®®
        failure_rate = test_suite.failed_tests / test_suite.total_tests if test_suite.total_tests > 0 else 0
        if failure_rate > 0.1:
            recommendations.append(f"å¤±è´¥ç‡è¾ƒé«˜ ({failure_rate:.1%})ï¼Œå»ºè®®æ£€æŸ¥ä»£ç è´¨é‡")
        
        # åŸºäºæ€§èƒ½çš„å»ºè®®
        slow_tests = [tc for tc in test_suite.test_cases 
                     if tc.execution_time > 5.0]
        if slow_tests:
            recommendations.append(f"å‘ç° {len(slow_tests)} ä¸ªæ…¢æµ‹è¯•ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½")
        
        # åŸºäºè¦†ç›–ç‡çš„å»ºè®®
        coverage_info = self._get_coverage_info()
        if coverage_info.get('coverage_percentage', 0) < 80:
            recommendations.append("æµ‹è¯•è¦†ç›–ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æµ‹è¯•ç”¨ä¾‹")
        
        return recommendations

# å…¨å±€æµ‹è¯•è¿è¡Œå™¨å®ä¾‹
_test_runner = None

def get_test_runner() -> AutomatedTestRunner:
    """è·å–æµ‹è¯•è¿è¡Œå™¨å•ä¾‹"""
    global _test_runner
    if _test_runner is None:
        _test_runner = AutomatedTestRunner()
    return _test_runner

# ä¾¿æ·å‡½æ•°
def run_automated_tests(test_modules: List[str], 
                       test_type: str = "unit",
                       parallel: bool = True) -> Dict[str, Any]:
    """ä¾¿æ·çš„è‡ªåŠ¨åŒ–æµ‹è¯•å‡½æ•°"""
    runner = get_test_runner()
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = runner.create_test_suite(
        name=f"Automated_{test_type}_Tests",
        test_modules=test_modules,
        test_type=TestType(test_type)
    )
    
    # è¿è¡Œæµ‹è¯•
    result_suite = runner.run_test_suite(test_suite, parallel=parallel)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = runner.generate_test_report(result_suite)
    
    return report

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    def example_function_add(a: int, b: int) -> int:
        """ç¤ºä¾‹å‡½æ•°ï¼šåŠ æ³•"""
        return a + b
    
    def example_function_divide(a: float, b: float) -> float:
        """ç¤ºä¾‹å‡½æ•°ï¼šé™¤æ³•"""
        if b == 0:
            raise ValueError("é™¤æ•°ä¸èƒ½ä¸ºé›¶")
        return a / b
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å—
    test_modules = ['__main__']
    
    # è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•
    print("ğŸ§ª å¼€å§‹è‡ªåŠ¨åŒ–æµ‹è¯•...")
    report = run_automated_tests(test_modules, test_type="unit")
    
    print("\nğŸ“Š æµ‹è¯•æŠ¥å‘Š:")
    print(json.dumps(report, indent=2, ensure_ascii=False))