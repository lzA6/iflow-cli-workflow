#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•å¥—ä»¶
ä¸“é—¨æµ‹è¯•å¤šæ™ºèƒ½ä½“å¹¶è¡Œå¤„ç†çš„åŠ é€Ÿæ¯”å’Œæ•ˆç‡ï¼ŒéªŒè¯V7å‡çº§ç‰ˆçš„æ€§èƒ½æå‡æ•ˆæœã€‚
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
import unittest
import statistics
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import threading
import psutil
import gc
from dataclasses import dataclass, field

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from iflow.core.parallel_agent_executor import ParallelAgentExecutor, AgentRole
from iflow.core.task_decomposer import TaskDecomposer
from iflow.core.workflow_stage_parallelizer import WorkflowStageParallelizer, WorkflowStage, WorkflowStageInfo
from iflow.core.optimized_fusion_cache import OptimizedFusionCache

logger = logging.getLogger(__name__)

@dataclass
class PerformanceTestResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    test_name: str
    test_type: str
    start_time: float
    end_time: float
    execution_time: float
    success: bool
    error: Optional[str] = None
    
    # å¹¶è¡Œæ€§èƒ½æŒ‡æ ‡
    serial_baseline: Optional[float] = None
    parallel_time: Optional[float] = None
    speedup_ratio: Optional[float] = None
    efficiency: Optional[float] = None
    throughput: Optional[float] = None
    
    # èµ„æºä½¿ç”¨æŒ‡æ ‡
    cpu_usage_avg: Optional[float] = None
    memory_usage_avg: Optional[float] = None
    memory_peak: Optional[float] = None
    resource_utilization: Optional[Dict[str, Any]] = None
    
    # è´¨é‡æŒ‡æ ‡
    quality_score: Optional[float] = None
    accuracy: Optional[float] = None
    consistency_score: Optional[float] = None
    
    # æµ‹è¯•é…ç½®
    test_config: Optional[Dict[str, Any]] = field(default_factory=dict)
    additional_metrics: Optional[Dict[str, Any]] = field(default_factory=dict)

class SystemResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.resource_data = []
        self.monitor_task = None
        self.process = psutil.Process()
    
    async def _monitor_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æº"""
        while self.monitoring:
            try:
                # CPUä½¿ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=0.1)
                
                # å†…å­˜ä½¿ç”¨
                memory_info = self.process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                
                # ç³»ç»Ÿæ•´ä½“å†…å­˜
                system_memory = psutil.virtual_memory().percent
                
                self.resource_data.append({
                    "timestamp": time.time(),
                    "cpu_percent": cpu_percent,
                    "memory_mb": memory_mb,
                    "system_memory_percent": system_memory,
                    "num_threads": threading.active_count()
                })
                
                await asyncio.sleep(0.1)  # æ¯100msé‡‡æ ·ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"èµ„æºç›‘æ§é”™è¯¯: {e}")
                break
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.resource_data = []
        self.monitor_task = asyncio.create_task(self._monitor_resources())
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_task:
            self.monitor_task.cancel()
            try:
                asyncio.run_until_complete(self.monitor_task)
            except:
                pass
    
    def get_resource_summary(self) -> Dict[str, Any]:
        """è·å–èµ„æºä½¿ç”¨æ‘˜è¦"""
        if not self.resource_data:
            return {}
        
        cpu_usage = [d["cpu_percent"] for d in self.resource_data]
        memory_usage = [d["memory_mb"] for d in self.resource_data]
        system_memory = [d["system_memory_percent"] for d in self.resource_data]
        
        return {
            "avg_cpu_usage": statistics.mean(cpu_usage),
            "max_cpu_usage": max(cpu_usage),
            "avg_memory_usage_mb": statistics.mean(memory_usage),
            "peak_memory_usage_mb": max(memory_usage),
            "avg_system_memory_percent": statistics.mean(system_memory),
            "max_system_memory_percent": max(system_memory),
            "monitoring_duration": len(self.resource_data) * 0.1,
            "total_samples": len(self.resource_data)
        }

class ParallelPerformanceTestSuite:
    """å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.test_results = []
        self.resource_monitor = SystemResourceMonitor()
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            "max_concurrent_levels": [1, 2, 4, 8, 16, 32],
            "test_iterations": 5,
            "warmup_iterations": 2,
            "timeout_seconds": 300,
            "task_complexity_levels": ["simple", "moderate", "complex", "expert"]
        }
        
        # æ€§èƒ½åŸºå‡†
        self.performance_baseline = {}
        
        logger.info("å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•å¥—ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    async def run_comprehensive_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„æ€§èƒ½æµ‹è¯•"""
        print("=" * 90)
        print("ğŸš€ Aé¡¹ç›®å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•å¥—ä»¶ V7")
        print("=" * 90)
        
        try:
            # 1. å¹¶è¡ŒåŠ é€Ÿæ¯”æµ‹è¯•
            await self._test_parallel_speedup()
            
            # 2. å¯æ‰©å±•æ€§æµ‹è¯•
            await self._test_scalability()
            
            # 3. ååé‡æµ‹è¯•
            await self._test_throughput()
            
            # 4. èµ„æºæ•ˆç‡æµ‹è¯•
            await self._test_resource_efficiency()
            
            # 5. è´Ÿè½½å‡è¡¡æµ‹è¯•
            await self._test_load_balancing()
            
            # 6. ç¼“å­˜æ€§èƒ½å½±å“æµ‹è¯•
            await self._test_cache_performance_impact()
            
        except Exception as e:
            logger.error(f"æ€§èƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            raise
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        return self._generate_performance_report()
    
    async def _test_parallel_speedup(self):
        """æµ‹è¯•å¹¶è¡ŒåŠ é€Ÿæ¯”"""
        print("\nğŸ“Š æµ‹è¯•å¹¶è¡ŒåŠ é€Ÿæ¯”...")
        
        for complexity in self.test_config["task_complexity_levels"]:
            print(f"\n   ğŸ“ˆ æµ‹è¯•å¤æ‚åº¦: {complexity}")
            
            for concurrency in self.test_config["max_concurrent_levels"]:
                if concurrency == 1:
                    # åŸºå‡†æµ‹è¯•ï¼ˆä¸²è¡Œï¼‰
                    baseline_result = await self._run_serial_baseline_test(complexity)
                    self.test_results.append(baseline_result)
                else:
                    # å¹¶è¡Œæµ‹è¯•
                    parallel_result = await self._run_parallel_test(complexity, concurrency)
                    self.test_results.append(parallel_result)
                    
                    # è®¡ç®—åŠ é€Ÿæ¯”
                    if baseline_result.execution_time > 0:
                        speedup = baseline_result.execution_time / parallel_result.execution_time
                        efficiency = speedup / concurrency
                        
                        parallel_result.speedup_ratio = speedup
                        parallel_result.efficiency = efficiency
                        parallel_result.serial_baseline = baseline_result.execution_time
                        
                        print(f"      ğŸ”§ {concurrency}å¹¶å‘: åŠ é€Ÿæ¯”={speedup:.2f}x, æ•ˆç‡={efficiency:.2%}")
    
    async def _run_serial_baseline_test(self, complexity: str) -> PerformanceTestResult:
        """è¿è¡Œä¸²è¡ŒåŸºå‡†æµ‹è¯•"""
        test_name = f"ä¸²è¡ŒåŸºå‡†æµ‹è¯•-{complexity}"
        result = PerformanceTestResult(
            test_name=test_name,
            test_type="serial_baseline",
            start_time=time.time(),
            test_config={"complexity": complexity, "concurrency": 1}
        )
        
        try:
            # åˆ›å»ºå•æ™ºèƒ½ä½“æ‰§è¡Œå™¨
            executor = ParallelAgentExecutor(max_concurrent_agents=1, enable_cache=False)
            
            # åˆ›å»ºä¸²è¡Œä»»åŠ¡
            subtasks = self._generate_test_subtasks(complexity, 5)
            
            # æ‰§è¡Œä»»åŠ¡
            self.resource_monitor.start_monitoring()
            
            serial_result = await executor.execute_parallel_task(
                task_description=f"ä¸²è¡ŒåŸºå‡†æµ‹è¯•-{complexity}",
                expert_assignments={"ä¸“å®¶1": AgentRole.SPECIALIST},
                subtasks=[subtasks[0]]  # åªç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡
            )
            
            self.resource_monitor.stop_monitoring()
            
            result.end_time = time.time()
            result.execution_time = result.end_time - result.start_time
            result.success = serial_result.success
            result.resource_utilization = serial_result.resource_usage
            result.resource_utilization.update(self.resource_monitor.get_resource_summary())
            result.quality_score = serial_result.quality_score
            
        except Exception as e:
            result.error = str(e)
        
        return result
    
    async def _run_parallel_test(self, complexity: str, concurrency: int) -> PerformanceTestResult:
        """è¿è¡Œå¹¶è¡Œæµ‹è¯•"""
        test_name = f"å¹¶è¡Œæµ‹è¯•-{complexity}-{concurrency}å¹¶å‘"
        result = PerformanceTestResult(
            test_name=test_name,
            test_type="parallel",
            start_time=time.time(),
            test_config={"complexity": complexity, "concurrency": concurrency}
        )
        
        try:
            # åˆ›å»ºå¹¶è¡Œæ‰§è¡Œå™¨
            executor = ParallelAgentExecutor(max_concurrent_agents=concurrency, enable_cache=False)
            
            # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
            subtasks = self._generate_test_subtasks(complexity, concurrency * 2)
            
            # åˆ†é…ä¸“å®¶
            expert_assignments = {f"ä¸“å®¶{i}": AgentRole.SPECIALIST for i in range(concurrency)}
            
            # è½¬æ¢å­ä»»åŠ¡æ ¼å¼
            executor_subtasks = []
            for i, subtask in enumerate(subtasks[:concurrency * 2]):
                executor_subtasks.append({
                    "description": subtask.subtask_description,
                    "preferred_agent": f"ä¸“å®¶{i % concurrency}",
                    "role": "SPECIALIST",
                    "priority": subtask.priority,
                    "dependencies": [],
                    "estimated_duration": subtask.estimated_duration * 0.5  # å‡å°‘å•ä¸ªä»»åŠ¡æ—¶é—´
                })
            
            # æ‰§è¡Œå¹¶è¡Œä»»åŠ¡
            self.resource_monitor.start_monitoring()
            
            parallel_result = await executor.execute_parallel_task(
                task_description=f"å¹¶è¡Œæµ‹è¯•-{complexity}-{concurrency}å¹¶å‘",
                expert_assignments=expert_assignments,
                subtasks=executor_subtasks
            )
            
            self.resource_monitor.stop_monitoring()
            
            result.end_time = time.time()
            result.execution_time = result.end_time - result.start_time
            result.success = parallel_result.success
            result.resource_utilization = parallel_result.resource_usage
            result.resource_utilization.update(self.resource_monitor.get_resource_summary())
            result.quality_score = parallel_result.quality_score
            
        except Exception as e:
            result.error = str(e)
        
        return result
    
    async def _test_scalability(self):
        """æµ‹è¯•å¯æ‰©å±•æ€§"""
        print("\nğŸ“ˆ æµ‹è¯•å¯æ‰©å±•æ€§...")
        
        scalability_results = []
        
        for concurrency in self.test_config["max_concurrent_levels"]:
            if concurrency == 1:
                continue  # è·³è¿‡ä¸²è¡Œæµ‹è¯•
            
            # è¿è¡Œå¯æ‰©å±•æ€§æµ‹è¯•
            test_name = f"å¯æ‰©å±•æ€§æµ‹è¯•-{concurrency}å¹¶å‘"
            result = PerformanceTestResult(
                test_name=test_name,
                test_type="scalability",
                start_time=time.time(),
                test_config={"concurrency": concurrency}
            )
            
            try:
                # åˆ›å»ºå¤§é‡ä»»åŠ¡æµ‹è¯•å¯æ‰©å±•æ€§
                executor = ParallelAgentExecutor(max_concurrent_agents=concurrency, enable_cache=False)
                
                # ç”Ÿæˆå¤§é‡å­ä»»åŠ¡
                subtasks = self._generate_test_subtasks("complex", concurrency * 5)
                
                # åˆ†é…ä¸“å®¶
                expert_assignments = {f"ä¸“å®¶{i}": AgentRole.SPECIALIST for i in range(min(concurrency, 16))}
                
                # è½¬æ¢ä»»åŠ¡æ ¼å¼
                executor_subtasks = []
                for i, subtask in enumerate(subtasks[:concurrency * 3]):
                    executor_subtasks.append({
                        "description": f"å¯æ‰©å±•æ€§ä»»åŠ¡{i}: {subtask.subtask_description}",
                        "preferred_agent": f"ä¸“å®¶{i % len(expert_assignments)}",
                        "role": "SPECIALIST",
                        "priority": subtask.priority,
                        "dependencies": [],
                        "estimated_duration": subtask.estimated_duration * 0.3
                    })
                
                # æ‰§è¡Œæµ‹è¯•
                start_time = time.time()
                scalability_result = await executor.execute_parallel_task(
                    task_description=f"å¯æ‰©å±•æ€§æµ‹è¯•-{concurrency}å¹¶å‘",
                    expert_assignments=expert_assignments,
                    subtasks=executor_subtasks
                )
                execution_time = time.time() - start_time
                
                # è®¡ç®—ååé‡
                throughput = len(executor_subtasks) / execution_time if execution_time > 0 else 0
                
                result.end_time = time.time()
                result.execution_time = execution_time
                result.success = scalability_result.success
                result.throughput = throughput
                result.resource_utilization = scalability_result.resource_usage
                result.test_config.update({
                    "tasks_count": len(executor_subtasks),
                    "agents_count": len(expert_assignments)
                })
                
                scalability_results.append(result)
                
                print(f"      ğŸ“Š {concurrency}å¹¶å‘: ååé‡={throughput:.2f}ä»»åŠ¡/ç§’, æˆåŠŸç‡={scalability_result.success}")
                
            except Exception as e:
                result.error = str(e)
                scalability_results.append(result)
        
        self.test_results.extend(scalability_results)
    
    async def _test_throughput(self):
        """æµ‹è¯•ååé‡"""
        print("\nğŸ”„ æµ‹è¯•ååé‡...")
        
        # é«˜è´Ÿè½½ååé‡æµ‹è¯•
        test_name = "é«˜è´Ÿè½½ååé‡æµ‹è¯•"
        result = PerformanceTestResult(
            test_name=test_name,
            test_type="throughput",
            start_time=time.time(),
            test_config={"load_type": "high", "duration": "60s"}
        )
        
        try:
            # åˆ›å»ºé«˜å¹¶å‘æµ‹è¯•
            executor = ParallelAgentExecutor(max_concurrent_agents=32, enable_cache=True)
            
            # ç”Ÿæˆå¤§é‡çŸ­ä»»åŠ¡
            subtasks = self._generate_test_subtasks("simple", 100)
            
            expert_assignments = {f"ä¸“å®¶{i}": AgentRole.SPECIALIST for i in range(16)}
            
            executor_subtasks = []
            for i, subtask in enumerate(subtasks[:80]):
                executor_subtasks.append({
                    "description": f"ååé‡ä»»åŠ¡{i}: {subtask.subtask_description}",
                    "preferred_agent": f"ä¸“å®¶{i % 16}",
                    "role": "SPECIALIST",
                    "priority": 1,
                    "dependencies": [],
                    "estimated_duration": 0.1  # å¾ˆçŸ­çš„ä»»åŠ¡
                })
            
            # æ‰§è¡Œé«˜ååé‡æµ‹è¯•
            start_time = time.time()
            throughput_result = await executor.execute_parallel_task(
                task_description="é«˜è´Ÿè½½ååé‡æµ‹è¯•",
                expert_assignments=expert_assignments,
                subtasks=executor_subtasks
            )
            execution_time = time.time() - start_time
            
            # è®¡ç®—ååé‡æŒ‡æ ‡
            total_tasks = len(executor_subtasks)
            completed_tasks = len([r for r in throughput_result.subtask_results.values() 
                                 if r.status.value == "completed"])
            throughput = total_tasks / execution_time if execution_time > 0 else 0
            completion_rate = completed_tasks / total_tasks if total_tasks > 0 else 0
            
            result.end_time = time.time()
            result.execution_time = execution_time
            result.success = throughput_result.success
            result.throughput = throughput
            result.accuracy = completion_rate
            result.resource_utilization = throughput_result.resource_usage
            
            print(f"      âš¡ ååé‡: {throughput:.2f}ä»»åŠ¡/ç§’, å®Œæˆç‡: {completion_rate:.2%}")
            
        except Exception as e:
            result.error = str(e)
        
        self.test_results.append(result)
    
    async def _test_resource_efficiency(self):
        """æµ‹è¯•èµ„æºæ•ˆç‡"""
        print("\nâš¡ æµ‹è¯•èµ„æºæ•ˆç‡...")
        
        efficiency_tests = []
        
        for concurrency in [4, 8, 16]:
            test_name = f"èµ„æºæ•ˆç‡æµ‹è¯•-{concurrency}å¹¶å‘"
            result = PerformanceTestResult(
                test_name=test_name,
                test_type="resource_efficiency",
                start_time=time.time(),
                test_config={"concurrency": concurrency}
            )
            
            try:
                # åˆ›å»ºèµ„æºæ•ˆç‡æµ‹è¯•
                executor = ParallelAgentExecutor(max_concurrent_agents=concurrency, enable_cache=False)
                
                # åˆ›å»ºä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡
                subtasks = self._generate_test_subtasks("moderate", concurrency * 3)
                
                expert_assignments = {f"ä¸“å®¶{i}": AgentRole.SPECIALIST for i in range(concurrency)}
                
                executor_subtasks = []
                for i, subtask in enumerate(subtasks[:concurrency * 2]):
                    executor_subtasks.append({
                        "description": f"æ•ˆç‡ä»»åŠ¡{i}: {subtask.subtask_description}",
                        "preferred_agent": f"ä¸“å®¶{i % concurrency}",
                        "role": "SPECIALIST",
                        "priority": subtask.priority,
                        "dependencies": [],
                        "estimated_duration": subtask.estimated_duration
                    })
                
                # ç›‘æ§èµ„æºä½¿ç”¨
                self.resource_monitor.start_monitoring()
                
                efficiency_result = await executor.execute_parallel_task(
                    task_description=f"èµ„æºæ•ˆç‡æµ‹è¯•-{concurrency}å¹¶å‘",
                    expert_assignments=expert_assignments,
                    subtasks=executor_subtasks
                )
                
                self.resource_monitor.stop_monitoring()
                
                resource_summary = self.resource_monitor.get_resource_summary()
                
                # è®¡ç®—èµ„æºæ•ˆç‡
                total_work = len(executor_subtasks)
                total_time = time.time() - result.start_time
                work_per_cpu_second = total_work / (resource_summary.get("avg_cpu_usage", 1) * total_time)
                memory_efficiency = total_work / resource_summary.get("peak_memory_usage_mb", 1)
                
                result.end_time = time.time()
                result.execution_time = total_time
                result.success = efficiency_result.success
                result.resource_utilization = resource_summary
                result.additional_metrics = {
                    "work_per_cpu_second": work_per_cpu_second,
                    "memory_efficiency": memory_efficiency,
                    "cpu_utilization": resource_summary.get("avg_cpu_usage"),
                    "memory_utilization": resource_summary.get("peak_memory_usage_mb")
                }
                
                efficiency_tests.append(result)
                
                print(f"      ğŸ’¡ {concurrency}å¹¶å‘: å·¥ä½œæ•ˆç‡={work_per_cpu_second:.2f}, å†…å­˜æ•ˆç‡={memory_efficiency:.2f}")
                
            except Exception as e:
                result.error = str(e)
                efficiency_tests.append(result)
        
        self.test_results.extend(efficiency_tests)
    
    async def _test_load_balancing(self):
        """æµ‹è¯•è´Ÿè½½å‡è¡¡"""
        print("\nâš–ï¸ æµ‹è¯•è´Ÿè½½å‡è¡¡...")
        
        test_name = "è´Ÿè½½å‡è¡¡æµ‹è¯•"
        result = PerformanceTestResult(
            test_name=test_name,
            test_type="load_balancing",
            start_time=time.time(),
            test_config={"agents": 8, "uneven_load": True}
        )
        
        try:
            # åˆ›å»ºè´Ÿè½½ä¸å‡è¡¡çš„æµ‹è¯•åœºæ™¯
            executor = ParallelAgentExecutor(max_concurrent_agents=8, enable_cache=False)
            
            # åˆ›å»ºä¸å‡åŒ€çš„ä»»åŠ¡è´Ÿè½½
            subtasks = []
            
            # ä¸€äº›é‡ä»»åŠ¡
            for i in range(4):
                subtasks.append({
                    "description": f"é‡ä»»åŠ¡{i}: å¤æ‚çš„æ•°æ®å¤„ç†",
                    "preferred_agent": f"ä¸“å®¶{i}",
                    "role": "SPECIALIST",
                    "priority": 1,
                    "dependencies": [],
                    "estimated_duration": 2.0
                })
            
            # ä¸€äº›è½»ä»»åŠ¡
            for i in range(4, 12):
                subtasks.append({
                    "description": f"è½»ä»»åŠ¡{i}: ç®€å•çš„æ•°æ®å¤„ç†",
                    "preferred_agent": f"ä¸“å®¶{i % 8}",
                    "role": "SPECIALIST",
                    "priority": 2,
                    "dependencies": [],
                    "estimated_duration": 0.2
                })
            
            expert_assignments = {f"ä¸“å®¶{i}": AgentRole.SPECIALIST for i in range(8)}
            
            # æ‰§è¡Œè´Ÿè½½å‡è¡¡æµ‹è¯•
            start_time = time.time()
            balancing_result = await executor.execute_parallel_task(
                task_description="è´Ÿè½½å‡è¡¡æµ‹è¯•",
                expert_assignments=expert_assignments,
                subtasks=subtasks
            )
            execution_time = time.time() - start_time
            
            # åˆ†æè´Ÿè½½åˆ†å¸ƒ
            agent_workload = {}
            for task_id, task_result in balancing_result.subtask_results.items():
                # è¿™é‡Œåº”è¯¥ä»å®é™…æ‰§è¡Œç»“æœä¸­æå–æ¯ä¸ªæ™ºèƒ½ä½“çš„å·¥ä½œé‡
                # ç®€åŒ–å®ç°
                agent_name = task_result.assigned_agent if hasattr(task_result, 'assigned_agent') else "unknown"
                if agent_name not in agent_workload:
                    agent_workload[agent_name] = 0
                agent_workload[agent_name] += 1
            
            # è®¡ç®—è´Ÿè½½å‡è¡¡åº¦
            if agent_workload:
                workload_values = list(agent_workload.values())
                workload_std = statistics.stdev(workload_values) if len(workload_values) > 1 else 0
                workload_mean = statistics.mean(workload_values)
                balance_score = 1 / (1 + workload_std / workload_mean) if workload_mean > 0 else 0
            else:
                balance_score = 0
            
            result.end_time = time.time()
            result.execution_time = execution_time
            result.success = balancing_result.success
            result.consistency_score = balance_score
            result.additional_metrics = {
                "agent_workload_distribution": agent_workload,
                "workload_balance_score": balance_score,
                "avg_execution_time": balancing_result.overall_duration if hasattr(balancing_result, 'overall_duration') else execution_time
            }
            
            print(f"      ğŸ¯ è´Ÿè½½å‡è¡¡åº¦: {balance_score:.2f}")
            
        except Exception as e:
            result.error = str(e)
        
        self.test_results.append(result)
    
    async def _test_cache_performance_impact(self):
        """æµ‹è¯•ç¼“å­˜å¯¹æ€§èƒ½çš„å½±å“"""
        print("\nğŸ’¾ æµ‹è¯•ç¼“å­˜æ€§èƒ½å½±å“...")
        
        # å¯¹æ¯”æœ‰ç¼“å­˜å’Œæ— ç¼“å­˜çš„æ€§èƒ½
        cache_test_results = []
        
        for cache_enabled in [False, True]:
            test_name = f"ç¼“å­˜æ€§èƒ½æµ‹è¯•-{'å¯ç”¨' if cache_enabled else 'ç¦ç”¨'}"
            result = PerformanceTestResult(
                test_name=test_name,
                test_type="cache_performance",
                start_time=time.time(),
                test_config={"cache_enabled": cache_enabled}
            )
            
            try:
                # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
                executor = ParallelAgentExecutor(max_concurrent_agents=8, enable_cache=cache_enabled)
                
                # ç¬¬ä¸€è½®ï¼šå†·å¯åŠ¨
                subtasks1 = self._generate_test_subtasks("moderate", 10)
                executor_subtasks1 = []
                for i, subtask in enumerate(subtasks1[:8]):
                    executor_subtasks1.append({
                        "description": f"ç¼“å­˜æµ‹è¯•ä»»åŠ¡{i}: {subtask.subtask_description}",
                        "preferred_agent": f"ä¸“å®¶{i % 4}",
                        "role": "SPECIALIST",
                        "priority": subtask.priority,
                        "dependencies": [],
                        "estimated_duration": subtask.estimated_duration * 0.5
                    })
                
                start_time = time.time()
                result1 = await executor.execute_parallel_task(
                    task_description=f"ç¼“å­˜æµ‹è¯•-ç¬¬ä¸€è½®",
                    expert_assignments={f"ä¸“å®¶{i}": AgentRole.SPECIALIST for i in range(4)},
                    subtasks=executor_subtasks1
                )
                first_run_time = time.time() - start_time
                
                # ç¬¬äºŒè½®ï¼šå¯èƒ½å‘½ä¸­ç¼“å­˜
                subtasks2 = self._generate_test_subtasks("moderate", 10)
                executor_subtasks2 = []
                for i, subtask in enumerate(subtasks2[:8]):
                    executor_subtasks2.append({
                        "description": f"ç¼“å­˜æµ‹è¯•ä»»åŠ¡{i}: {subtask.subtask_description}",
                        "preferred_agent": f"ä¸“å®¶{i % 4}",
                        "role": "SPECIALIST",
                        "priority": subtask.priority,
                        "dependencies": [],
                        "estimated_duration": subtask.estimated_duration * 0.5
                    })
                
                start_time = time.time()
                result2 = await executor.execute_parallel_task(
                    task_description=f"ç¼“å­˜æµ‹è¯•-ç¬¬äºŒè½®",
                    expert_assignments={f"ä¸“å®¶{i}": AgentRole.SPECIALIST for i in range(4)},
                    subtasks=executor_subtasks2
                )
                second_run_time = time.time() - start_time
                
                # è®¡ç®—ç¼“å­˜æ•ˆæœ
                speedup_ratio = first_run_time / second_run_time if second_run_time > 0 else 1.0
                
                result.end_time = time.time()
                result.execution_time = first_run_time + second_run_time
                result.success = result1.success and result2.success
                result.speedup_ratio = speedup_ratio
                result.additional_metrics = {
                    "first_run_time": first_run_time,
                    "second_run_time": second_run_time,
                    "cache_speedup": speedup_ratio,
                    "cache_hit_rate": getattr(executor.cache, 'get_cache_statistics', lambda: {'cache_hit_rate': 0})()['cache_hit_rate'] if cache_enabled and executor.cache else 0
                }
                
                cache_test_results.append(result)
                
                print(f"      ğŸ“Š ç¼“å­˜{'å¯ç”¨' if cache_enabled else 'ç¦ç”¨'}: åŠ é€Ÿæ¯”={speedup_ratio:.2f}x")
                
            except Exception as e:
                result.error = str(e)
                cache_test_results.append(result)
        
        self.test_results.extend(cache_test_results)
    
    def _generate_test_subtasks(self, complexity: str, count: int) -> List:
        """ç”Ÿæˆæµ‹è¯•å­ä»»åŠ¡"""
        decomposer = TaskDecomposer()
        
        # åˆ›å»ºæµ‹è¯•ä»»åŠ¡æè¿°
        complexity_tasks = {
            "simple": "ç®€å•çš„æ•°æ®å¤„ç†ä»»åŠ¡",
            "moderate": "ä¸­ç­‰å¤æ‚åº¦çš„ç®—æ³•å®ç°",
            "complex": "å¤æ‚çš„ç³»ç»Ÿæ¶æ„è®¾è®¡",
            "expert": "ä¸“å®¶çº§åˆ«çš„æ€§èƒ½ä¼˜åŒ–ä»»åŠ¡"
        }
        
        task_description = f"{complexity_tasks.get(complexity, 'æµ‹è¯•ä»»åŠ¡')} (åŒ…å«{count}ä¸ªå­ä»»åŠ¡)"
        
        # åˆ†è§£ä»»åŠ¡
        subtasks = decomposer.decompose_task(
            original_task=task_description,
            domain="æ€§èƒ½æµ‹è¯•",
            max_subtasks=count
        )
        
        return subtasks
    
    def _generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n" + "=" * 90)
        print("ğŸ“Š Aé¡¹ç›®å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("=" * 90)
        
        # åˆ†ææµ‹è¯•ç»“æœ
        parallel_tests = [r for r in self.test_results if r.test_type == "parallel"]
        scalability_tests = [r for r in self.test_results if r.test_type == "scalability"]
        throughput_tests = [r for r in self.test_results if r.test_type == "throughput"]
        efficiency_tests = [r for r in self.test_results if r.test_type == "resource_efficiency"]
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        performance_summary = {
            "parallel_performance": self._analyze_parallel_performance(parallel_tests),
            "scalability_analysis": self._analyze_scalability(scalability_tests),
            "throughput_analysis": self._analyze_throughput(throughput_tests),
            "efficiency_analysis": self._analyze_efficiency(efficiency_tests),
            "overall_assessment": self._generate_overall_assessment()
        }
        
        # æ‰“å°æ€§èƒ½æ‘˜è¦
        print(f"\nğŸ“ˆ æ€§èƒ½æ‘˜è¦:")
        print(f"   ğŸš€ æœ€é«˜åŠ é€Ÿæ¯”: {performance_summary['parallel_performance']['max_speedup']:.2f}x")
        print(f"   âš¡ æœ€é«˜ååé‡: {performance_summary['throughput_analysis']['max_throughput']:.2f}ä»»åŠ¡/ç§’")
        print(f"   ğŸ’¡ æœ€ä½³æ•ˆç‡: {performance_summary['efficiency_analysis']['max_efficiency']:.2%}")
        print(f"   ğŸ“Š å¹³å‡åŠ é€Ÿæ¯”: {performance_summary['parallel_performance']['avg_speedup']:.2f}x")
        
        # æ‰“å°å»ºè®®
        recommendations = performance_summary['overall_assessment']['recommendations']
        if recommendations:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations[:3], 1):
                print(f"   {i}. {rec}")
        
        print("\n" + "=" * 90)
        print("âœ… å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•å®Œæˆ")
        print("=" * 90)
        
        return performance_summary
    
    def _analyze_parallel_performance(self, parallel_tests: List[PerformanceTestResult]) -> Dict[str, Any]:
        """åˆ†æå¹¶è¡Œæ€§èƒ½"""
        if not parallel_tests:
            return {"max_speedup": 0, "avg_speedup": 0, "max_efficiency": 0, "avg_efficiency": 0}
        
        speedups = [r.speedup_ratio for r in parallel_tests if r.speedup_ratio]
        efficiencies = [r.efficiency for r in parallel_tests if r.efficiency]
        
        return {
            "max_speedup": max(speedups) if speedups else 0,
            "avg_speedup": statistics.mean(speedups) if speedups else 0,
            "max_efficiency": max(efficiencies) if efficiencies else 0,
            "avg_efficiency": statistics.mean(efficiencies) if efficiencies else 0,
            "test_count": len(parallel_tests)
        }
    
    def _analyze_scalability(self, scalability_tests: List[PerformanceTestResult]) -> Dict[str, Any]:
        """åˆ†æå¯æ‰©å±•æ€§"""
        if not scalability_tests:
            return {"max_throughput": 0, "scalability_score": 0, "concurrency_levels": []}
        
        throughputs = [(r.test_config.get("concurrency", 0), r.throughput or 0) for r in scalability_tests]
        throughputs.sort()
        
        max_throughput = max([t[1] for t in throughputs]) if throughputs else 0
        
        # è®¡ç®—å¯æ‰©å±•æ€§åˆ†æ•°ï¼ˆåŸºäºååé‡å¢é•¿è¶‹åŠ¿ï¼‰
        if len(throughputs) >= 2:
            # ç®€å•çš„çº¿æ€§è¶‹åŠ¿åˆ†æ
            x = [t[0] for t in throughputs]
            y = [t[1] for t in throughputs]
            if len(x) > 1:
                correlation = np.corrcoef(x, y)[0, 1] if len(x) > 1 else 0
                scalability_score = max(0, correlation)  # åªå–æ­£ç›¸å…³
            else:
                scalability_score = 0
        else:
            scalability_score = 0
        
        return {
            "max_throughput": max_throughput,
            "scalability_score": scalability_score,
            "concurrency_levels": throughputs
        }
    
    def _analyze_throughput(self, throughput_tests: List[PerformanceTestResult]) -> Dict[str, Any]:
        """åˆ†æååé‡"""
        if not throughput_tests:
            return {"max_throughput": 0, "avg_throughput": 0, "success_rate": 0}
        
        throughputs = [r.throughput for r in throughput_tests if r.throughput]
        success_rates = [1 if r.success else 0 for r in throughput_tests]
        
        return {
            "max_throughput": max(throughputs) if throughputs else 0,
            "avg_throughput": statistics.mean(throughputs) if throughputs else 0,
            "success_rate": statistics.mean(success_rates) if success_rates else 0
        }
    
    def _analyze_efficiency(self, efficiency_tests: List[PerformanceTestResult]) -> Dict[str, Any]:
        """åˆ†æèµ„æºæ•ˆç‡"""
        if not efficiency_tests:
            return {"max_efficiency": 0, "avg_efficiency": 0, "resource_optimization": {}}
        
        efficiencies = []
        work_per_cpu = []
        memory_efficiency = []
        
        for test in efficiency_tests:
            if test.additional_metrics:
                if "work_per_cpu_second" in test.additional_metrics:
                    work_per_cpu.append(test.additional_metrics["work_per_cpu_second"])
                if "memory_efficiency" in test.additional_metrics:
                    memory_efficiency.append(test.additional_metrics["memory_efficiency"])
        
        avg_work_per_cpu = statistics.mean(work_per_cpu) if work_per_cpu else 0
        avg_memory_efficiency = statistics.mean(memory_efficiency) if memory_efficiency else 0
        
        return {
            "max_efficiency": 0,  # éœ€è¦å…·ä½“è®¡ç®—
            "avg_efficiency": 0,
            "resource_optimization": {
                "avg_work_per_cpu": avg_work_per_cpu,
                "avg_memory_efficiency": avg_memory_efficiency
            }
        }
    
    def _generate_overall_assessment(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€»ä½“è¯„ä¼°"""
        recommendations = []
        
        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        parallel_perf = self._analyze_parallel_performance([r for r in self.test_results if r.test_type == "parallel"])
        
        if parallel_perf["max_speedup"] < 2.0:
            recommendations.append("å¹¶è¡ŒåŠ é€Ÿæ¯”åä½ï¼Œå»ºè®®ä¼˜åŒ–ä»»åŠ¡åˆ†è§£ç­–ç•¥")
        
        if parallel_perf["avg_efficiency"] < 0.5:
            recommendations.append("å¹¶è¡Œæ•ˆç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–è´Ÿè½½å‡è¡¡ç®—æ³•")
        
        scalability_perf = self._analyze_scalability([r for r in self.test_results if r.test_type == "scalability"])
        if scalability_perf["scalability_score"] < 0.7:
            recommendations.append("å¯æ‰©å±•æ€§ä¸è¶³ï¼Œå»ºè®®ä¼˜åŒ–èµ„æºç®¡ç†ç­–ç•¥")
        
        if not recommendations:
            recommendations.append("æ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼Œç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼")
        
        return {
            "overall_score": min(10.0, max(1.0, parallel_perf["avg_speedup"] * 0.3 + 
                                          scalability_perf["scalability_score"] * 0.3 +
                                          parallel_perf["avg_efficiency"] * 0.4)),
            "recommendations": recommendations
        }

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    performance_test_suite = ParallelPerformanceTestSuite()
    report = await performance_test_suite.run_comprehensive_performance_tests()
    
    # ä¿å­˜æ€§èƒ½æŠ¥å‘Š
    report_file = PROJECT_ROOT / "iflow" / "tests" / "reports" / f"parallel_performance_report_{int(time.time())}.json"
    report_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ“„ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    asyncio.run(main())