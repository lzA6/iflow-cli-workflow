#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQæ¨ç†å¼•æ“æ€§èƒ½æµ‹è¯•è„šæœ¬
è¯„ä¼°å½“å‰ARQ V2.0å¼•æ“çš„æ€§èƒ½è¡¨ç°ï¼Œè¯†åˆ«ç“¶é¢ˆå¹¶æä¾›ä¼˜åŒ–å»ºè®®
"""

import time
import json
import asyncio
import psutil
import os
from pathlib import Path
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
import sys
sys.path.insert(0, str(PROJECT_ROOT))

try:
    from iflow.core.arq_v2_enhanced_engine import ARQV2EnhancedEngine, ReasoningMode, ProblemType
except ImportError as e:
    print(f"æ— æ³•å¯¼å…¥ARQå¼•æ“: {e}")
    exit(1)

class ARQPerformanceTester:
    """ARQæ¨ç†å¼•æ“æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.engine = None
        self.test_results = []
        self.memory_usage = []
        self.cpu_usage = []
        
    async def initialize_engine(self):
        """åˆå§‹åŒ–ARQå¼•æ“"""
        print("ğŸš€ åˆå§‹åŒ–ARQæ¨ç†å¼•æ“...")
        try:
            self.engine = ARQV2EnhancedEngine()
            print("âœ… ARQå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ ARQå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def generate_test_tasks(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæµ‹è¯•ä»»åŠ¡"""
        test_tasks = [
            {
                "task": "è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿæ¶æ„",
                "complexity": "high",
                "context": [{"type": "project_info", "content": "éœ€è¦æ”¯æŒé«˜å¹¶å‘è¯»å†™"}]
            },
            {
                "task": "åˆ†æç°æœ‰ä»£ç çš„æ€§èƒ½ç“¶é¢ˆ",
                "complexity": "medium", 
                "context": [{"type": "code_analysis", "content": "éœ€è¦ä¼˜åŒ–æ€§èƒ½"}]
            },
            {
                "task": "åˆ›å»ºä¸€ä¸ªç®€å•çš„ç”¨æˆ·è®¤è¯ç³»ç»Ÿ",
                "complexity": "low",
                "context": [{"type": "security", "content": "éœ€è¦åŸºæœ¬è®¤è¯åŠŸèƒ½"}]
            }
        ]
        return test_tasks
    
    async def test_single_task(self, task_data: Dict[str, Any], task_id: int) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªä»»åŠ¡çš„æ€§èƒ½"""
        print(f"\nğŸ§ª æµ‹è¯•ä»»åŠ¡ {task_id + 1}: {task_data['task'][:30]}...")
        
        # è®°å½•åˆå§‹èµ„æºä½¿ç”¨
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        initial_cpu = process.cpu_percent()
        
        start_time = time.time()
        
        try:
            # æ‰§è¡ŒARQæ¨ç†
            result = await self.engine.process_enhanced_reasoning(
                task=task_data["task"],
                context=task_data["context"],
                reasoning_mode=ReasoningMode.STRUCTURED,
                problem_type=ProblemType.ARCHITECTURE
            )
            
            end_time = time.time()
            
            # è®°å½•èµ„æºä½¿ç”¨
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            final_cpu = process.cpu_percent()
            
            execution_time = end_time - start_time
            memory_increase = final_memory - initial_memory
            
            # æ„å»ºæµ‹è¯•ç»“æœ
            test_result = {
                "task_id": task_id,
                "task_description": task_data["task"],
                "complexity": task_data["complexity"],
                "execution_time": execution_time,
                "success": result["success"],
                "compliance_score": result.get("compliance_score", 0),
                "confidence_score": result.get("confidence_score", 0),
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "memory_increase_mb": memory_increase,
                "cpu_usage": final_cpu,
                "error": result.get("error", "")
            }
            
            self.test_results.append(test_result)
            
            # è¾“å‡ºç»“æœ
            print(f"  âœ… æ‰§è¡Œç»“æœ: {'æˆåŠŸ' if result['success'] else 'å¤±è´¥'}")
            print(f"  â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")
            print(f"  ğŸ“Š åˆè§„åˆ†æ•°: {result.get('compliance_score', 0):.2f}")
            print(f"  ğŸ¯ ç½®ä¿¡åº¦: {result.get('confidence_score', 0):.2f}")
            print(f"  ğŸ’¾ å†…å­˜å¢é•¿: {memory_increase:.2f}MB")
            
            return test_result
            
        except Exception as e:
            print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
            return {
                "task_id": task_id,
                "task_description": task_data["task"],
                "complexity": task_data["complexity"],
                "execution_time": 0,
                "success": False,
                "error": str(e)
            }
    
    async def run_performance_test(self, num_iterations: int = 5):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print(f"\nğŸš€ å¼€å§‹ARQæ¨ç†å¼•æ“æ€§èƒ½æµ‹è¯• (è¿­ä»£æ¬¡æ•°: {num_iterations})")
        print("=" * 60)
        
        if not await self.initialize_engine():
            return False
        
        test_tasks = self.generate_test_tasks()
        
        # å¤šæ¬¡è¿­ä»£æµ‹è¯•
        for iteration in range(num_iterations):
            print(f"\nğŸ”„ è¿­ä»£ {iteration + 1}/{num_iterations}")
            print("-" * 40)
            
            for i, task in enumerate(test_tasks):
                result = await self.test_single_task(task, i)
                
                # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…èµ„æºç«äº‰
                await asyncio.sleep(0.5)
        
        return True
    
    def analyze_results(self) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        if not self.test_results:
            return {}
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        execution_times = [r["execution_time"] for r in self.test_results if r["success"]]
        compliance_scores = [r["compliance_score"] for r in self.test_results if r["success"]]
        confidence_scores = [r["confidence_score"] for r in self.test_results if r["success"]]
        memory_increases = [r["memory_increase_mb"] for r in self.test_results]
        
        analysis = {
            "total_tasks": len(self.test_results),
            "successful_tasks": sum(1 for r in self.test_results if r["success"]),
            "success_rate": sum(1 for r in self.test_results if r["success"]) / len(self.test_results),
            "avg_execution_time": sum(execution_times) / len(execution_times) if execution_times else 0,
            "min_execution_time": min(execution_times) if execution_times else 0,
            "max_execution_time": max(execution_times) if execution_times else 0,
            "avg_compliance_score": sum(compliance_scores) / len(compliance_scores) if compliance_scores else 0,
            "avg_confidence_score": sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0,
            "avg_memory_increase": sum(memory_increases) / len(memory_increases) if memory_increases else 0,
            "max_memory_increase": max(memory_increases) if memory_increases else 0
        }
        
        return analysis
    
    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        analysis = self.analyze_results()
        
        report = f"""
ARQæ¨ç†å¼•æ“æ€§èƒ½æµ‹è¯•æŠ¥å‘Š
{'=' * 60}

ğŸ“Š åŸºæœ¬æŒ‡æ ‡:
- æ€»ä»»åŠ¡æ•°: {analysis.get('total_tasks', 0)}
- æˆåŠŸä»»åŠ¡æ•°: {analysis.get('successful_tasks', 0)}
- æˆåŠŸç‡: {analysis.get('success_rate', 0):.2%}

â±ï¸ æ‰§è¡Œæ€§èƒ½:
- å¹³å‡æ‰§è¡Œæ—¶é—´: {analysis.get('avg_execution_time', 0):.3f}ç§’
- æœ€çŸ­æ‰§è¡Œæ—¶é—´: {analysis.get('min_execution_time', 0):.3f}ç§’
- æœ€é•¿æ‰§è¡Œæ—¶é—´: {analysis.get('max_execution_time', 0):.3f}ç§’

ğŸ¯ è´¨é‡æŒ‡æ ‡:
- å¹³å‡åˆè§„åˆ†æ•°: {analysis.get('avg_compliance_score', 0):.2f}
- å¹³å‡ç½®ä¿¡åº¦: {analysis.get('avg_confidence_score', 0):.2f}

ğŸ’¾ èµ„æºä½¿ç”¨:
- å¹³å‡å†…å­˜å¢é•¿: {analysis.get('avg_memory_increase', 0):.2f}MB
- æœ€å¤§å†…å­˜å¢é•¿: {analysis.get('max_memory_increase', 0):.2f}MB

ğŸ” æ€§èƒ½è¯„ä¼°:
"""
        
        # æ€§èƒ½è¯„ä¼°
        avg_time = analysis.get('avg_execution_time', 0)
        if avg_time < 2:
            report += "- âœ… æ‰§è¡Œé€Ÿåº¦: ä¼˜ç§€ (< 2ç§’)\n"
        elif avg_time < 5:
            report += "- âš ï¸ æ‰§è¡Œé€Ÿåº¦: ä¸€èˆ¬ (2-5ç§’)\n"
        else:
            report += "- âŒ æ‰§è¡Œé€Ÿåº¦: è¾ƒæ…¢ (> 5ç§’)\n"
        
        success_rate = analysis.get('success_rate', 0)
        if success_rate > 0.95:
            report += "- âœ… ç¨³å®šæ€§: ä¼˜ç§€ (> 95%)\n"
        elif success_rate > 0.8:
            report += "- âš ï¸ ç¨³å®šæ€§: ä¸€èˆ¬ (80-95%)\n"
        else:
            report += "- âŒ ç¨³å®šæ€§: éœ€è¦æ”¹è¿› (< 80%)\n"
        
        avg_compliance = analysis.get('avg_compliance_score', 0)
        if avg_compliance > 0.9:
            report += "- âœ… åˆè§„æ€§: ä¼˜ç§€ (> 90%)\n"
        elif avg_compliance > 0.7:
            report += "- âš ï¸ åˆè§„æ€§: ä¸€èˆ¬ (70-90%)\n"
        else:
            report += "- âŒ åˆè§„æ€§: éœ€è¦æ”¹è¿› (< 70%)\n"
        
        return report
    
    def save_results(self, filename: str = "arq_performance_results.json"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        results_data = {
            "test_metadata": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_tasks": len(self.test_results),
                "engine_version": "ARQ V2.0 Enhanced"
            },
            "test_results": self.test_results,
            "analysis": self.analyze_results()
        }
        
        results_path = PROJECT_ROOT / "iflow" / "tests" / "benchmark" / filename
        results_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š æµ‹è¯•ç»“æœå·²ä¿å­˜: {results_path}")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.engine:
            await self.engine.cleanup()

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ARQæ¨ç†å¼•æ“æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ARQPerformanceTester()
    
    try:
        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        success = await tester.run_performance_test(num_iterations=3)
        
        if success:
            # ç”ŸæˆæŠ¥å‘Š
            report = tester.generate_performance_report()
            print("\n" + report)
            
            # ä¿å­˜ç»“æœ
            tester.save_results()
            
        else:
            print("âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥")
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    finally:
        # æ¸…ç†èµ„æº
        await tester.cleanup()

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())