#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·¥ä½œæµå¼•æ“æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
é‡ç‚¹å…³æ³¨æ‰§è¡Œæ•ˆç‡å’Œèµ„æºç®¡ç†
"""

import time
import asyncio
import psutil
import os
import tracemalloc
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import matplotlib.pyplot as plt
import pandas as pd
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
import sys
sys.path.insert(0, str(PROJECT_ROOT))

try:
    from iflow.core.ultimate_workflow_engine_v6 import UltimateWorkflowEngineV6, TaskPriority, ExecutionMode
    from iflow.core.male_system import MultiAgentLearningEngine
except ImportError as e:
    print(f"æ— æ³•å¯¼å…¥å·¥ä½œæµå¼•æ“: {e}")
    exit(1)

class WorkflowEngineBenchmark:
    """å·¥ä½œæµå¼•æ“æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.engine = None
        self.male_system = None
        
        # æ€§èƒ½æŒ‡æ ‡
        self.execution_times = []
        self.memory_usage = []
        self.cpu_usage = []
        self.resource_snapshots = []
        
        # æµ‹è¯•é…ç½®
        self.test_configs = {
            "concurrent_tasks": [1, 2, 4, 8, 16],
            "task_complexity": ["low", "medium", "high", "extreme"],
            "priority_levels": [TaskPriority.LOW, TaskPriority.MEDIUM, TaskPriority.HIGH],
            "execution_modes": [ExecutionMode.SINGLE_STEP, ExecutionMode.ITERATIVE, ExecutionMode.AUTONOMOUS]
        }
    
    async def initialize_engine(self) -> bool:
        """åˆå§‹åŒ–å·¥ä½œæµå¼•æ“"""
        try:
            self.engine = UltimateWorkflowEngineV6()
            await self.engine.initialize()
            print("âœ… å·¥ä½œæµå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ å·¥ä½œæµå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def capture_resource_snapshot(self, label: str = "") -> Dict[str, Any]:
        """æ•è·èµ„æºå¿«ç…§"""
        process = psutil.Process(os.getpid())
        snapshot = {
            "timestamp": datetime.now().isoformat(),
            "label": label,
            "memory_mb": process.memory_info().rss / 1024 / 1024,
            "memory_percent": process.memory_percent(),
            "cpu_percent": process.cpu_percent(),
            "num_threads": process.num_threads(),
            "num_fds": process.num_fds() if hasattr(process, 'num_fds') else 0
        }
        self.resource_snapshots.append(snapshot)
        return snapshot
    
    async def test_concurrent_execution_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘æ‰§è¡Œæ€§èƒ½"""
        print("\nğŸš€ å¹¶å‘æ‰§è¡Œæ€§èƒ½æµ‹è¯•")
        print("-" * 40)
        
        results = {}
        
        for concurrent_count in self.test_configs["concurrent_tasks"]:
            print(f"  ğŸ“Š æµ‹è¯•å¹¶å‘æ•°: {concurrent_count}")
            
            # å‡†å¤‡å¹¶å‘ä»»åŠ¡
            tasks = []
            for i in range(concurrent_count):
                task_input = f"ä»»åŠ¡{i+1}: å¼€å‘ä¸€ä¸ªé«˜æ€§èƒ½çš„{'åˆ†å¸ƒå¼ç¼“å­˜' if i % 3 == 0 else 'ç”¨æˆ·è®¤è¯' if i % 3 == 1 else 'æ•°æ®å¤„ç†'}ç³»ç»Ÿ"
                tasks.append({
                    "user_input": task_input,
                    "priority": TaskPriority.MEDIUM,
                    "execution_mode": ExecutionMode.AUTONOMOUS,
                    "metadata": {"complexity": "medium", "test_type": "concurrent"}
                })
            
            # æ•è·åˆå§‹çŠ¶æ€
            initial_snapshot = self.capture_resource_snapshot(f"Concurrent_{concurrent_count}_Start")
            start_time = time.time()
            
            try:
                # å¹¶å‘æ‰§è¡Œä»»åŠ¡
                semaphore = asyncio.Semaphore(concurrent_count)
                
                async def execute_task_with_semaphore(task_config):
                    async with semaphore:
                        return await self.engine.execute_task(**task_config)
                
                task_coroutines = [execute_task_with_semaphore(task) for task in tasks]
                results_list = await asyncio.gather(*task_coroutines, return_exceptions=True)
                
                end_time = time.time()
                final_snapshot = self.capture_resource_snapshot(f"Concurrent_{concurrent_count}_End")
                
                # åˆ†æç»“æœ
                successful_tasks = sum(1 for r in results_list if not isinstance(r, Exception) and r.success)
                total_time = end_time - start_time
                avg_time_per_task = total_time / len(results_list)
                
                # è®¡ç®—èµ„æºä½¿ç”¨å³°å€¼
                peak_memory = max(s["memory_mb"] for s in self.resource_snapshots 
                                if s["label"].startswith(f"Concurrent_{concurrent_count}"))
                
                test_result = {
                    "concurrent_count": concurrent_count,
                    "total_tasks": len(tasks),
                    "successful_tasks": successful_tasks,
                    "success_rate": successful_tasks / len(tasks),
                    "total_execution_time": total_time,
                    "avg_time_per_task": avg_time_per_task,
                    "throughput_tasks_per_second": len(tasks) / total_time,
                    "peak_memory_mb": peak_memory,
                    "memory_increase_mb": final_snapshot["memory_mb"] - initial_snapshot["memory_mb"],
                    "cpu_usage_peak": max(s["cpu_percent"] for s in self.resource_snapshots 
                                        if s["label"].startswith(f"Concurrent_{concurrent_count}"))
                }
                
                results[concurrent_count] = test_result
                
                print(f"    âœ… æˆåŠŸç‡: {test_result['success_rate']:.2%}")
                print(f"    â±ï¸ æ€»è€—æ—¶: {total_time:.3f}s (å¹³å‡: {avg_time_per_task:.3f}s/ä»»åŠ¡)")
                print(f"    ğŸš€ ååé‡: {test_result['throughput_tasks_per_second']:.2f} ä»»åŠ¡/ç§’")
                print(f"    ğŸ’¾ å†…å­˜å³°å€¼: {peak_memory:.2f}MB")
                
            except Exception as e:
                print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                results[concurrent_count] = {"error": str(e)}
        
        return results
    
    async def test_task_complexity_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•ä»»åŠ¡å¤æ‚åº¦æ€§èƒ½"""
        print("\nğŸ§  ä»»åŠ¡å¤æ‚åº¦æ€§èƒ½æµ‹è¯•")
        print("-" * 40)
        
        complexity_tasks = {
            "low": "åˆ›å»ºä¸€ä¸ªç®€å•çš„Hello Worldç¨‹åº",
            "medium": "è®¾è®¡ä¸€ä¸ªç”¨æˆ·ç®¡ç†ç³»ç»Ÿï¼ŒåŒ…å«æ³¨å†Œã€ç™»å½•åŠŸèƒ½",
            "high": "å¼€å‘ä¸€ä¸ªå®Œæ•´çš„ç”µå•†å¹³å°ï¼Œæ”¯æŒå•†å“ç®¡ç†ã€è®¢å•å¤„ç†ã€æ”¯ä»˜é›†æˆ",
            "extreme": "æ„å»ºä¸€ä¸ªåˆ†å¸ƒå¼å¾®æœåŠ¡æ¶æ„çš„å¤§å‹ç¤¾äº¤ç½‘ç»œå¹³å°ï¼Œæ”¯æŒåƒä¸‡çº§ç”¨æˆ·å¹¶å‘"
        }
        
        results = {}
        
        for complexity, task_description in complexity_tasks.items():
            print(f"  ğŸ“‹ å¤æ‚åº¦: {complexity} - {task_description[:30]}...")
            
            initial_snapshot = self.capture_resource_snapshot(f"Complexity_{complexity}_Start")
            start_time = time.time()
            
            try:
                # æ‰§è¡Œä»»åŠ¡
                result = await self.engine.execute_task(
                    user_input=task_description,
                    priority=TaskPriority.HIGH,
                    execution_mode=ExecutionMode.ITERATIVE,
                    metadata={"complexity": complexity, "test_type": "complexity"}
                )
                
                end_time = time.time()
                final_snapshot = self.capture_resource_snapshot(f"Complexity_{complexity}_End")
                
                execution_time = end_time - start_time
                memory_increase = final_snapshot["memory_mb"] - initial_snapshot["memory_mb"]
                
                test_result = {
                    "complexity": complexity,
                    "success": result.success,
                    "execution_time": execution_time,
                    "confidence_score": result.confidence_score,
                    "tool_calls": len(result.tool_calls),
                    "memory_increase_mb": memory_increase,
                    "error": result.error if not result.success else ""
                }
                
                results[complexity] = test_result
                
                print(f"    âœ… ç»“æœ: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}")
                print(f"    â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s")
                print(f"    ğŸ¯ ç½®ä¿¡åº¦: {result.confidence_score:.2f}")
                print(f"    ğŸ› ï¸ å·¥å…·è°ƒç”¨: {len(result.tool_calls)} æ¬¡")
                
            except Exception as e:
                print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                results[complexity] = {"error": str(e)}
        
        return results
    
    async def test_priority_scheduling_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•ä¼˜å…ˆçº§è°ƒåº¦æ€§èƒ½"""
        print("\nğŸ¯ ä¼˜å…ˆçº§è°ƒåº¦æ€§èƒ½æµ‹è¯•")
        print("-" * 40)
        
        # åˆ›å»ºæ··åˆä¼˜å…ˆçº§ä»»åŠ¡
        mixed_tasks = [
            {"priority": TaskPriority.LOW, "task": "ä¼˜åŒ–ä»£ç æ³¨é‡Šå’Œæ–‡æ¡£"},
            {"priority": TaskPriority.MEDIUM, "task": "ä¿®å¤å·²çŸ¥çš„æ€§èƒ½é—®é¢˜"},
            {"priority": TaskPriority.HIGH, "task": "è§£å†³å…³é”®çš„å®‰å…¨æ¼æ´"},
            {"priority": TaskPriority.HIGH, "task": "å¤„ç†ç´§æ€¥çš„ç³»ç»Ÿæ•…éšœ"},
            {"priority": TaskPriority.MEDIUM, "task": "å®æ–½æ–°çš„åŠŸèƒ½ç‰¹æ€§"},
            {"priority": TaskPriority.LOW, "task": "æ›´æ–°ç”¨æˆ·ç•Œé¢æ ·å¼"}
        ]
        
        results = []
        
        print(f"  ğŸ“‹ æ‰§è¡Œ {len(mixed_tasks)} ä¸ªæ··åˆä¼˜å…ˆçº§ä»»åŠ¡...")
        
        initial_snapshot = self.capture_resource_snapshot("Priority_Start")
        start_time = time.time()
        
        try:
            # å¹¶å‘æ‰§è¡Œä»»åŠ¡ï¼Œè§‚å¯Ÿä¼˜å…ˆçº§è°ƒåº¦
            task_coroutines = []
            for i, task_config in enumerate(mixed_tasks):
                task_input = f"ä»»åŠ¡{i+1}({task_config['priority'].value}): {task_config['task']}"
                task_coroutines.append(
                    self.engine.execute_task(
                        user_input=task_input,
                        priority=task_config["priority"],
                        execution_mode=ExecutionMode.AUTONOMOUS,
                        metadata={"test_type": "priority", "original_order": i}
                    )
                )
            
            # ä½¿ç”¨as_completedè§‚å¯Ÿæ‰§è¡Œé¡ºåº
            execution_order = []
            async for completed_task in asyncio.as_completed(task_coroutines):
                execution_order.append(completed_task)
            
            # æ”¶é›†æ‰€æœ‰ç»“æœ
            all_results = await asyncio.gather(*execution_order, return_exceptions=True)
            
            end_time = time.time()
            final_snapshot = self.capture_resource_snapshot("Priority_End")
            
            # åˆ†ææ‰§è¡Œé¡ºåº
            successful_results = [r for r in all_results if not isinstance(r, Exception) and r.success]
            avg_execution_time = sum(r.execution_time for r in successful_results) / len(successful_results) if successful_results else 0
            
            priority_analysis = {
                "total_tasks": len(mixed_tasks),
                "successful_tasks": len(successful_results),
                "success_rate": len(successful_results) / len(mixed_tasks),
                "total_execution_time": end_time - start_time,
                "avg_execution_time": avg_execution_time,
                "memory_increase_mb": final_snapshot["memory_mb"] - initial_snapshot["memory_mb"]
            }
            
            results = priority_analysis
            
            print(f"    âœ… æˆåŠŸç‡: {priority_analysis['success_rate']:.2%}")
            print(f"    â±ï¸ æ€»è€—æ—¶: {priority_analysis['total_execution_time']:.3f}s")
            print(f"    ğŸ“Š å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_execution_time:.3f}s")
            
        except Exception as e:
            print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
            results = {"error": str(e)}
        
        return results
    
    async def test_execution_mode_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æ‰§è¡Œæ¨¡å¼æ€§èƒ½"""
        print("\nâš™ï¸ æ‰§è¡Œæ¨¡å¼æ€§èƒ½æµ‹è¯•")
        print("-" * 40)
        
        test_task = "å¼€å‘ä¸€ä¸ªé«˜æ€§èƒ½çš„APIæœåŠ¡ï¼Œæ”¯æŒç”¨æˆ·è®¤è¯ã€æ•°æ®å­˜å‚¨å’Œå®æ—¶é€šä¿¡"
        
        results = {}
        
        for mode in self.test_configs["execution_modes"]:
            print(f"  ğŸ”„ æµ‹è¯•æ¨¡å¼: {mode.value}")
            
            initial_snapshot = self.capture_resource_snapshot(f"Mode_{mode.value}_Start")
            start_time = time.time()
            
            try:
                # æ‰§è¡Œä»»åŠ¡
                result = await self.engine.execute_task(
                    user_input=test_task,
                    priority=TaskPriority.HIGH,
                    execution_mode=mode,
                    metadata={"test_type": "execution_mode", "mode": mode.value}
                )
                
                end_time = time.time()
                final_snapshot = self.capture_resource_snapshot(f"Mode_{mode.value}_End")
                
                execution_time = end_time - start_time
                memory_increase = final_snapshot["memory_mb"] - initial_snapshot["memory_mb"]
                
                test_result = {
                    "mode": mode.value,
                    "success": result.success,
                    "execution_time": execution_time,
                    "confidence_score": result.confidence_score,
                    "tool_calls": len(result.tool_calls),
                    "iterations": result.execution_time if hasattr(result, 'execution_time') else 0,
                    "memory_increase_mb": memory_increase,
                    "error": result.error if not result.success else ""
                }
                
                results[mode.value] = test_result
                
                print(f"    âœ… ç»“æœ: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}")
                print(f"    â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s")
                print(f"    ğŸ¯ ç½®ä¿¡åº¦: {result.confidence_score:.2f}")
                print(f"    ğŸ”„ è¿­ä»£æ¬¡æ•°: {test_result.get('iterations', 0)}")
                
            except Exception as e:
                print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                results[mode.value] = {"error": str(e)}
        
        return results
    
    async def test_memory_management_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ç®¡ç†æ€§èƒ½"""
        print("\nğŸ’¾ å†…å­˜ç®¡ç†æ€§èƒ½æµ‹è¯•")
        print("-" * 40)
        
        # é•¿æ—¶é—´è¿è¡Œæµ‹è¯•
        test_duration = 60  # 60ç§’
        memory_samples = []
        
        print(f"  ğŸ“Š è¿›è¡Œ {test_duration} ç§’çš„å†…å­˜å‹åŠ›æµ‹è¯•...")
        
        start_time = time.time()
        baseline_snapshot = self.capture_resource_snapshot("Memory_Test_Start")
        baseline_memory = baseline_snapshot["memory_mb"]
        
        task_count = 0
        
        async def memory_monitor():
            """å†…å­˜ç›‘æ§åç¨‹"""
            nonlocal memory_samples
            while time.time() - start_time < test_duration:
                snapshot = self.capture_resource_snapshot("Memory_Sample")
                memory_samples.append({
                    "timestamp": snapshot["timestamp"],
                    "elapsed": time.time() - start_time,
                    "memory_mb": snapshot["memory_mb"],
                    "memory_percent": snapshot["memory_percent"],
                    "cpu_percent": snapshot["cpu_percent"]
                })
                await asyncio.sleep(1)  # æ¯ç§’é‡‡æ ·ä¸€æ¬¡
        
        async def task_generator():
            """ä»»åŠ¡ç”Ÿæˆåç¨‹"""
            nonlocal task_count
            while time.time() - start_time < test_duration:
                try:
                    task_input = f"å†…å­˜æµ‹è¯•ä»»åŠ¡{task_count}: {['åˆ†æä»£ç æ€§èƒ½', 'ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢', 'é‡æ„ç³»ç»Ÿæ¶æ„'][task_count % 3]}"
                    result = await self.engine.execute_task(
                        user_input=task_input,
                        priority=TaskPriority.MEDIUM,
                        execution_mode=ExecutionMode.SINGLE_STEP,
                        metadata={"test_type": "memory_stress"}
                    )
                    task_count += 1
                    await asyncio.sleep(0.5)  # æ¯0.5ç§’ç”Ÿæˆä¸€ä¸ªä»»åŠ¡
                except Exception:
                    await asyncio.sleep(0.1)
        
        # åŒæ—¶è¿è¡Œå†…å­˜ç›‘æ§å’Œä»»åŠ¡ç”Ÿæˆ
        await asyncio.gather(
            memory_monitor(),
            task_generator()
        )
        
        # åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ
        final_snapshot = self.capture_resource_snapshot("Memory_Test_End")
        final_memory = final_snapshot["memory_mb"]
        
        memory_usage_stats = {
            "test_duration": test_duration,
            "total_tasks": task_count,
            "baseline_memory_mb": baseline_memory,
            "final_memory_mb": final_memory,
            "peak_memory_mb": max(s["memory_mb"] for s in memory_samples),
            "memory_increase_mb": final_memory - baseline_memory,
            "memory_growth_rate": (final_memory - baseline_memory) / test_duration if test_duration > 0 else 0,
            "avg_memory_mb": sum(s["memory_mb"] for s in memory_samples) / len(memory_samples) if memory_samples else 0,
            "memory_efficiency": task_count / (final_memory - baseline_memory) if (final_memory - baseline_memory) > 0 else float('inf')
        }
        
        print(f"    ğŸ“ˆ æ‰§è¡Œä»»åŠ¡æ•°: {task_count}")
        print(f"    ğŸ’¾ åŸºçº¿å†…å­˜: {baseline_memory:.2f}MB")
        print(f"    ğŸ’¾ å³°å€¼å†…å­˜: {memory_usage_stats['peak_memory_mb']:.2f}MB")
        print(f"    ğŸ“Š å¹³å‡å†…å­˜: {memory_usage_stats['avg_memory_mb']:.2f}MB")
        print(f"    ğŸ“ˆ å†…å­˜å¢é•¿ç‡: {memory_usage_stats['memory_growth_rate']:.2f}MB/ç§’")
        
        return memory_usage_stats
    
    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å·¥ä½œæµå¼•æ“ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        
        if not await self.initialize_engine():
            return {}
        
        # æ•è·åˆå§‹çŠ¶æ€
        self.capture_resource_snapshot("Benchmark_Start")
        
        # æ‰§è¡Œå„é¡¹æµ‹è¯•
        print("\n" + "="*60)
        concurrent_results = await self.test_concurrent_execution_performance()
        
        print("\n" + "="*60)
        complexity_results = await self.test_task_complexity_performance()
        
        print("\n" + "="*60)
        priority_results = await self.test_priority_scheduling_performance()
        
        print("\n" + "="*60)
        mode_results = await self.test_execution_mode_performance()
        
        print("\n" + "="*60)
        memory_results = await self.test_memory_management_performance()
        
        # æ•è·æœ€ç»ˆçŠ¶æ€
        self.capture_resource_snapshot("Benchmark_End")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        comprehensive_results = {
            "test_metadata": {
                "timestamp": datetime.now().isoformat(),
                "test_type": "workflow_engine_benchmark",
                "engine_version": "v6",
                "total_tests": 5
            },
            "concurrent_execution": concurrent_results,
            "task_complexity": complexity_results,
            "priority_scheduling": priority_results,
            "execution_modes": mode_results,
            "memory_management": memory_results,
            "resource_snapshots": self.resource_snapshots,
            "overall_performance": self.calculate_overall_performance(concurrent_results, complexity_results, priority_results, mode_results, memory_results)
        }
        
        return comprehensive_results
    
    def calculate_overall_performance(self, concurrent_results, complexity_results, priority_results, mode_results, memory_results) -> Dict[str, Any]:
        """è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡"""
        overall = {
            "efficiency_score": 0,
            "scalability_score": 0,
            "reliability_score": 0,
            "resource_optimization_score": 0
        }
        
        # è®¡ç®—æ•ˆç‡åˆ†æ•° (åŸºäºå¹¶å‘æ€§èƒ½å’Œæ‰§è¡Œæ—¶é—´)
        if concurrent_results:
            max_throughput = max(r.get("throughput_tasks_per_second", 0) for r in concurrent_results.values() if isinstance(r, dict))
            overall["efficiency_score"] = min(max_throughput / 10, 1.0)  # æ ‡å‡†åŒ–åˆ°0-1
        
        # è®¡ç®—å¯æ‰©å±•æ€§åˆ†æ•° (åŸºäºå¹¶å‘æ€§èƒ½çš„çº¿æ€§åº¦)
        if len(concurrent_results) >= 2:
            throughput_values = [r.get("throughput_tasks_per_second", 0) for r in concurrent_results.values() if isinstance(r, dict)]
            if len(throughput_values) > 1:
                # ç®€å•çš„çº¿æ€§ç›¸å…³æ€§è®¡ç®—
                import numpy as np
                concurrency_levels = list(concurrent_results.keys())
                correlation = np.corrcoef(concurrency_levels, throughput_values)[0, 1] if len(throughput_values) == len(concurrency_levels) else 0
                overall["scalability_score"] = max(0, correlation)
        
        # è®¡ç®—å¯é æ€§åˆ†æ•° (åŸºäºæˆåŠŸç‡)
        success_rates = []
        if complexity_results:
            success_rates.extend([r.get("success_rate", 0) for r in complexity_results.values() if isinstance(r, dict)])
        if priority_results and isinstance(priority_results, dict):
            success_rates.append(priority_results.get("success_rate", 0))
        
        if success_rates:
            overall["reliability_score"] = sum(success_rates) / len(success_rates)
        
        # è®¡ç®—èµ„æºä¼˜åŒ–åˆ†æ•° (åŸºäºå†…å­˜ä½¿ç”¨æ•ˆç‡)
        if memory_results and isinstance(memory_results, dict):
            memory_efficiency = memory_results.get("memory_efficiency", 0)
            overall["resource_optimization_score"] = min(memory_efficiency / 100, 1.0)  # æ ‡å‡†åŒ–åˆ°0-1
        
        # è®¡ç®—ç»¼åˆæ€§èƒ½åˆ†æ•°
        overall["composite_score"] = sum(overall.values()) / len(overall)
        
        return overall
    
    def generate_performance_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        overall = results.get("overall_performance", {})
        
        report = f"""
å·¥ä½œæµå¼•æ“æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š
{'=' * 60}

ğŸ¯ ç»¼åˆæ€§èƒ½è¯„åˆ†:
- æ•ˆç‡åˆ†æ•°: {overall.get('efficiency_score', 0):.2f}/1.0
- å¯æ‰©å±•æ€§åˆ†æ•°: {overall.get('scalability_score', 0):.2f}/1.0  
- å¯é æ€§åˆ†æ•°: {overall.get('reliability_score', 0):.2%}
- èµ„æºä¼˜åŒ–åˆ†æ•°: {overall.get('resource_optimization_score', 0):.2f}/1.0
- ç»¼åˆæ€§èƒ½åˆ†æ•°: {overall.get('composite_score', 0):.2f}/1.0

ğŸ“Š å¹¶å‘æ‰§è¡Œæ€§èƒ½:
"""
        
        concurrent_results = results.get("concurrent_execution", {})
        if concurrent_results:
            for count, data in concurrent_results.items():
                if isinstance(data, dict) and "throughput_tasks_per_second" in data:
                    report += f"- {count}å¹¶å‘: {data['throughput_tasks_per_second']:.2f} ä»»åŠ¡/ç§’, æˆåŠŸç‡: {data.get('success_rate', 0):.2%}\n"
        
        report += f"""
ğŸ§  ä»»åŠ¡å¤æ‚åº¦æ€§èƒ½:
"""
        complexity_results = results.get("task_complexity", {})
        if complexity_results:
            for complexity, data in complexity_results.items():
                if isinstance(data, dict) and "execution_time" in data:
                    report += f"- {complexity}: {data['execution_time']:.3f}s, æˆåŠŸç‡: {data.get('success_rate', 0):.2%}\n"
        
        report += f"""
ğŸ’¾ å†…å­˜ç®¡ç†æ€§èƒ½:
"""
        memory_results = results.get("memory_management", {})
        if memory_results:
            report += f"- æ‰§è¡Œä»»åŠ¡æ•°: {memory_results.get('total_tasks', 0)}\n"
            report += f"- å³°å€¼å†…å­˜: {memory_results.get('peak_memory_mb', 0):.2f}MB\n"
            report += f"- å†…å­˜æ•ˆç‡: {memory_results.get('memory_efficiency', 0):.2f} ä»»åŠ¡/MB\n"
        
        # æ€§èƒ½è¯„ä¼°
        composite_score = overall.get("composite_score", 0)
        if composite_score > 0.8:
            report += "\nâœ… æ€§èƒ½è¯„çº§: ä¼˜ç§€ (> 0.8)\n"
            report += "ğŸ’¡ å·¥ä½œæµå¼•æ“æ€§èƒ½è¡¨ç°å“è¶Šï¼Œå¯ä»¥å¤„ç†é«˜å¹¶å‘å’Œå¤æ‚ä»»åŠ¡è´Ÿè½½ã€‚\n"
        elif composite_score > 0.6:
            report += "\nâš ï¸ æ€§èƒ½è¯„çº§: è‰¯å¥½ (0.6-0.8)\n"
            report += "ğŸ’¡ æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†å’Œå¹¶å‘å¤„ç†ç­–ç•¥ã€‚\n"
        else:
            report += "\nâŒ æ€§èƒ½è¯„çº§: éœ€è¦æ”¹è¿› (< 0.6)\n"
            report += "ğŸ’¡ å»ºè®®é‡ç‚¹ä¼˜åŒ–æ‰§è¡Œæ•ˆç‡ã€èµ„æºç®¡ç†å’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚\n"
        
        return report
    
    def save_benchmark_results(self, results: Dict[str, Any], filename: str = "workflow_engine_benchmark_results.json"):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        results_data = {
            "benchmark_metadata": {
                "timestamp": datetime.now().isoformat(),
                "test_type": "workflow_engine_comprehensive_benchmark",
                "engine_version": "v6",
                "test_duration_minutes": (datetime.now() - datetime.fromisoformat(results.get("test_metadata", {}).get("timestamp", datetime.now().isoformat()))).total_seconds() / 60
            },
            "test_results": results,
            "performance_report": self.generate_performance_report(results)
        }
        
        results_path = PROJECT_ROOT / "iflow" / "tests" / "benchmark" / filename
        results_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜: {results_path}")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.engine:
            await self.engine.shutdown()

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å·¥ä½œæµå¼•æ“æ€§èƒ½åŸºå‡†æµ‹è¯•å¯åŠ¨")
    print("=" * 60)
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
    benchmark = WorkflowEngineBenchmark()
    
    try:
        # è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•
        results = await benchmark.run_comprehensive_benchmark()
        
        if results:
            # ç”ŸæˆæŠ¥å‘Š
            report = benchmark.generate_performance_report(results)
            print("\n" + report)
            
            # ä¿å­˜ç»“æœ
            benchmark.save_benchmark_results(results)
            
        else:
            print("âŒ åŸºå‡†æµ‹è¯•å¤±è´¥")
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await benchmark.cleanup()

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())