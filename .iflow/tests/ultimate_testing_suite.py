#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¬ ç»ˆææµ‹è¯•å¥—ä»¶ V3 - T-MIA æ¶æ„ç‰ˆ
Ultimate Testing Suite V3 - T-MIA Architecture Edition

ä¸€ä¸ªèƒ½å¤ŸçœŸå®è°ƒç”¨T-MIAç»ˆæå·¥ä½œæµå¼•æ“ã€æ‰§è¡Œç«¯åˆ°ç«¯å¤æ‚ä»»åŠ¡ã€
å¹¶è¿›è¡Œç§‘å­¦çš„ã€å¤šç»´åº¦é‡åŒ–è¯„ä¼°çš„è‡ªåŠ¨åŒ–æµ‹è¯•ä¸å¯¹æ¯”æ¡†æ¶ã€‚

ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
import statistics
import traceback
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable, Type
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import psutil

# åŠ¨æ€æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
try:
    project_root = Path(__file__).resolve().parent.parent.parent
    if project_root.name != 'Aé¡¹ç›®':
         project_root = Path(__file__).resolve().parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

    from iflow.core.ultimate_workflow_engine import TMIAUltimateWorkflowEngine
except ImportError as e:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    logger.error(f"å…³é”®æ¨¡å—å¯¼å…¥å¤±è´¥: {e}ã€‚è¯·ç¡®ä¿è„šæœ¬åœ¨æ­£ç¡®çš„é¡¹ç›®ç»“æ„ä¸‹è¿è¡Œã€‚")
    sys.exit(1)

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# --- æšä¸¾ä¸æ•°æ®ç±» (å€Ÿé‰´è‡ª Cé¡¹ç›® V10) ---

class TestCategory(Enum):
    FUNCTIONALITY = "functionality"
    PERFORMANCE = "performance"
    QUALITY = "quality"
    INTELLIGENCE = "intelligence" # æ–°å¢ï¼šæ™ºèƒ½ç¨‹åº¦

@dataclass
class TestScenario:
    name: str
    description: str
    task_description: str
    input_data: Dict[str, Any]
    test_function: Callable
    category: TestCategory
    expected_keywords: List[str] = field(default_factory=list)
    complexity: int = 5

@dataclass
class TestResult:
    scenario_name: str
    system_name: str
    success: bool
    execution_time: float
    metrics: Dict[str, float] = field(default_factory=dict)
    error_message: Optional[str] = None
    artifacts: Dict[str, Any] = field(default_factory=dict)

# --- ç»ˆææµ‹è¯•å¥—ä»¶ V3 ---

class UltimateTestingSuite:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.scenarios: List[TestScenario] = []
        self.systems: Dict[str, TMIAUltimateWorkflowEngine] = {}
        self.output_dir = Path(self.config.get('output_dir', 'Aé¡¹ç›®/iflow/tests/reports'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.evaluation_weights = {
            'quality': 0.3, 'efficiency': 0.3, 'intelligence': 0.4
        }
        self._initialize_scenarios()

    async def initialize_systems(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¾…æµ‹è¯•çš„ç³»ç»Ÿå®ä¾‹"""
        logger.info("åˆå§‹åŒ– T-MIA ç»ˆæå·¥ä½œæµå¼•æ“ç”¨äºæµ‹è¯•...")
        
        # åœ¨æ­¤å¯ä»¥åˆå§‹åŒ–ä¸åŒé…ç½®çš„å¼•æ“å®ä¾‹ä»¥è¿›è¡Œå¯¹æ¯”
        # ä¾‹å¦‚: old_engine_config, new_engine_config
        engine_v3 = TMIAUltimateWorkflowEngine()
        await engine_v3.initialize()
        
        self.systems['TMIA_Engine_V3'] = engine_v3
        logger.info("å·²æ·»åŠ æµ‹è¯•ç³»ç»Ÿ: TMIA_Engine_V3")

    def _initialize_scenarios(self):
        """åˆå§‹åŒ–å†…ç½®çš„åŸºå‡†æµ‹è¯•åœºæ™¯"""
        self.scenarios = [
            TestScenario(
                name="å¤æ‚ä»»åŠ¡-ä»£ç ä¸æ¶æ„",
                description="æµ‹è¯•å¼•æ“å¤„ç†ä¸€ä¸ªåŒ…å«ä»£ç ç”Ÿæˆã€åˆ†æå’Œæ¶æ„è®¾è®¡çš„å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚",
                task_description="æˆ‘éœ€è¦ä¸ºä¸€ä¸ªæ–°çš„ç¤¾äº¤åª’ä½“åŠŸèƒ½ï¼ˆâ€œåŠ¨æ€Gifsâ€ï¼‰è®¾è®¡å¹¶å®ç°åç«¯APIã€‚è¯·ä½¿ç”¨Python FastAPIï¼ŒåŠŸèƒ½åŒ…æ‹¬ä¸Šä¼ GIFï¼Œé€šè¿‡æ ‡ç­¾æœç´¢GIFï¼Œå¹¶è®°å½•è§‚çœ‹æ¬¡æ•°ã€‚æ•°æ®åº“è¯·ä½¿ç”¨PostgreSQLã€‚è¯·ç¡®ä¿ä»£ç æ˜¯ç”Ÿäº§çº§åˆ«çš„ï¼ŒåŒ…å«é”™è¯¯å¤„ç†ã€æ—¥å¿—è®°å½•å’Œå•å…ƒæµ‹è¯•ã€‚",
                input_data={
                    "tech_stack": ["Python", "FastAPI", "PostgreSQL"],
                    "feature": "Dynamic Gifs"
                },
                test_function=self.run_end_to_end_task,
                category=TestCategory.FUNCTIONALITY,
                expected_keywords=["FastAPI", "PostgreSQL", "def upload_gif", "def search_gifs", "CREATE TABLE gifs"],
                complexity=8
            ),
            TestScenario(
                name="æ€§èƒ½åˆ†æä¸ä¼˜åŒ–",
                description="æµ‹è¯•å¼•æ“åˆ†ææ€§èƒ½é—®é¢˜å¹¶æå‡ºä¼˜åŒ–æ–¹æ¡ˆçš„èƒ½åŠ›ã€‚",
                task_description="åˆ†æä¸€ä¸ªç”µå•†å¹³å°çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶æå‡ºä¸€å¥—å®Œæ•´çš„ã€åŒ…å«å‰ç«¯ã€åç«¯å’Œæ•°æ®åº“çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚",
                input_data={
                    "platform_tech_stack": ["React", "Node.js", "PostgreSQL"],
                    "current_issues": ["é¡µé¢åŠ è½½æ…¢", "é«˜å¹¶å‘ä¸‹APIå“åº”å»¶è¿Ÿé«˜"]
                },
                test_function=self.run_end_to_end_task,
                category=TestCategory.PERFORMANCE,
                expected_keywords=["ç¼“å­˜", "CDN", "æ•°æ®åº“ç´¢å¼•", "ä»£ç åˆ†å‰²", "æ‡’åŠ è½½"],
                complexity=9
            ),
        ]

    async def run_comparison(self, system_names: List[str]) -> Dict[str, Any]:
        """è¿è¡ŒæŒ‡å®šç³»ç»Ÿé—´çš„å¯¹æ¯”æµ‹è¯•"""
        systems_to_test = {name: self.systems[name] for name in system_names if name in self.systems}
        if not systems_to_test:
            raise ValueError("æ²¡æœ‰å·²æ³¨å†Œçš„ç³»ç»Ÿå¯ä¾›æµ‹è¯•ã€‚")

        logger.info(f"å¼€å§‹å¯¹æ¯”æµ‹è¯•: {', '.join(systems_to_test.keys())}")
        all_results: Dict[str, TestResult] = {}
        for scenario in self.scenarios:
            for system_name, system_instance in systems_to_test.items():
                logger.info(f"æ‰§è¡Œåœºæ™¯ '{scenario.name}' äºç³»ç»Ÿ '{system_name}'")
                result = await self._run_single_test(scenario, system_name, system_instance)
                all_results[f"{scenario.name}_{system_name}"] = result
        
        report = self._generate_report(list(systems_to_test.keys()), all_results)
        self._save_report(report)
        return report

    async def _run_single_test(self, scenario: TestScenario, system_name: str, system_instance: TMIAUltimateWorkflowEngine) -> TestResult:
        """è¿è¡Œå•ä¸ªç«¯åˆ°ç«¯æµ‹è¯•å¹¶è¯„ä¼°ç»“æœ"""
        process = psutil.Process(os.getpid())
        
        # æµ‹è¯•å‰æ”¶é›†ç³»ç»ŸçŠ¶æ€
        cpu_before = process.cpu_percent(interval=None)
        mem_before = process.memory_info().rss
        start_time = time.time()
        
        try:
            # æ‰§è¡Œæµ‹è¯•å‡½æ•°
            output = await asyncio.wait_for(
                scenario.test_function(system_instance, scenario), 
                timeout=300
            )
            
            # æµ‹è¯•åæ”¶é›†ç³»ç»ŸçŠ¶æ€
            execution_time = time.time() - start_time
            cpu_after = process.cpu_percent(interval=None)
            mem_after = process.memory_info().rss
            
            # è¯„ä¼°ç»“æœ
            quality = self._evaluate_quality(output, scenario.expected_keywords)
            intelligence = self._evaluate_intelligence(output)
            efficiency = self._evaluate_efficiency(execution_time, cpu_after - cpu_before, mem_after - mem_before, scenario.complexity)

            overall_score = (quality * self.evaluation_weights['quality'] +
                             efficiency * self.evaluation_weights['efficiency'] +
                             intelligence * self.evaluation_weights['intelligence'])
            
            return TestResult(
                scenario_name=scenario.name, system_name=system_name, success=True,
                execution_time=execution_time,
                metrics={
                    "overall_score": overall_score, "quality": quality, "efficiency": efficiency, "intelligence": intelligence,
                    "cpu_usage": cpu_after - cpu_before, "memory_usage_mb": (mem_after - mem_before) / (1024*1024)
                },
                artifacts={'output': output}
            )
        except Exception as e:
            logger.error(f"æµ‹è¯• '{scenario.name}' åœ¨ '{system_name}' ä¸Šå¤±è´¥: {e}", exc_info=True)
            return TestResult(
                scenario_name=scenario.name, system_name=system_name, success=False,
                execution_time=time.time() - start_time, error_message=traceback.format_exc()
            )

    async def run_end_to_end_task(self, engine: TMIAUltimateWorkflowEngine, scenario: TestScenario) -> Dict[str, Any]:
        """ä¸€ä¸ªé€šç”¨çš„ç«¯åˆ°ç«¯ä»»åŠ¡æ‰§è¡Œå‡½æ•°"""
        result = await engine.execute_workflow(scenario.task_description, scenario.input_data)
        if result['status'] == 'FAILED':
            raise Exception(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return result['result']

    # --- è¯„ä¼°æ–¹æ³• (æ›´ç§‘å­¦) ---
    def _evaluate_quality(self, output: Dict[str, Any], expected_keywords: List[str]) -> float:
        """è¯„ä¼°ç»“æœçš„è´¨é‡å’Œç›¸å…³æ€§"""
        output_text = json.dumps(output)
        if not output_text or not expected_keywords: return 0.0
        matches = sum(1 for keyword in expected_keywords if keyword.lower() in output_text.lower())
        return matches / len(expected_keywords)

    def _evaluate_intelligence(self, output: Dict[str, Any]) -> float:
        """è¯„ä¼°ç»“æœçš„æ™ºèƒ½ç¨‹åº¦ (ä¾‹å¦‚ï¼Œè®¡åˆ’çš„æ·±åº¦)"""
        reasoning = output.get('reasoning', {})
        if not reasoning: return 0.1
        
        decomposition_depth = len(reasoning.get('problem_decomposition', []))
        rules_activated = len(reasoning.get('activated_rules', []))
        
        score = 0.0
        score += min(decomposition_depth / 5.0, 1.0) * 0.6 # æœ€å¤š5å±‚åˆ†è§£
        score += min(rules_activated / 3.0, 1.0) * 0.4   # æœ€å¤š3ä¸ªè§„åˆ™
        return score

    def _evaluate_efficiency(self, exec_time: float, cpu: float, mem_mb: float, complexity: int) -> float:
        """è¯„ä¼°æ‰§è¡Œæ•ˆç‡ (æ—¶é—´ã€CPUã€å†…å­˜)"""
        # ç›®æ ‡ï¼šå¤æ‚ä»»åŠ¡ï¼ˆ10ï¼‰åº”è¯¥åœ¨60ç§’å†…å®Œæˆ
        time_score = max(0.0, 1.0 - (exec_time / (complexity * 6)))
        
        # èµ„æºåˆ†æ•°ï¼šCPUå’Œå†…å­˜ä½¿ç”¨è¶Šä½è¶Šå¥½
        cpu_score = max(0.0, 1.0 - (cpu / 100.0))
        mem_score = max(0.0, 1.0 - (mem_mb / 512.0)) # å‡è®¾512MBæ˜¯èµ„æºå ç”¨çš„ä¸€ä¸ªé˜ˆå€¼
        
        return time_score * 0.5 + cpu_score * 0.25 + mem_score * 0.25

    # --- æŠ¥å‘Šç”Ÿæˆä¸ä¿å­˜ ---
    def _generate_report(self, system_names: List[str], results: Dict[str, TestResult]) -> Dict:
        # ... (ä¸V8ç‰ˆæœ¬ç±»ä¼¼ï¼Œä½†æ•°æ®ç»“æ„æ›´æ–°)
        # æ­¤å¤„ç®€åŒ–
        final_scores = defaultdict(list)
        for result in results.values():
            if result.success:
                final_scores[result.system_name].append(result.metrics['overall_score'])
        
        avg_scores = {name: statistics.mean(scores) if scores else 0 for name, scores in final_scores.items()}
        winner = max(avg_scores, key=avg_scores.get) if avg_scores else "N/A"
        
        return {
            "test_date": datetime.now().isoformat(),
            "winner": winner,
            "average_scores": avg_scores,
            "detailed_results": {k: asdict(v) for k, v in results.items()}
        }

    def _save_report(self, report: Dict):
        report_path = self.output_dir / f"ultimate_comparison_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=4, ensure_ascii=False, default=str)
        logger.info(f"ç»ˆæå¯¹æ¯”æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")


# --- ä¸»æ‰§è¡Œå‡½æ•° ---
async def main():
    logger.info("--- å¯åŠ¨ç»ˆææµ‹è¯•å¥—ä»¶V3 ---")
    suite = UltimateTestingSuite()
    
    try:
        await suite.initialize_systems()
        report = await suite.run_comparison(list(suite.systems.keys()))

        print("\n--- ç»ˆææµ‹è¯•æŠ¥å‘Šæ‘˜è¦ ---")
        print(f"æµ‹è¯•æ—¥æœŸ: {report['test_date']}")
        print(f"ğŸ† æœ€ç»ˆè·èƒœè€…: {report['winner']}")
        print("\nğŸ“Š ç³»ç»Ÿå¹³å‡æ€»åˆ†:")
        for system, score in report['average_scores'].items():
            print(f"  - {system}: {score:.3f}")
    except Exception as e:
        logger.error(f"æµ‹è¯•æ¡†æ¶æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
    finally:
        logger.info("--- å…³é—­æµ‹è¯•å¥—ä»¶ ---")


if __name__ == "__main__":
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())