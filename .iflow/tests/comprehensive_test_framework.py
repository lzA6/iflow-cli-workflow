#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§ª ç»¼åˆæµ‹è¯•æ¡†æ¶ V1.0
Comprehensive Testing Framework V1.0

ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import asyncio
import json
import logging
import time
import traceback
import unittest
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
try:
    from path_manager import get_path_manager
    from core.performance_optimizer import get_performance_optimizer
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥ä¾èµ–æ¨¡å—: {e}")
    get_path_manager = None
    get_performance_optimizer = None

logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    test_name: str
    status: str  # passed, failed, skipped, error
    duration: float
    error_message: Optional[str] = None
    traceback: Optional[str] = None
    metrics: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class TestSuite:
    """æµ‹è¯•å¥—ä»¶"""
    name: str
    tests: List[TestResult] = field(default_factory=list)
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0
    skipped_tests: int = 0
    error_tests: int = 0
    total_duration: float = 0.0
    success_rate: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)

class ComprehensiveTestFramework:
    """ç»¼åˆæµ‹è¯•æ¡†æ¶"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶"""
        self.path_manager = get_path_manager() if get_path_manager else None
        self.performance_optimizer = get_performance_optimizer() if get_performance_optimizer else None
        self.test_suites = []
        self.test_categories = {
            'unit': [],      # å•å…ƒæµ‹è¯•
            'integration': [], # é›†æˆæµ‹è¯•
            'performance': [], # æ€§èƒ½æµ‹è¯•
            'security': [],   # å®‰å…¨æµ‹è¯•
            'compatibility': [] # å…¼å®¹æ€§æµ‹è¯•
        }
        
        # é…ç½®æ—¥å¿—
        self._setup_logging()
        
        logger.info("ğŸ§ª ç»¼åˆæµ‹è¯•æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = self.path_manager.log_dir if self.path_manager else Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        # æµ‹è¯•æ—¥å¿—æ–‡ä»¶
        test_log_file = log_dir / f"test_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # é…ç½®æµ‹è¯•æ—¥å¿—
        test_logger = logging.getLogger("test_framework")
        test_logger.setLevel(logging.INFO)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(test_log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # æ ¼å¼åŒ–å™¨
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        test_logger.addHandler(file_handler)
        test_logger.addHandler(console_handler)
        
        self.test_logger = test_logger
        self.test_log_file = test_log_file
    
    def register_test(self, category: str, test_func: Callable, test_name: Optional[str] = None):
        """æ³¨å†Œæµ‹è¯•"""
        if category not in self.test_categories:
            self.test_categories[category] = []
        
        test_info = {
            'func': test_func,
            'name': test_name or test_func.__name__,
            'category': category
        }
        
        self.test_categories[category].append(test_info)
        self.test_logger.info(f"ğŸ“ æ³¨å†Œæµ‹è¯•: {category}.{test_info['name']}")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.test_logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ‰€æœ‰æµ‹è¯•...")
        
        all_results = {}
        overall_start = time.time()
        
        for category, tests in self.test_categories.items():
            if tests:
                self.test_logger.info(f"ğŸ“‚ è¿è¡Œ {category} æµ‹è¯•å¥—ä»¶...")
                suite_result = await self._run_test_suite(category, tests)
                all_results[category] = asdict(suite_result)
                self.test_suites.append(suite_result)
        
        overall_duration = time.time() - overall_start
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tests = sum(suite.total_tests for suite in self.test_suites)
        total_passed = sum(suite.passed_tests for suite in self.test_suites)
        total_failed = sum(suite.failed_tests for suite in self.test_suites)
        total_skipped = sum(suite.skipped_tests for suite in self.test_suites)
        total_errors = sum(suite.error_tests for suite in self.test_suites)
        
        overall_success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'total_duration': overall_duration,
            'total_tests': total_tests,
            'passed_tests': total_passed,
            'failed_tests': total_failed,
            'skipped_tests': total_skipped,
            'error_tests': total_errors,
            'overall_success_rate': overall_success_rate,
            'test_suites': all_results,
            'log_file': str(self.test_log_file)
        }
        
        self.test_logger.info(f"âœ… æµ‹è¯•å®Œæˆ - æ€»ä½“æˆåŠŸç‡: {overall_success_rate:.1f}%")
        
        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        await self._save_test_report(summary)
        
        return summary
    
    async def _run_test_suite(self, category: str, tests: List[Dict[str, Any]]) -> TestSuite:
        """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
        suite = TestSuite(name=category)
        suite_start = time.time()
        
        for test_info in tests:
            test_result = await self._run_single_test(test_info)
            suite.tests.append(test_result)
            
            # æ›´æ–°ç»Ÿè®¡
            suite.total_tests += 1
            suite.total_duration += test_result.duration
            
            if test_result.status == 'passed':
                suite.passed_tests += 1
            elif test_result.status == 'failed':
                suite.failed_tests += 1
            elif test_result.status == 'skipped':
                suite.skipped_tests += 1
            elif test_result.status == 'error':
                suite.error_tests += 1
        
        suite.success_rate = (suite.passed_tests / suite.total_tests * 100) if suite.total_tests > 0 else 0
        suite.duration = time.time() - suite_start
        
        self.test_logger.info(f"ğŸ“Š {category} å¥—ä»¶å®Œæˆ - æˆåŠŸç‡: {suite.success_rate:.1f}%")
        
        return suite
    
    async def _run_single_test(self, test_info: Dict[str, Any]) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        test_func = test_info['func']
        test_name = test_info['name']
        category = test_info['category']
        
        self.test_logger.info(f"ğŸ” è¿è¡Œæµ‹è¯•: {category}.{test_name}")
        
        start_time = time.time()
        
        try:
            # å¦‚æœæœ‰æ€§èƒ½ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨ä¼˜åŒ–æ‰§è¡Œ
            if self.performance_optimizer:
                result = await self.performance_optimizer.execute_with_optimization(test_func)
            else:
                if asyncio.iscoroutinefunction(test_func):
                    result = await test_func()
                else:
                    result = test_func()
            
            duration = time.time() - start_time
            
            # æ£€æŸ¥ç»“æœ
            if result is True or result is None:
                status = 'passed'
                error_message = None
                traceback_info = None
            else:
                status = 'failed'
                error_message = str(result)
                traceback_info = "æµ‹è¯•è¿”å›False"
        
        except AssertionError as e:
            duration = time.time() - start_time
            status = 'failed'
            error_message = str(e)
            traceback_info = traceback.format_exc()
        
        except Exception as e:
            duration = time.time() - start_time
            status = 'error'
            error_message = str(e)
            traceback_info = traceback.format_exc()
        
        # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        metrics = {}
        if self.performance_optimizer:
            metrics = self.performance_optimizer.get_performance_report()
        
        test_result = TestResult(
            test_name=test_name,
            status=status,
            duration=duration,
            error_message=error_message,
            traceback=traceback_info,
            metrics=metrics
        )
        
        status_icon = {"passed": "âœ…", "failed": "âŒ", "skipped": "â­ï¸", "error": "ğŸš¨"}[status]
        self.test_logger.info(f"{status_icon} {category}.{test_name}: {status} ({duration:.3f}s)")
        
        return test_result
    
    async def _save_test_report(self, summary: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        if not self.path_manager:
            return
        
        reports_dir = self.path_manager.project_root / "test_reports"
        reports_dir.mkdir(exist_ok=True)
        
        # JSONæŠ¥å‘Š
        json_file = reports_dir / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, default=str, ensure_ascii=False)
        
        # HTMLæŠ¥å‘Š
        html_file = reports_dir / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        html_content = self._generate_html_report(summary)
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        self.test_logger.info(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {json_file}, {html_file}")
    
    def _generate_html_report(self, summary: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>æµ‹è¯•æŠ¥å‘Š - {summary['timestamp']}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .summary {{ margin: 20px 0; }}
        .test-suite {{ margin: 20px 0; border: 1px solid #ddd; border-radius: 5px; }}
        .test-suite h3 {{ background: #f8f8f8; margin: 0; padding: 10px; }}
        .test-results {{ padding: 10px; }}
        .test-result {{ margin: 5px 0; padding: 5px; border-radius: 3px; }}
        .passed {{ background: #d4edda; }}
        .failed {{ background: #f8d7da; }}
        .error {{ background: #fff3cd; }}
        .skipped {{ background: #e2e3e5; }}
        table {{ width: 100%; border-collapse: collapse; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ§ª ç»¼åˆæµ‹è¯•æŠ¥å‘Š</h1>
        <p><strong>æ—¶é—´:</strong> {summary['timestamp']}</p>
        <p><strong>æ€»è€—æ—¶:</strong> {summary['total_duration']:.2f}ç§’</p>
    </div>
    
    <div class="summary">
        <h2>ğŸ“Š æ€»ä½“ç»Ÿè®¡</h2>
        <table>
            <tr><th>é¡¹ç›®</th><th>æ•°å€¼</th></tr>
            <tr><td>æ€»æµ‹è¯•æ•°</td><td>{summary['total_tests']}</td></tr>
            <tr><td>é€šè¿‡</td><td>{summary['passed_tests']}</td></tr>
            <tr><td>å¤±è´¥</td><td>{summary['failed_tests']}</td></tr>
            <tr><td>è·³è¿‡</td><td>{summary['skipped_tests']}</td></tr>
            <tr><td>é”™è¯¯</td><td>{summary['error_tests']}</td></tr>
            <tr><td>æˆåŠŸç‡</td><td>{summary['overall_success_rate']:.1f}%</td></tr>
        </table>
    </div>
"""
        
        # æ·»åŠ å„æµ‹è¯•å¥—ä»¶çš„è¯¦ç»†ç»“æœ
        for category, suite_data in summary['test_suites'].items():
            html += f"""
    <div class="test-suite">
        <h3>ğŸ“‚ {category.title()} æµ‹è¯•å¥—ä»¶</h3>
        <div class="test-results">
            <p><strong>æˆåŠŸç‡:</strong> {suite_data['success_rate']:.1f}% ({suite_data['passed_tests']}/{suite_data['total_tests']})</p>
"""
            
            # æ·»åŠ æµ‹è¯•ç»“æœåˆ—è¡¨
            for test in suite_data.get('tests', []):
                css_class = test['status']
                html += f"""
            <div class="test-result {css_class}">
                <strong>{test['test_name']}</strong>: {test['status']} 
                ({test['duration']:.3f}s)
                {f'<br><em>é”™è¯¯: {test["error_message"]}</em>' if test.get('error_message') else ''}
            </div>
"""
            
            html += """
        </div>
    </div>
"""
        
        html += """
</body>
</html>
"""
        return html

# å†…ç½®æµ‹è¯•å‡½æ•°
async def test_path_manager():
    """æµ‹è¯•è·¯å¾„ç®¡ç†å™¨"""
    if not get_path_manager:
        raise ImportError("PathManagerä¸å¯ç”¨")
    
    pm = get_path_manager()
    assert pm.project_root.exists(), "é¡¹ç›®æ ¹ç›®å½•ä¸å­˜åœ¨"
    assert pm.tools_dir.exists(), "å·¥å…·ç›®å½•ä¸å­˜åœ¨"
    assert len(pm.get_python_files()) > 0, "æ²¡æœ‰æ‰¾åˆ°Pythonæ–‡ä»¶"
    
    return True

async def test_performance_optimizer():
    """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨"""
    if not get_performance_optimizer:
        raise ImportError("PerformanceOptimizerä¸å¯ç”¨")
    
    optimizer = get_performance_optimizer()
    report = optimizer.get_performance_report()
    assert report is not None, "æ€§èƒ½æŠ¥å‘Šä¸ºç©º"
    
    return True

async def test_dependencies():
    """æµ‹è¯•ä¾èµ–"""
    dependencies = ['numpy', 'psutil', 'asyncio']
    for dep in dependencies:
        try:
            __import__(dep)
        except ImportError:
            raise ImportError(f"ä¾èµ– {dep} ä¸å¯ç”¨")
    
    return True

async def test_file_structure():
    """æµ‹è¯•æ–‡ä»¶ç»“æ„"""
    if not get_path_manager:
        raise ImportError("PathManagerä¸å¯ç”¨")
    
    pm = get_path_manager()
    
    # æ£€æŸ¥æ ¸å¿ƒç›®å½•
    required_dirs = ['core', 'tools', 'tests', 'hooks']
    for dir_name in required_dirs:
        dir_path = pm.project_root / ".iflow" / dir_name
        if not dir_path.exists():
            raise AssertionError(f"ç›®å½• {dir_name} ä¸å­˜åœ¨")
    
    return True

async def main():
    """ä¸»å‡½æ•° - è¿è¡Œæµ‹è¯•"""
    framework = ComprehensiveTestFramework()
    
    # æ³¨å†Œå†…ç½®æµ‹è¯•
    framework.register_test('unit', test_path_manager, "è·¯å¾„ç®¡ç†å™¨æµ‹è¯•")
    framework.register_test('unit', test_performance_optimizer, "æ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•")
    framework.register_test('integration', test_dependencies, "ä¾èµ–æµ‹è¯•")
    framework.register_test('integration', test_file_structure, "æ–‡ä»¶ç»“æ„æµ‹è¯•")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results = await framework.run_all_tests()
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    print(f"æ€»ä½“æˆåŠŸç‡: {results['overall_success_rate']:.1f}%")
    print(f"è¯¦ç»†æŠ¥å‘Š: {results['log_file']}")

if __name__ == "__main__":
    asyncio.run(main())
