#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§ª é›†æˆæµ‹è¯• V6
æµ‹è¯•å¤šæ¨¡å‹é€‚é…å™¨å’Œè‡ªæˆ‘è¿›åŒ–å¼•æ“çš„é›†æˆåŠŸèƒ½
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import asyncio
import time
import logging
import json
from pathlib import Path
from typing import Dict, List, Any

# åŠ¨æ€æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
try:
    project_root = Path(__file__).resolve().parent.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    from iflow.core.universal_llm_adapter_v14 import UniversalLLMAdapterV14, ModelConfig, ModelType, ModelProvider
    from iflow.core.self_evolution_engine_v6 import SelfEvolutionEngineV6, EvolutionType, EvolutionSource
except ImportError as e:
    print(f"Warning: Import failed: {e}")
    print("Using simplified version for testing...")

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class IntegrationTestV6:
    """é›†æˆæµ‹è¯•V6"""
    
    def __init__(self):
        self.test_results = []
        self.start_time = time.time()
        
        logger.info("ğŸ§ª é›†æˆæµ‹è¯•V6å¯åŠ¨")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹é›†æˆæµ‹è¯•V6")
        print("=" * 60)
        
        # 1. æµ‹è¯•å¤šæ¨¡å‹é€‚é…å™¨
        await self.test_llm_adapter()
        
        # 2. æµ‹è¯•è‡ªæˆ‘è¿›åŒ–å¼•æ“
        await self.test_evolution_engine()
        
        # 3. æµ‹è¯•é›†æˆåŠŸèƒ½
        await self.test_integration()
        
        # 4. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = self.generate_report()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š é›†æˆæµ‹è¯•å®Œæˆ")
        
        return report
    
    async def test_llm_adapter(self):
        """æµ‹è¯•å¤šæ¨¡å‹é€‚é…å™¨"""
        print("\nğŸŒ æµ‹è¯•å¤šæ¨¡å‹é€‚é…å™¨...")
        
        try:
            # åˆ›å»ºæµ‹è¯•é…ç½®
            configs = [
                ModelConfig(
                    model_type=ModelType.GPT,
                    provider=ModelProvider.OPENAI,
                    api_key="test-key",
                    base_url="https://api.openai.com/v1",
                    max_tokens=100,
                    temperature=0.7,
                    cost_per_token=0.001,
                    speed_score=0.9,
                    quality_score=0.95
                ),
                ModelConfig(
                    model_type=ModelType.CLAUDE,
                    provider=ModelProvider.ANTHROPIC,
                    api_key="test-key",
                    base_url="https://api.anthropic.com",
                    max_tokens=100,
                    temperature=0.7,
                    cost_per_token=0.002,
                    speed_score=0.8,
                    quality_score=0.9
                )
            ]
            
            # åˆå§‹åŒ–é€‚é…å™¨
            adapter = UniversalLLMAdapterV14(configs)
            await adapter.initialize()
            
            # æµ‹è¯•çŠ¶æ€è·å–
            status = adapter.get_system_status()
            
            # è®°å½•æµ‹è¯•ç»“æœ
            self.test_results.append({
                "test_name": "llm_adapter_initialization",
                "success": True,
                "details": {
                    "available_models": [m.value for m in status.get("available_models", [])],
                    "total_adapters": status.get("total_adapters", 0)
                }
            })
            
            print(f"  âœ… LLM Adapter initialized successfully, supports {len(status.get('available_models', []))} models")
            
            # æµ‹è¯•æ¨¡å‹è°ƒç”¨
            test_prompt = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æç¤ºï¼Œç”¨äºéªŒè¯æ¨¡å‹è°ƒç”¨åŠŸèƒ½ã€‚"
            result = await adapter.call(test_prompt)
            
            self.test_results.append({
                "test_name": "llm_adapter_call",
                "success": result["success"],
                "details": {
                    "prompt_length": len(test_prompt),
                    "response_length": len(result.get("response", "")) if result["success"] else 0,
                    "model_used": result.get("metadata", {}).get("model", "unknown") if result["success"] else None
                }
            })
            
            if result["success"]:
                print(f"  âœ… Model call successful, response length: {len(result.get('response', ''))}")
            else:
                print(f"  âŒ Model call failed: {result.get('error', 'unknown error')}")
            
            # å…³é—­é€‚é…å™¨
            await adapter.shutdown()
            
        except Exception as e:
            logger.error(f"âŒ å¤šæ¨¡å‹é€‚é…å™¨æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                "test_name": "llm_adapter_test",
                "success": False,
                "error": str(e)
            })
    
    async def test_evolution_engine(self):
        """æµ‹è¯•è‡ªæˆ‘è¿›åŒ–å¼•æ“"""
        print("\nTesting Evolution Engine...")
        
        try:
            # åˆå§‹åŒ–è¿›åŒ–å¼•æ“
            engine = SelfEvolutionEngineV6("data/test_evolution.db")
            await engine.initialize()
            
            # æµ‹è¯•è¿›åŒ–åŠŸèƒ½
            experience_data = [
                {
                    "response_time": 1.5 + i * 0.1,
                    "success_rate": 0.95 - i * 0.01,
                    "memory_usage": 100 + i * 10,
                    "timestamp": time.time() - i * 100
                }
                for i in range(10)
            ]
            
            evolution_result = await engine.evolve_based_on_experience(experience_data)
            
            self.test_results.append({
                "test_name": "evolution_engine_evolve",
                "success": evolution_result["success"],
                "details": {
                    "patterns_found": evolution_result.get("patterns_found", 0),
                    "improvements_suggested": evolution_result.get("improvements_suggested", 0),
                    "evolution_id": evolution_result.get("evolution_id", "")
                }
            })
            
            print(f"  âœ… Evolution analysis completed, found {evolution_result.get('patterns_found', 0)} patterns")
            
            # æµ‹è¯•ç›®æ ‡è®¾ç½®
            goal_id = await engine.set_evolution_goal(
                "æ€§èƒ½ä¼˜åŒ–ç›®æ ‡",
                {"avg_response_time": 1.0, "success_rate": 0.98},
                priority=8
            )
            
            self.test_results.append({
                "test_name": "evolution_engine_goal_setting",
                "success": bool(goal_id),
                "details": {
                    "goal_id": goal_id
                }
            })
            
            print(f"  âœ… Evolution goal set successfully: {goal_id}")
            
            # æµ‹è¯•çŠ¶æ€è·å–
            status = await engine.get_evolution_status()
            
            self.test_results.append({
                "test_name": "evolution_engine_status",
                "success": True,
                "details": {
                    "total_records": status.get("total_evolution_records", 0),
                    "total_patterns": status.get("total_learning_patterns", 0),
                    "active_goals": status.get("active_goals", 0)
                }
            })
            
            print(f"  âœ… Evolution status: {status.get('total_evolution_records', 0)} records, {status.get('total_learning_patterns', 0)} patterns")
            
            # å…³é—­å¼•æ“
            await engine.shutdown()
            
        except Exception as e:
            logger.error(f"âŒ è‡ªæˆ‘è¿›åŒ–å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                "test_name": "evolution_engine_test",
                "success": False,
                "error": str(e)
            })
    
    async def test_integration(self):
        """æµ‹è¯•é›†æˆåŠŸèƒ½"""
        print("\nTesting Integration...")
        
        try:
            # æ¨¡æ‹Ÿå®Œæ•´çš„é›†æˆåœºæ™¯
            integration_scenario = {
                "step": "integration_test",
                "description": "æµ‹è¯•å¤šæ¨¡å‹é€‚é…å™¨ä¸è‡ªæˆ‘è¿›åŒ–å¼•æ“çš„ååŒå·¥ä½œ",
                "start_time": time.time(),
                "models_tested": [],
                "evolution_triggers": []
            }
            
            # 1. åˆå§‹åŒ–ä¸¤ä¸ªç»„ä»¶
            configs = [
                ModelConfig(
                    model_type=ModelType.GPT,
                    provider=ModelProvider.OPENAI,
                    api_key="test-key",
                    max_tokens=50,
                    temperature=0.5
                )
            ]
            
            adapter = UniversalLLMAdapterV14(configs)
            await adapter.initialize()
            
            engine = SelfEvolutionEngineV6("data/test_integration.db")
            await engine.initialize()
            
            # 2. æ‰§è¡Œå¤šæ¬¡æ¨¡å‹è°ƒç”¨ï¼Œæ”¶é›†æ€§èƒ½æ•°æ®
            performance_data = []
            
            for i in range(5):
                start_time = time.time()
                test_prompt = f"æµ‹è¯•æç¤º {i+1}: è¯·ç®€è¦è¯´æ˜äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚"
                
                result = await adapter.call(test_prompt)
                response_time = time.time() - start_time
                
                performance_data.append({
                    "call_number": i + 1,
                    "response_time": response_time,
                    "success": result["success"],
                    "model": result.get("metadata", {}).get("model", "unknown") if result["success"] else None,
                    "timestamp": time.time()
                })
                
                integration_scenario["models_tested"].append({
                    "call": i + 1,
                    "success": result["success"],
                    "response_time": response_time
                })
            
            # 3. åŸºäºæ€§èƒ½æ•°æ®è§¦å‘è¿›åŒ–
            evolution_trigger = {
                "type": "performance_analysis",
                "data": performance_data,
                "analysis_time": time.time()
            }
            
            integration_scenario["evolution_triggers"].append(evolution_trigger)
            
            # 4. æ‰§è¡Œè¿›åŒ–åˆ†æ
            evolution_result = await engine.evolve_based_on_experience(performance_data)
            
            # 5. éªŒè¯é›†æˆæ•ˆæœ
            integration_result = {
                "success": True,
                "models_tested": len(performance_data),
                "successful_calls": sum(1 for data in performance_data if data["success"]),
                "avg_response_time": sum(data["response_time"] for data in performance_data) / len(performance_data),
                "evolution_triggered": evolution_result["success"],
                "patterns_found": evolution_result.get("patterns_found", 0),
                "improvements_suggested": evolution_result.get("improvements_suggested", 0)
            }
            
            self.test_results.append({
                "test_name": "integration_test",
                "success": True,
                "details": integration_result
            })
            
            print(f"  âœ… Integration test completed:")
            print(f"    - Model calls: {integration_result['models_tested']} times")
            print(f"    - Successful calls: {integration_result['successful_calls']} times")
            print(f"    - Average response time: {integration_result['avg_response_time']:.2f} seconds")
            print(f"    - Evolution triggered: {integration_result['evolution_triggered']}")
            print(f"    - Patterns found: {integration_result['patterns_found']} patterns")
            
            # 6. æ¸…ç†èµ„æº
            await adapter.shutdown()
            await engine.shutdown()
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                "test_name": "integration_test",
                "success": False,
                "error": str(e)
            })
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results if result["success"])
        failed_tests = total_tests - successful_tests
        
        # è®¡ç®—æµ‹è¯•è¦†ç›–ç‡
        test_coverage = {
            "llm_adapter": any("llm_adapter" in result["test_name"] for result in self.test_results),
            "evolution_engine": any("evolution_engine" in result["test_name"] for result in self.test_results),
            "integration": any("integration" in result["test_name"] for result in self.test_results)
        }
        
        # æå–å…³é”®æŒ‡æ ‡
        key_metrics = {}
        for result in self.test_results:
            if result["success"] and "details" in result:
                details = result["details"]
                if "avg_response_time" in details:
                    key_metrics["avg_response_time"] = details["avg_response_time"]
                if "patterns_found" in details:
                    key_metrics["total_patterns"] = details.get("patterns_found", 0)
                if "improvements_suggested" in details:
                    key_metrics["total_improvements"] = details.get("improvements_suggested", 0)
        
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": failed_tests,
                "success_rate": successful_tests / total_tests if total_tests > 0 else 0,
                "test_duration": time.time() - self.start_time
            },
            "test_coverage": test_coverage,
            "key_metrics": key_metrics,
            "test_details": self.test_results,
            "recommendations": self._generate_recommendations()
        }
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        failed_tests = [result for result in self.test_results if not result["success"]]
        
        if failed_tests:
            recommendations.append("ä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
        
        # æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
        avg_response_time = None
        for result in self.test_results:
            if result["success"] and "details" in result and "avg_response_time" in result["details"]:
                avg_response_time = result["details"]["avg_response_time"]
                break
        
        if avg_response_time and avg_response_time > 2.0:
            recommendations.append("ä¼˜åŒ–æ¨¡å‹è°ƒç”¨å“åº”æ—¶é—´ï¼Œå½“å‰å¹³å‡å“åº”æ—¶é—´è¶…è¿‡2ç§’")
        
        # æ£€æŸ¥è¿›åŒ–åŠŸèƒ½
        evolution_tests = [result for result in self.test_results if "evolution" in result["test_name"]]
        if evolution_tests and all(test["success"] for test in evolution_tests):
            recommendations.append("è¿›åŒ–å¼•æ“å·¥ä½œæ­£å¸¸ï¼Œå»ºè®®å¢åŠ æ›´å¤šå­¦ä¹ å™¨ç±»å‹")
        
        if not recommendations:
            recommendations.append("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿè¿è¡Œè‰¯å¥½")
        
        return recommendations

# --- ä¸»æµ‹è¯•å‡½æ•° ---
async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª Integration Test V6")
    print("Testing LLM Adapter and Evolution Engine integration")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    tester = IntegrationTestV6()
    
    # è¿è¡Œæµ‹è¯•
    report = await tester.run_all_tests()
    
    # æ‰“å°è¯¦ç»†æŠ¥å‘Š
    print("\nğŸ“Š Test Report")
    print("=" * 60)
    
    summary = report["test_summary"]
    print(f"ğŸ“‹ Test Summary:")
    print(f"  - Total tests: {summary['total_tests']}")
    print(f"  - Successful tests: {summary['successful_tests']}")
    print(f"  - Failed tests: {summary['failed_tests']}")
    print(f"  - Success rate: {summary['success_rate']:.1%}")
    print(f"  - Test duration: {summary['test_duration']:.2f} seconds")
    
    print(f"\nğŸ” Test Coverage:")
    for component, covered in report["test_coverage"].items():
        status = "âœ…" if covered else "âŒ"
        print(f"  {status} {component}")
    
    if report["key_metrics"]:
        print(f"\nğŸ“ˆ Key Metrics:")
        for metric, value in report["key_metrics"].items():
            print(f"  - {metric}: {value}")
    
    print(f"\nğŸ’¡ Recommendations:")
    for i, recommendation in enumerate(report["recommendations"], 1):
        print(f"  {i}. {recommendation}")
    
    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    report_file = "iflow/tests/reports/integration_test_report_v6.json"
    os.makedirs(os.path.dirname(report_file), exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ Test report saved to: {report_file}")
    
    # è¿”å›æµ‹è¯•ç»“æœ
    return summary["success_rate"] > 0.8  # 80%ä»¥ä¸ŠæˆåŠŸç‡è®¤ä¸ºæµ‹è¯•é€šè¿‡

if __name__ == "__main__":
    # ç¡®ä¿åœ¨Windowsä¸Šasyncioäº‹ä»¶å¾ªç¯æ­£å¸¸å·¥ä½œ
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    try:
        success = asyncio.run(main())
        if success:
            print("\nğŸ‰ Integration test passed!")
            sys.exit(0)
        else:
            print("\nâš ï¸ Integration test partially failed, please check failed test cases")
            sys.exit(1)
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}", exc_info=True)
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
        sys.exit(1)