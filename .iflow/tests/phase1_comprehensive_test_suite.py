#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 1 ç»¼åˆæµ‹è¯•å¥—ä»¶
===================

è¿™æ˜¯Phase 1çš„ç»¼åˆæµ‹è¯•å¥—ä»¶ï¼Œç”¨äºéªŒè¯ï¼š
1. æ™ºèƒ½ä½“Promptå¼•æ“V1åŠŸèƒ½
2. æ™ºèƒ½Hooksç³»ç»ŸV9åŠŸèƒ½  
3. å¢å¼ºç‰ˆæ™ºèƒ½ä½“æ¡†æ¶V2åŠŸèƒ½
4. ç³»ç»Ÿé›†æˆå’Œåä½œèƒ½åŠ›

æµ‹è¯•è¦†ç›–ï¼š
- âœ… å•å…ƒæµ‹è¯•ï¼šå„ä¸ªç»„ä»¶çš„ç‹¬ç«‹åŠŸèƒ½
- âœ… é›†æˆæµ‹è¯•ï¼šç»„ä»¶é—´çš„åä½œ
- âœ… æ€§èƒ½æµ‹è¯•ï¼šå“åº”æ—¶é—´å’Œç¨³å®šæ€§
- âœ… é”™è¯¯æ¢å¤ï¼šå®¹é”™å’Œæ¢å¤èƒ½åŠ›
- âœ… è¶…çº§æ€è€ƒæ¨¡å¼ï¼šå¼ºåˆ¶æ·±åº¦æ€è€ƒéªŒè¯
"""

import asyncio
import json
import time
import logging
import unittest
import sys
import os
from typing import Dict, List, Any
from unittest.mock import Mock, patch, MagicMock

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æµ‹è¯•æ¨¡å—
from iflow.agents.core.super_agent_prompt_engine_v1 import (
    SuperAgentPromptEngine, AgentSpecialization, ExpertiseLevel, AgentProfile
)
from iflow.hooks.intelligent_hooks_system_v9 import (
    IntelligentHooksSystemV9, HookEventType, HookActionType, 
    HookPriority, HookExecutionMode, HookDefinition, HookCondition, HookAction
)
from iflow.agents.enhanced_expert_agent_framework_v2 import (
    EnhancedExpertAgentFrameworkV2, AgentTask, TaskComplexity, AgentStatus,
    EnhancedAgent
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('.iflow/logs/test_results.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = "1.0.0"
__author__ = "iFlow Team"
__description__ = "Phase 1 ç»¼åˆæµ‹è¯•å¥—ä»¶"


class TestResults:
    """æµ‹è¯•ç»“æœæ”¶é›†å™¨"""
    
    def __init__(self):
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
        self.test_details = []
        self.start_time = time.time()
    
    def add_test_result(self, test_name: str, passed: bool, details: str = ""):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        self.total_tests += 1
        if passed:
            self.passed_tests += 1
            logger.info(f"âœ… {test_name}: é€šè¿‡")
        else:
            self.failed_tests += 1
            logger.error(f"âŒ {test_name}: å¤±è´¥ - {details}")
        
        self.test_details.append({
            "name": test_name,
            "passed": passed,
            "details": details,
            "timestamp": time.time()
        })
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–æµ‹è¯•æ‘˜è¦"""
        duration = time.time() - self.start_time
        success_rate = (self.passed_tests / max(self.total_tests, 1)) * 100
        
        return {
            "total_tests": self.total_tests,
            "passed_tests": self.passed_tests,
            "failed_tests": self.failed_tests,
            "success_rate": success_rate,
            "duration": duration,
            "status": "PASSED" if self.failed_tests == 0 else "FAILED"
        }
    
    def save_results(self, file_path: str):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            results_data = {
                "summary": self.get_summary(),
                "details": self.test_details,
                "timestamp": time.time(),
                "version": __version__
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {file_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")


class Phase1ComprehensiveTestSuite:
    """Phase 1 ç»¼åˆæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.results = TestResults()
        self.test_data = self._load_test_data()
        
        logger.info("ğŸš€ å¼€å§‹ Phase 1 ç»¼åˆæµ‹è¯•å¥—ä»¶")
        logger.info(f"ğŸ“Š æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.test_data)} ç»„æµ‹è¯•ç”¨ä¾‹")
    
    def _load_test_data(self) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        return {
            "prompt_engine": {
                "agents": ["æŠ€æœ¯æ„¿æ™¯å¸ˆ", "å…¨æ ˆå·¥ç¨‹å¸ˆ", "è´¨é‡å·¥ç¨‹å¸ˆ", "åˆ›æ–°å‘ç°å¸ˆ", "ç³»ç»Ÿè¿›åŒ–å¸ˆ"],
                "contexts": [
                    {"project_type": "web_application", "tech_stack": ["Python", "React"]},
                    {"company_size": "enterprise", "industry": "fintech"},
                    {"task_complexity": "high", "deadline": "1ä¸ªæœˆ"}
                ]
            },
            "hooks_system": {
                "events": [event.value for event in HookEventType],
                "conditions": [
                    {"type": "event_matcher", "pattern": "*"},
                    {"type": "context_matcher", "key": "test", "operator": "==", "value": "value"}
                ],
                "actions": [
                    {"type": HookActionType.LOG, "message": "æµ‹è¯•æ¶ˆæ¯"},
                    {"type": HookActionType.FUNCTION, "function": "test_function"}
                ]
            },
            "agent_framework": {
                "tasks": [
                    {
                        "task_id": "test_task_001",
                        "task_type": "development",
                        "complexity": TaskComplexity.MODERATE,
                        "description": "å¼€å‘ä¸€ä¸ªç®€å•çš„Webåº”ç”¨"
                    }
                ]
            }
        }
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹è¿è¡Œ Phase 1 ç»¼åˆæµ‹è¯•")
        
        # 1. æµ‹è¯•æ™ºèƒ½ä½“Promptå¼•æ“
        await self.test_prompt_engine()
        
        # 2. æµ‹è¯•Hooksç³»ç»Ÿ
        await self.test_hooks_system()
        
        # 3. æµ‹è¯•æ™ºèƒ½ä½“æ¡†æ¶
        await self.test_agent_framework()
        
        # 4. æµ‹è¯•ç³»ç»Ÿé›†æˆ
        await self.test_system_integration()
        
        # 5. æµ‹è¯•æ€§èƒ½æŒ‡æ ‡
        await self.test_performance_metrics()
        
        # 6. æµ‹è¯•é”™è¯¯æ¢å¤
        await self.test_error_recovery()
        
        # 7. æµ‹è¯•è¶…çº§æ€è€ƒæ¨¡å¼
        await self.test_super_thinking_mode()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        summary = self.results.get_summary()
        logger.info("ğŸ¯ Phase 1 ç»¼åˆæµ‹è¯•å®Œæˆ")
        logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœ: {summary['passed_tests']}/{summary['total_tests']} é€šè¿‡")
        logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.2f}%")
        logger.info(f"â±ï¸ è€—æ—¶: {summary['duration']:.2f}ç§’")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        self.results.save_results('.iflow/tests/phase1_test_results.json')
        
        return summary
    
    async def test_prompt_engine(self):
        """æµ‹è¯•æ™ºèƒ½ä½“Promptå¼•æ“"""
        logger.info("ğŸ§ª æµ‹è¯•æ™ºèƒ½ä½“Promptå¼•æ“")
        
        try:
            # åˆ›å»ºPromptå¼•æ“å®ä¾‹
            engine = SuperAgentPromptEngine()
            
            # æµ‹è¯•1: éªŒè¯æ™ºèƒ½ä½“æ¡£æ¡ˆåŠ è½½
            agents = engine.agent_profiles
            test1_passed = len(agents) >= 5
            self.results.add_test_result(
                "æ™ºèƒ½ä½“æ¡£æ¡ˆåŠ è½½", 
                test1_passed, 
                f"åŠ è½½äº† {len(agents)} ä¸ªæ™ºèƒ½ä½“" if test1_passed else "æ™ºèƒ½ä½“æ•°é‡ä¸è¶³"
            )
            
            # æµ‹è¯•2: éªŒè¯Promptæ¨¡æ¿ç”Ÿæˆ
            prompt = engine.generate_agent_prompt("å…¨æ ˆå·¥ç¨‹å¸ˆ", {"test": "context"})
            test2_passed = len(prompt) > 1000 and "ultrathink" in prompt
            self.results.add_test_result(
                "Promptæ¨¡æ¿ç”Ÿæˆ", 
                test2_passed, 
                "Promptç”ŸæˆæˆåŠŸ" if test2_passed else "Promptç”Ÿæˆå¤±è´¥æˆ–ç¼ºå°‘è¶…çº§æ€è€ƒæ¨¡å¼"
            )
            
            # æµ‹è¯•3: éªŒè¯Promptè´¨é‡éªŒè¯
            validation = engine.validate_prompt(prompt, "å…¨æ ˆå·¥ç¨‹å¸ˆ")
            test3_passed = validation["valid"] and validation["score"] >= 80
            self.results.add_test_result(
                "Promptè´¨é‡éªŒè¯", 
                test3_passed, 
                f"è´¨é‡åˆ†æ•°: {validation['score']}" if test3_passed else f"éªŒè¯å¤±è´¥: {validation['issues']}"
            )
            
            # æµ‹è¯•4: éªŒè¯ä¸“ä¸šåŒ–å·¥å…·é›†
            capabilities = engine.get_agent_capabilities("æŠ€æœ¯æ„¿æ™¯å¸ˆ")
            test4_passed = len(capabilities.get("tool_capabilities", [])) > 0
            self.results.add_test_result(
                "ä¸“ä¸šåŒ–å·¥å…·é›†", 
                test4_passed, 
                "å·¥å…·é›†åŠ è½½æˆåŠŸ" if test4_passed else "å·¥å…·é›†ä¸ºç©º"
            )
            
        except Exception as e:
            self.results.add_test_result("æ™ºèƒ½ä½“Promptå¼•æ“", False, f"å¼‚å¸¸: {str(e)}")
    
    async def test_hooks_system(self):
        """æµ‹è¯•Hooksç³»ç»Ÿ"""
        logger.info("ğŸ§ª æµ‹è¯•æ™ºèƒ½Hooksç³»ç»Ÿ")
        
        try:
            # åˆ›å»ºHooksç³»ç»Ÿå®ä¾‹
            hooks_system = IntelligentHooksSystemV9()
            
            # æµ‹è¯•1: éªŒè¯Hookæ³¨å†Œ
            test_hook = HookDefinition(
                name="test_hook",
                events=[HookEventType.PRE_TOOL_USE.value],
                conditions=[
                    HookCondition(
                        type="event_matcher",
                        config={"pattern": "*"}
                    )
                ],
                actions=[
                    HookAction(
                        type=HookActionType.LOG,
                        config={"level": "INFO", "message": "æµ‹è¯•Hook"}
                    )
                ],
                priority=HookPriority.MEDIUM,
                execution_mode=HookExecutionMode.ASYNC
            )
            
            registration_success = hooks_system.register_hook(test_hook)
            self.results.add_test_result(
                "Hookæ³¨å†Œ", 
                registration_success, 
                "Hookæ³¨å†ŒæˆåŠŸ" if registration_success else "Hookæ³¨å†Œå¤±è´¥"
            )
            
            # æµ‹è¯•2: éªŒè¯Hookè§¦å‘
            results = await hooks_system.trigger_hook(
                HookEventType.PRE_TOOL_USE.value,
                {"test": "context"},
                {"test": "data"}
            )
            test2_passed = len(results) > 0
            self.results.add_test_result(
                "Hookè§¦å‘", 
                test2_passed, 
                f"è§¦å‘äº† {len(results)} ä¸ªHook" if test2_passed else "Hookè§¦å‘å¤±è´¥"
            )
            
            # æµ‹è¯•3: éªŒè¯æ¡ä»¶åŒ¹é…
            matching_hooks = hooks_system.get_all_hooks()
            test3_passed = len(matching_hooks) > 0
            self.results.add_test_result(
                "æ¡ä»¶åŒ¹é…", 
                test3_passed, 
                f"æ‰¾åˆ° {len(matching_hooks)} ä¸ªåŒ¹é…çš„Hook" if test3_passed else "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„Hook"
            )
            
            # æµ‹è¯•4: éªŒè¯æ€§èƒ½ç›‘æ§
            stats = hooks_system.get_hook_statistics("test_hook")
            test4_passed = "execution_count" in stats
            self.results.add_test_result(
                "æ€§èƒ½ç›‘æ§", 
                test4_passed, 
                "æ€§èƒ½ç›‘æ§æ­£å¸¸" if test4_passed else "æ€§èƒ½ç›‘æ§å¼‚å¸¸"
            )
            
        except Exception as e:
            self.results.add_test_result("æ™ºèƒ½Hooksç³»ç»Ÿ", False, f"å¼‚å¸¸: {str(e)}")
    
    async def test_agent_framework(self):
        """æµ‹è¯•æ™ºèƒ½ä½“æ¡†æ¶"""
        logger.info("ğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆæ™ºèƒ½ä½“æ¡†æ¶")
        
        try:
            # åˆ›å»ºæ™ºèƒ½ä½“æ¡†æ¶å®ä¾‹
            framework = EnhancedExpertAgentFrameworkV2()
            
            # æµ‹è¯•1: éªŒè¯æ™ºèƒ½ä½“åˆ›å»º
            agents = framework.get_available_agents()
            test1_passed = len(agents) >= 5
            self.results.add_test_result(
                "æ™ºèƒ½ä½“åˆ›å»º", 
                test1_passed, 
                f"åˆ›å»ºäº† {len(agents)} ä¸ªæ™ºèƒ½ä½“" if test1_passed else "æ™ºèƒ½ä½“åˆ›å»ºå¤±è´¥"
            )
            
            # æµ‹è¯•2: éªŒè¯ä»»åŠ¡åˆ†é…
            test_task = AgentTask(
                task_id="test_task_001",
                task_type="development",
                complexity=TaskComplexity.MODERATE,
                description="æµ‹è¯•ä»»åŠ¡",
                requirements=["æµ‹è¯•è¦æ±‚"],
                context={"test": "context"}
            )
            
            assigned_agent = await framework.assign_task(test_task)
            test2_passed = assigned_agent in [agent["name"] for agent in agents]
            self.results.add_test_result(
                "ä»»åŠ¡åˆ†é…", 
                test2_passed, 
                f"åˆ†é…ç»™: {assigned_agent}" if test2_passed else "ä»»åŠ¡åˆ†é…å¤±è´¥"
            )
            
            # æµ‹è¯•3: éªŒè¯ä»»åŠ¡æ‰§è¡Œ
            if test_task.task_id in framework.active_tasks:
                result = await framework.execute_task(test_task.task_id)
                test3_passed = result is not None
                self.results.add_test_result(
                    "ä»»åŠ¡æ‰§è¡Œ", 
                    test3_passed, 
                    "ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ" if test3_passed else "ä»»åŠ¡æ‰§è¡Œå¤±è´¥"
                )
            
            # æµ‹è¯•4: éªŒè¯æ€§èƒ½ç›‘æ§
            performance = framework.get_agent_performance_report(assigned_agent)
            test4_passed = "total_tasks" in performance
            self.results.add_test_result(
                "æ€§èƒ½ç›‘æ§", 
                test4_passed, 
                "æ€§èƒ½ç›‘æ§æ­£å¸¸" if test4_passed else "æ€§èƒ½ç›‘æ§å¼‚å¸¸"
            )
            
        except Exception as e:
            self.results.add_test_result("å¢å¼ºç‰ˆæ™ºèƒ½ä½“æ¡†æ¶", False, f"å¼‚å¸¸: {str(e)}")
    
    async def test_system_integration(self):
        """æµ‹è¯•ç³»ç»Ÿé›†æˆ"""
        logger.info("ğŸ§ª æµ‹è¯•ç³»ç»Ÿé›†æˆ")
        
        try:
            # æµ‹è¯•1: éªŒè¯Promptå¼•æ“ä¸æ™ºèƒ½ä½“æ¡†æ¶é›†æˆ
            framework = EnhancedExpertAgentFrameworkV2()
            agent = framework.agents.get("å…¨æ ˆå·¥ç¨‹å¸ˆ")
            
            if agent:
                prompt = agent.generate_specialized_prompt({"test": "integration"})
                test1_passed = len(prompt) > 500 and "ultrathink" in prompt
                self.results.add_test_result(
                    "Promptå¼•æ“é›†æˆ", 
                    test1_passed, 
                    "é›†æˆæˆåŠŸ" if test1_passed else "é›†æˆå¤±è´¥"
                )
            
            # æµ‹è¯•2: éªŒè¯Hooksç³»ç»Ÿä¸æ™ºèƒ½ä½“æ¡†æ¶é›†æˆ
            hooks_system = framework.hooks_system
            hooks = hooks_system.get_all_hooks()
            test2_passed = len(hooks) > 0
            self.results.add_test_result(
                "Hooksç³»ç»Ÿé›†æˆ", 
                test2_passed, 
                f"é›†æˆäº† {len(hooks)} ä¸ªHook" if test2_passed else "é›†æˆå¤±è´¥"
            )
            
            # æµ‹è¯•3: éªŒè¯ç«¯åˆ°ç«¯æµç¨‹
            test_task = AgentTask(
                task_id="integration_test_001",
                task_type="analysis",
                complexity=TaskComplexity.SIMPLE,
                description="é›†æˆæµ‹è¯•ä»»åŠ¡",
                requirements=["æµ‹è¯•é›†æˆæµç¨‹"],
                context={"test": "end_to_end"}
            )
            
            assigned_agent = await framework.assign_task(test_task)
            result = await framework.execute_task(test_task.task_id)
            test3_passed = result is not None and hasattr(result, 'success')
            self.results.add_test_result(
                "ç«¯åˆ°ç«¯æµç¨‹", 
                test3_passed, 
                "ç«¯åˆ°ç«¯æµç¨‹æ­£å¸¸" if test3_passed else "ç«¯åˆ°ç«¯æµç¨‹å¼‚å¸¸"
            )
            
        except Exception as e:
            self.results.add_test_result("ç³»ç»Ÿé›†æˆ", False, f"å¼‚å¸¸: {str(e)}")
    
    async def test_performance_metrics(self):
        """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡"""
        logger.info("ğŸ§ª æµ‹è¯•æ€§èƒ½æŒ‡æ ‡")
        
        try:
            # æµ‹è¯•1: æµ‹è¯•å“åº”æ—¶é—´
            start_time = time.time()
            
            framework = EnhancedExpertAgentFrameworkV2()
            agents = framework.get_available_agents()
            response_time = time.time() - start_time
            
            test1_passed = response_time < 5.0  # 5ç§’å†…å“åº”
            self.results.add_test_result(
                "å“åº”æ—¶é—´æµ‹è¯•", 
                test1_passed, 
                f"å“åº”æ—¶é—´: {response_time:.2f}s" if test1_passed else f"å“åº”æ—¶é—´è¿‡é•¿: {response_time:.2f}s"
            )
            
            # æµ‹è¯•2: æµ‹è¯•å¹¶å‘å¤„ç†èƒ½åŠ›
            async def create_task(task_id: str):
                task = AgentTask(
                    task_id=task_id,
                    task_type="development",
                    complexity=TaskComplexity.SIMPLE,
                    description=f"å¹¶å‘æµ‹è¯•ä»»åŠ¡ {task_id}",
                    requirements=["æµ‹è¯•"],
                    context={"test": "concurrent"}
                )
                return await framework.assign_task(task)
            
            start_time = time.time()
            tasks = [create_task(f"concurrent_task_{i}") for i in range(5)]
            assigned_agents = await asyncio.gather(*tasks, return_exceptions=True)
            concurrent_time = time.time() - start_time
            
            successful_assignments = sum(1 for agent in assigned_agents if isinstance(agent, str))
            test2_passed = successful_assignments >= 4 and concurrent_time < 10.0
            self.results.add_test_result(
                "å¹¶å‘å¤„ç†èƒ½åŠ›", 
                test2_passed, 
                f"æˆåŠŸåˆ†é… {successful_assignments}/5 ä¸ªä»»åŠ¡ï¼Œè€—æ—¶: {concurrent_time:.2f}s" 
                if test2_passed else f"å¹¶å‘å¤„ç†å¤±è´¥ï¼Œè€—æ—¶: {concurrent_time:.2f}s"
            )
            
            # æµ‹è¯•3: æµ‹è¯•å†…å­˜ä½¿ç”¨
            import psutil
            process = psutil.Process()
            memory_usage = process.memory_info().rss / 1024 / 1024  # MB
            
            test3_passed = memory_usage < 500  # 500MBä»¥å†…
            self.results.add_test_result(
                "å†…å­˜ä½¿ç”¨", 
                test3_passed, 
                f"å†…å­˜ä½¿ç”¨: {memory_usage:.2f}MB" if test3_passed else f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_usage:.2f}MB"
            )
            
        except Exception as e:
            self.results.add_test_result("æ€§èƒ½æŒ‡æ ‡", False, f"å¼‚å¸¸: {str(e)}")
    
    async def test_error_recovery(self):
        """æµ‹è¯•é”™è¯¯æ¢å¤"""
        logger.info("ğŸ§ª æµ‹è¯•é”™è¯¯æ¢å¤")
        
        try:
            # æµ‹è¯•1: æµ‹è¯•Hooké”™è¯¯æ¢å¤
            hooks_system = IntelligentHooksSystemV9()
            
            # æ³¨å†Œä¸€ä¸ªå¯èƒ½å¤±è´¥çš„Hook
            error_hook = HookDefinition(
                name="error_test_hook",
                events=[HookEventType.SYSTEM_ERROR.value],
                conditions=[
                    HookCondition(
                        type="context_matcher",
                        config={"key": "error_test", "operator": "==", "value": True}
                    )
                ],
                actions=[
                    HookAction(
                        type=HookActionType.FUNCTION,
                        config={"function_name": "nonexistent_function"}
                    )
                ],
                priority=HookPriority.HIGH,
                execution_mode=HookExecutionMode.SYNC
            )
            
            hooks_system.register_hook(error_hook)
            
            # è§¦å‘é”™è¯¯Hook
            results = await hooks_system.trigger_hook(
                HookEventType.SYSTEM_ERROR.value,
                {"error_test": True}
            )
            
            # æ£€æŸ¥é”™è¯¯å¤„ç†
            error_results = [r for r in results if not r.success]
            test1_passed = len(error_results) > 0  # ç¡®å®æœ‰é”™è¯¯å‘ç”Ÿ
            self.results.add_test_result(
                "Hooké”™è¯¯å¤„ç†", 
                test1_passed, 
                f"æ­£ç¡®å¤„ç†äº† {len(error_results)} ä¸ªé”™è¯¯" if test1_passed else "é”™è¯¯å¤„ç†æœºåˆ¶å¼‚å¸¸"
            )
            
            # æµ‹è¯•2: æµ‹è¯•æ™ºèƒ½ä½“ä»»åŠ¡å¤±è´¥æ¢å¤
            framework = EnhancedExpertAgentFrameworkV2()
            
            # åˆ›å»ºä¸€ä¸ªä¼šå¯¼è‡´å¤±è´¥çš„ä»»åŠ¡
            error_task = AgentTask(
                task_id="error_task_001",
                task_type="development",
                complexity=TaskComplexity.EXPERT,
                description="ä¸€ä¸ªä¼šå¯¼è‡´å¤±è´¥çš„å¤æ‚ä»»åŠ¡",
                requirements=["ä¸å­˜åœ¨çš„åŠŸèƒ½"],
                context={"simulate_error": True}
            )
            
            # å°è¯•æ‰§è¡Œä»»åŠ¡ï¼ˆé¢„æœŸä¼šå¤±è´¥ï¼Œä½†ç³»ç»Ÿåº”è¯¥èƒ½å¤„ç†ï¼‰
            try:
                await framework.assign_task(error_task)
                result = await framework.execute_task(error_task.task_id)
                test2_passed = True  # æ— è®ºç»“æœå¦‚ä½•ï¼Œèƒ½æ­£å¸¸å¤„ç†å°±ç®—æˆåŠŸ
            except Exception:
                test2_passed = True  # å¼‚å¸¸è¢«æ­£ç¡®å¤„ç†
            
            self.results.add_test_result(
                "æ™ºèƒ½ä½“é”™è¯¯æ¢å¤", 
                test2_passed, 
                "é”™è¯¯æ¢å¤æœºåˆ¶æ­£å¸¸" if test2_passed else "é”™è¯¯æ¢å¤æœºåˆ¶å¼‚å¸¸"
            )
            
        except Exception as e:
            self.results.add_test_result("é”™è¯¯æ¢å¤", False, f"å¼‚å¸¸: {str(e)}")
    
    async def test_super_thinking_mode(self):
        """æµ‹è¯•è¶…çº§æ€è€ƒæ¨¡å¼"""
        logger.info("ğŸ§ª æµ‹è¯•è¶…çº§æ€è€ƒæ¨¡å¼")
        
        try:
            # æµ‹è¯•1: éªŒè¯Promptä¸­åŒ…å«è¶…çº§æ€è€ƒæ¿€æ´»
            engine = SuperAgentPromptEngine()
            prompt = engine.generate_agent_prompt("å…¨æ ˆå·¥ç¨‹å¸ˆ", {"test": "super_thinking"})
            
            required_phrases = [
                "ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒ",
                "æé™æ€è€ƒ",
                "æ·±åº¦æ€è€ƒ",
                "å…¨åŠ›æ€è€ƒ",
                "è¶…å¼ºæ€è€ƒ",
                "è®¤çœŸä»”ç»†æ€è€ƒ",
                "ultrathink",
                "think really super hard",
                "think intensely"
            ]
            
            super_thinking_phrases = [phrase for phrase in required_phrases if phrase in prompt]
            test1_passed = len(super_thinking_phrases) >= 6  # è‡³å°‘åŒ…å«6ä¸ªå…³é”®çŸ­è¯­
            
            self.results.add_test_result(
                "è¶…çº§æ€è€ƒæ¨¡å¼æ¿€æ´»", 
                test1_passed, 
                f"åŒ…å« {len(super_thinking_phrases)}/{len(required_phrases)} ä¸ªå…³é”®çŸ­è¯­" 
                if test1_passed else f"åªåŒ…å« {len(super_thinking_phrases)} ä¸ªå…³é”®çŸ­è¯­"
            )
            
            # æµ‹è¯•2: éªŒè¯æ™ºèƒ½ä½“ä½¿ç”¨è¶…çº§æ€è€ƒæ¨¡å¼
            framework = EnhancedExpertAgentFrameworkV2()
            agent = framework.agents.get("æŠ€æœ¯æ„¿æ™¯å¸ˆ")
            
            if agent:
                prompt = agent.generate_specialized_prompt({"test": "strategic"})
                test2_passed = "ultrathink" in prompt and "think really super hard" in prompt
                self.results.add_test_result(
                    "æ™ºèƒ½ä½“è¶…çº§æ€è€ƒ", 
                    test2_passed, 
                    "æ™ºèƒ½ä½“ä½¿ç”¨è¶…çº§æ€è€ƒæ¨¡å¼" if test2_passed else "æ™ºèƒ½ä½“æœªä½¿ç”¨è¶…çº§æ€è€ƒæ¨¡å¼"
                )
            
            # æµ‹è¯•3: éªŒè¯è¶…çº§æ€è€ƒå¯¹è¾“å‡ºè´¨é‡çš„å½±å“
            # è¿™é‡Œå¯ä»¥é€šè¿‡æ¯”è¾ƒæœ‰æ— è¶…çº§æ€è€ƒæ¨¡å¼çš„Promptè´¨é‡æ¥éªŒè¯
            simple_prompt = "è¯·å›ç­”è¿™ä¸ªé—®é¢˜"
            enhanced_prompt = engine.generate_agent_prompt("å…¨æ ˆå·¥ç¨‹å¸ˆ", {})
            
            # ç®€å•çš„è´¨é‡è¯„ä¼°ï¼ˆé•¿åº¦ã€ç»“æ„å¤æ‚åº¦ç­‰ï¼‰
            quality_indicators = [
                len(enhanced_prompt) > len(simple_prompt) * 5,  # é•¿åº¦æ˜¾è‘—å¢åŠ 
                "å·¥ä½œæµç¨‹" in enhanced_prompt,  # åŒ…å«å·¥ä½œæµç¨‹
                "è´¨é‡æ ‡å‡†" in enhanced_prompt,  # åŒ…å«è´¨é‡æ ‡å‡†
                "å·¥å…·ä½¿ç”¨" in enhanced_prompt,  # åŒ…å«å·¥å…·ä½¿ç”¨
                "äº¤ä»˜æ ‡å‡†" in enhanced_prompt   # åŒ…å«äº¤ä»˜æ ‡å‡†
            ]
            
            test3_passed = sum(quality_indicators) >= 4
            self.results.add_test_result(
                "è¶…çº§æ€è€ƒè´¨é‡æå‡", 
                test3_passed, 
                f"è´¨é‡æŒ‡æ ‡: {sum(quality_indicators)}/5" 
                if test3_passed else f"è´¨é‡æŒ‡æ ‡ä¸è¶³: {sum(quality_indicators)}/5"
            )
            
        except Exception as e:
            self.results.add_test_result("è¶…çº§æ€è€ƒæ¨¡å¼", False, f"å¼‚å¸¸: {str(e)}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨ Phase 1 ç»¼åˆæµ‹è¯•å¥—ä»¶")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = Phase1ComprehensiveTestSuite()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    summary = await test_suite.run_all_tests()
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    print("\n" + "="*60)
    print("ğŸ¯ Phase 1 ç»¼åˆæµ‹è¯•ç»“æœ")
    print("="*60)
    print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
    print(f"âœ… é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
    print(f"âŒ å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.2f}%")
    print(f"â±ï¸ æ€»è€—æ—¶: {summary['duration']:.2f}ç§’")
    print(f"ğŸ“Š çŠ¶æ€: {summary['status']}")
    
    if summary['status'] == 'PASSED':
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Phase 1 åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
    else:
        print(f"\nâš ï¸ æœ‰ {summary['failed_tests']} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
    
    print("\nğŸ“„ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: .iflow/tests/phase1_test_results.json")
    print("="*60)
    
    return summary


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    result = asyncio.run(main())
    
    # é€€å‡ºç 
    sys.exit(0 if result['status'] == 'PASSED' else 1)