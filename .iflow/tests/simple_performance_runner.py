#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„æ€§èƒ½æµ‹è¯•è¿è¡Œå™¨
é¿å…å¤æ‚çš„å¯¼å…¥é—®é¢˜ï¼Œç›´æ¥è¿è¡ŒåŸºç¡€æµ‹è¯•
"""

import subprocess
import sys
import os
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import asyncio

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

class SimplePerformanceRunner:
    """ç®€åŒ–çš„æ€§èƒ½æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.test_results = {}
        self.test_scripts = [
            {
                "name": "ARQæ€§èƒ½æµ‹è¯•",
                "script": "iflow/tests/arq_performance_test.py",
                "timeout": 300,
                "description": "æµ‹è¯•ARQæ¨ç†å¼•æ“çš„åŸºç¡€æ€§èƒ½"
            },
            {
                "name": "æ„è¯†æµç³»ç»Ÿæµ‹è¯•", 
                "script": "iflow/tests/consciousness_stream_test.py",
                "timeout": 300,
                "description": "æµ‹è¯•æ„è¯†æµç³»ç»Ÿçš„ä¸Šä¸‹æ–‡ç®¡ç†"
            },
            {
                "name": "å·¥ä½œæµå¼•æ“åŸºå‡†æµ‹è¯•",
                "script": "iflow/tests/workflow_engine_benchmark.py", 
                "timeout": 600,
                "description": "æµ‹è¯•å·¥ä½œæµå¼•æ“çš„æ‰§è¡Œæ•ˆç‡"
            },
            {
                "name": "Hooksç³»ç»Ÿæµ‹è¯•",
                "script": "iflow/tests/hooks_system_test.py",
                "timeout": 180,
                "description": "æµ‹è¯•Hooksç³»ç»Ÿçš„å®Œæ•´æ€§å’Œæ•ˆç‡"
            }
        ]
    
    def run_test_script(self, script_path: str, timeout: int = 300) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•è„šæœ¬"""
        full_path = PROJECT_ROOT / script_path
        
        if not full_path.exists():
            return {
                "success": False,
                "error": f"æµ‹è¯•è„šæœ¬ä¸å­˜åœ¨: {script_path}",
                "execution_time": 0,
                "output": ""
            }
        
        try:
            print(f"è¿è¡Œæµ‹è¯•: {script_path}")
            start_time = time.time()
            
            # è¿è¡Œæµ‹è¯•è„šæœ¬
            result = subprocess.run(
                [sys.executable, str(full_path)],
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=str(PROJECT_ROOT),
                env={**os.environ, "PYTHONPATH": str(PROJECT_ROOT)}
            )
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            # åˆ†æç»“æœ
            success = result.returncode == 0
            output = result.stdout + result.stderr
            
            return {
                "success": success,
                "execution_time": execution_time,
                "returncode": result.returncode,
                "output": output,
                "error": result.stderr if not success else ""
            }
            
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "error": f"æµ‹è¯•è¶…æ—¶ ({timeout} ç§’)",
                "execution_time": timeout,
                "output": ""
            }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "execution_time": 0,
                "output": ""
            }
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ç®€åŒ–æ€§èƒ½æµ‹è¯•è¿è¡Œå™¨å¯åŠ¨")
        print("=" * 60)
        
        all_results = {
            "test_metadata": {
                "timestamp": datetime.now().isoformat(),
                "runner_type": "simple_performance_runner",
                "test_count": len(self.test_scripts)
            },
            "test_results": {},
            "summary": {},
            "recommendations": []
        }
        
        total_start_time = time.time()
        
        # ä¾æ¬¡è¿è¡Œæ¯ä¸ªæµ‹è¯•
        for i, test_config in enumerate(self.test_scripts):
            print(f"\n{'='*60}")
            print(f"æµ‹è¯• {i+1}/{len(self.test_scripts)}: {test_config['name']}")
            print(f"æè¿°: {test_config['description']}")
            print(f"è„šæœ¬: {test_config['script']}")
            print(f"è¶…æ—¶: {test_config['timeout']} ç§’")
            print(f"{'='*60}")
            
            # åŒæ­¥è¿è¡Œæµ‹è¯•ï¼ˆé¿å…å¼‚æ­¥å¯¼å…¥é—®é¢˜ï¼‰
            result = self.run_test_script(test_config["script"], test_config["timeout"])
            all_results["test_results"][test_config["name"]] = result
            
            # æ˜¾ç¤ºç»“æœ
            status = "æˆåŠŸ" if result["success"] else "å¤±è´¥"
            print(f"ç»“æœ: {status}")
            print(f"æ‰§è¡Œæ—¶é—´: {result['execution_time']:.2f}ç§’")
            
            if not result["success"]:
                print(f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                if result.get("returncode", 0) != 0:
                    print(f"è¿”å›ç : {result['returncode']}")
        
        total_end_time = time.time()
        total_execution_time = total_end_time - total_start_time
        
        # ç”Ÿæˆæ‘˜è¦
        successful_tests = sum(1 for r in all_results["test_results"].values() if r["success"])
        total_tests = len(all_results["test_results"])
        
        all_results["summary"] = {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": successful_tests / total_tests if total_tests > 0 else 0,
            "total_execution_time": total_execution_time,
            "average_execution_time": sum(r["execution_time"] for r in all_results["test_results"].values()) / total_tests if total_tests > 0 else 0,
            "fastest_test": "",
            "slowest_test": ""
        }
        
        # æ‰¾å‡ºæœ€å¿«å’Œæœ€æ…¢çš„æµ‹è¯•
        if all_results["test_results"]:
            fastest = min(all_results["test_results"].items(), key=lambda x: x[1]["execution_time"])
            slowest = max(all_results["test_results"].items(), key=lambda x: x[1]["execution_time"])
            all_results["summary"]["fastest_test"] = fastest[0]
            all_results["summary"]["slowest_test"] = slowest[0]
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        all_results["recommendations"] = self.generate_recommendations(all_results["test_results"])
        
        # ä¿å­˜ç»“æœ
        self.save_results(all_results)
        
        # æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
        self.display_final_report(all_results)
        
        return all_results
    
    def generate_recommendations(self, test_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        failed_tests = [name for name, result in test_results.items() if not result["success"]]
        successful_tests = [name for name, result in test_results.items() if result["success"]]
        
        if failed_tests:
            recommendations.append(f"ğŸ”§ {len(failed_tests)} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ç›¸å…³æ¨¡å—")
            
            for test_name in failed_tests:
                if "ARQ" in test_name:
                    recommendations.append("ä¼˜åŒ–ARQæ¨ç†å¼•æ“çš„å¯¼å…¥å’Œåˆå§‹åŒ–é€»è¾‘")
                elif "consciousness" in test_name:
                    recommendations.append("æ£€æŸ¥æ„è¯†æµç³»ç»Ÿçš„ä¾èµ–å’Œé…ç½®")
                elif "workflow" in test_name:
                    recommendations.append("ä¼˜åŒ–å·¥ä½œæµå¼•æ“çš„å¹¶å‘å¤„ç†èƒ½åŠ›")
                elif "Hooks" in test_name:
                    recommendations.append("æ¸…ç†Hooksç³»ç»Ÿçš„é‡å¤æ–‡ä»¶å’Œé…ç½®å†²çª")
        
        # åŸºäºæ‰§è¡Œæ—¶é—´çš„å»ºè®®
        total_time = sum(result["execution_time"] for result in test_results.values())
        if total_time > 600:  # è¶…è¿‡10åˆ†é’Ÿ
            recommendations.append("â±ï¸ æ€»æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æµ‹è¯•è„šæœ¬æ€§èƒ½")
        elif total_time > 300:  # è¶…è¿‡5åˆ†é’Ÿ
            recommendations.append("â° æ€»æ‰§è¡Œæ—¶é—´åé•¿ï¼Œå¯ä»¥è€ƒè™‘å¹¶è¡Œæ‰§è¡Œ")
        
        # åŸºäºæˆåŠŸç‡çš„å»ºè®®
        success_rate = len(successful_tests) / len(test_results) if test_results else 0
        if success_rate < 0.5:
            recommendations.append("ğŸ“Š æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤åŸºç¡€åŠŸèƒ½")
        elif success_rate < 0.8:
            recommendations.append("ğŸ“ˆ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦é’ˆå¯¹æ€§ä¼˜åŒ–")
        else:
            recommendations.append("æµ‹è¯•æˆåŠŸç‡è‰¯å¥½ï¼Œç³»ç»ŸåŸºç¡€ç¨³å®š")
        
        return recommendations
    
    def save_results(self, results: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"simple_performance_results_{timestamp}.json"
        
        results_path = PROJECT_ROOT / "iflow" / "tests" / "benchmark" / filename
        results_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜: {results_path}")
    
    def display_final_report(self, results: Dict[str, Any]):
        """æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š"""
        summary = results["summary"]
        
        print(f"\n{'='*60}")
        print("ğŸ“Š ç®€åŒ–æ€§èƒ½æµ‹è¯•æœ€ç»ˆæŠ¥å‘Š")
        print(f"{'='*60}")
        
        print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"æˆåŠŸæµ‹è¯•æ•°: {summary['successful_tests']}")
        print(f"æˆåŠŸç‡: {summary['success_rate']:.2%}")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {summary['total_execution_time']:.2f}ç§’")
        print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {summary['average_execution_time']:.2f}ç§’")
        print(f"æœ€å¿«æµ‹è¯•: {summary['fastest_test']}")
        print(f"æœ€æ…¢æµ‹è¯•: {summary['slowest_test']}")
        
        print(f"\nè¯¦ç»†ç»“æœ:")
        for test_name, result in results["test_results"].items():
            status = "æˆåŠŸ" if result["success"] else "å¤±è´¥"
            print(f"- {test_name}: {status} ({result['execution_time']:.2f}s)")
            if not result["success"] and result.get("error"):
                print(f"  é”™è¯¯: {result['error'][:100]}...")
        
        print(f"\nä¼˜åŒ–å»ºè®®:")
        for i, recommendation in enumerate(results["recommendations"], 1):
            print(f"{i}. {recommendation}")
        
        # æ€§èƒ½è¯„çº§
        success_rate = summary["success_rate"]
        if success_rate == 1.0:
            rating = "ä¼˜ç§€"
        elif success_rate >= 0.8:
            rating = "è‰¯å¥½"
        elif success_rate >= 0.6:
            rating = "ä¸€èˆ¬"
        else:
            rating = "éœ€è¦æ”¹è¿›"
        
        print(f"\næ€§èƒ½è¯„çº§: {rating}")

async def main():
    """ä¸»å‡½æ•°"""
    runner = SimplePerformanceRunner()
    await runner.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())